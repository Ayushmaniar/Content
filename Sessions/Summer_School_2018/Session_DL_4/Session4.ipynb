{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Session4.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "gq3BsxawcYyn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Recurrent Neural Networks\n",
        "\n",
        "Recurrent Neural Networks (RNN) are very effective for Natural Language Processing and other sequence tasks because they have \"memory\". They can read inputs $x^{\\langle t \\rangle}$ (such as words) one at a time, and remember some information/context through the hidden layer activations that get passed from one time-step to the next. This allows a uni-directional RNN to take information from the past to process later inputs. A bidirection RNN can take context from both the past and the future. "
      ]
    },
    {
      "metadata": {
        "id": "Rkj0lZMhcY0_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Notation**:\n",
        "- Superscript $[l]$ denotes an object associated with the $l^{th}$ layer. \n",
        "    - Example: $a^{[4]}$ is the $4^{th}$ layer activation. $W^{[5]}$ and $b^{[5]}$ are the $5^{th}$ layer parameters.\n",
        "\n",
        "- Superscript $(i)$ denotes an object associated with the $i^{th}$ example. \n",
        "    - Example: $x^{(i)}$ is the $i^{th}$ training example input.\n",
        "\n",
        "- Superscript $\\langle t \\rangle$ denotes an object at the $t^{th}$ time-step. \n",
        "    - Example: $x^{\\langle t \\rangle}$ is the input x at the $t^{th}$ time-step. $x^{(i)\\langle t \\rangle}$ is the input at the $t^{th}$ timestep of example $i$.\n",
        "    \n",
        "- Lowerscript $i$ denotes the $i^{th}$ entry of a vector.\n",
        "    - Example: $a^{[l]}_i$ denotes the $i^{th}$ entry of the activations in layer $l$.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "rG1EltVycY8i",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1 - Forward propagation for the basic Recurrent Neural Network\n",
        " \n",
        "The basic RNN that you will implement has the structure below. In this example, $T_x = T_y$. "
      ]
    },
    {
      "metadata": {
        "id": "yotqgB3IcY3F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://imgur.com/Yaa79IN.png\" style=\"width:500;height:300px;\">\n",
        "<caption><center> **Figure 1**: Basic RNN model </center></caption>"
      ]
    },
    {
      "metadata": {
        "id": "ncAhbvAxcY5J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here's how you can implement an RNN: \n",
        "\n",
        "**Code Instructions**:\n",
        "1. Implement the calculations needed for one time-step of the RNN.\n",
        "2. Implement a loop over $T_x$ time-steps in order to process all the inputs, one at a time. \n",
        "\n",
        "Let's go!\n",
        "\n",
        "## 1.1 - RNN cell\n",
        "\n",
        "A Recurrent neural network can be seen as the repetition of a single cell. You are first going to implement the computations for a single time-step. The following figure describes the operations for a single time-step of an RNN cell. \n",
        "\n",
        "<img src=\"https://imgur.com/vGxAY57.png\" style=\"width:700px;height:300px;\">\n",
        "<caption><center> **Figure 2**: Basic RNN cell. Takes as input $x^{\\langle t \\rangle}$ (current input) and $a^{\\langle t - 1\\rangle}$ (previous hidden state containing information from the past), and outputs $a^{\\langle t \\rangle}$ which is given to the next RNN cell and also used to predict $y^{\\langle t \\rangle}$ </center></caption>\n",
        "\n",
        "\n",
        "**Code Instructions**:\n",
        "1. Compute the hidden state with tanh activation: $a^{\\langle t \\rangle} = \\tanh(W_{aa} a^{\\langle t-1 \\rangle} + W_{ax} x^{\\langle t \\rangle} + b_a)$.\n",
        "2. Using your new hidden state $a^{\\langle t \\rangle}$, compute the prediction $\\hat{y}^{\\langle t \\rangle} = softmax(W_{ya} a^{\\langle t \\rangle} + b_y)$. We provided you a function: `softmax`.\n",
        "3. Store $(a^{\\langle t \\rangle}, a^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}, parameters)$ in cache\n",
        "4. Return $a^{\\langle t \\rangle}$ , $y^{\\langle t \\rangle}$ and cache\n",
        "\n",
        "We will vectorize over $m$ examples. Thus, $x^{\\langle t \\rangle}$ will have dimension $(n_x,m)$, and $a^{\\langle t \\rangle}$ will have dimension $(n_a,m)$. "
      ]
    },
    {
      "metadata": {
        "id": "e1YomYKhdyvz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.2 - RNN forward pass \n",
        "\n",
        "RNN as the repetition of the cell you've just built. If your input sequence of data is carried over 10 time steps, then you will copy the RNN cell 10 times. Each cell tak\n",
        "es as input the hidden state from the previous cell ($a^{\\langle t-1 \\rangle}$) and the current time-step's input data ($x^{\\langle t \\rangle}$). It outputs a hidden state ($a^{\\langle t \\rangle}$) and a prediction ($y^{\\langle t \\rangle}$) for this time-step.\n",
        "\n",
        "\n",
        "<img src=\"https://imgur.com/YdNCgkN.png\" style=\"width:800px;height:300px;\">\n",
        "<caption><center> **Figure 3**: Basic RNN. The input sequence $x = (x^{\\langle 1 \\rangle}, x^{\\langle 2 \\rangle}, ..., x^{\\langle T_x \\rangle})$  is carried over $T_x$ time steps. The network outputs $y = (y^{\\langle 1 \\rangle}, y^{\\langle 2 \\rangle}, ..., y^{\\langle T_x \\rangle})$. </center></caption>\n",
        "\n",
        "**Code Instructions**:\n",
        "1. Create a vector of zeros ($a$) that will store all the hidden states computed by the RNN.\n",
        "2. Initialize the \"next\" hidden state as $a_0$ (initial hidden state).\n",
        "3. Start looping over each time step, your incremental index is $t$ :\n",
        "    - Update the \"next\" hidden state and the cache by running `rnn_cell_forward`\n",
        "    - Store the \"next\" hidden state in $a$ ($t^{th}$ position) \n",
        "    - Store the prediction in y\n",
        "    - Add the cache to the list of caches\n",
        "4. Return $a$, $y$ and caches\n"
      ]
    },
    {
      "metadata": {
        "id": "pC17Ysu7dyzm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.3 - Basic RNN  backward pass\n",
        "\n",
        "We will start by computing the backward pass for the basic RNN-cell.\n",
        "\n",
        "<img src=\"https://imgur.com/3EniMu4.png\" style=\"width:500;height:300px;\"> <br>\n",
        "<caption><center> **Figure 4**: RNN-cell's backward pass. Just like in a fully-connected neural network, the derivative of the cost function $J$ backpropagates through the RNN by following the chain-rule from calculus. The chain-rule is also used to calculate $(\\frac{\\partial J}{\\partial W_{ax}},\\frac{\\partial J}{\\partial W_{aa}},\\frac{\\partial J}{\\partial b})$ to update the parameters $(W_{ax}, W_{aa}, b_a)$. </center></caption>"
      ]
    },
    {
      "metadata": {
        "id": "L6pw-MLudy3n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Deriving the one step backward functions: \n",
        "\n",
        "To compute the `rnn_cell_backward` you need to compute the following equations. It is a good exercise to derive them by hand. \n",
        "\n",
        "The derivative of $\\tanh$ is $1-\\tanh(x)^2$.\n",
        "\n",
        "Similarly for $\\frac{ \\partial a^{\\langle t \\rangle} } {\\partial W_{ax}}, \\frac{ \\partial a^{\\langle t \\rangle} } {\\partial W_{aa}},  \\frac{ \\partial a^{\\langle t \\rangle} } {\\partial b}$, the derivative of  $\\tanh(u)$ is $(1-\\tanh(u)^2)du$. \n",
        "\n",
        "The final two equations also follow same rule and are derived using the $\\tanh$ derivative. Note that the arrangement is done in a way to get the same dimensions to match."
      ]
    },
    {
      "metadata": {
        "id": "P5NjmBkUdy6b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Backward pass through the RNN\n",
        "\n",
        "Computing the gradients of the cost with respect to $a^{\\langle t \\rangle}$ at every time-step $t$ is useful because it is what helps the gradient backpropagate to the previous RNN-cell. To do so, you need to iterate through all the time steps starting at the end, and at each step, you increment the overall $db_a$, $dW_{aa}$, $dW_{ax}$ and you store $dx$.\n",
        "\n",
        "**Instructions**:\n",
        "\n",
        "Implement the `rnn_backward` function. Initialize the return variables with zeros first and then loop through all the time steps while calling the `rnn_cell_backward` at each time timestep, update the other variables accordingly."
      ]
    },
    {
      "metadata": {
        "id": "KprcSzeady9s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the next part, you will build a more complex LSTM model, which is better at addressing vanishing gradients. The LSTM will be better able to remember a piece of information and keep it saved for many timesteps. "
      ]
    },
    {
      "metadata": {
        "id": "GwUKMxiEdy2A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2 - Long Short-Term Memory (LSTM) network\n",
        "\n",
        "This following figure shows the operations of an LSTM-cell.\n",
        "\n",
        "<img src=\"https://imgur.com/wRyYVQ6.png\" style=\"width:500;height:400px;\">\n",
        "<caption><center> **Figure 4**: LSTM-cell. This tracks and updates a \"cell state\" or memory variable $c^{\\langle t \\rangle}$ at every time-step, which can be different from $a^{\\langle t \\rangle}$. </center></caption>\n",
        "\n",
        "Similar to the RNN example above, you will start by understanding the LSTM cell for a single time-step. Then you can iteratively call it from inside a for-loop to have it process an input with $T_x$ time-steps. \n",
        "\n",
        "### About the gates\n",
        "\n",
        "#### - Forget gate\n",
        "\n",
        "For the sake of this illustration, lets assume we are reading words in a piece of text, and want use an LSTM to keep track of grammatical structures, such as whether the subject is singular or plural. If the subject changes from a singular word to a plural word, we need to find a way to get rid of our previously stored memory value of the singular/plural state. In an LSTM, the forget gate lets us do this: \n",
        "\n",
        "$$\\Gamma_f^{\\langle t \\rangle} = \\sigma(W_f[a^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}] + b_f)\\tag{1} $$\n",
        "\n",
        "Here, $W_f$ are weights that govern the forget gate's behavior. We concatenate $[a^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}]$ and multiply by $W_f$. The equation above results in a vector $\\Gamma_f^{\\langle t \\rangle}$ with values between 0 and 1. This forget gate vector will be multiplied element-wise by the previous cell state $c^{\\langle t-1 \\rangle}$. So if one of the values of $\\Gamma_f^{\\langle t \\rangle}$ is 0 (or close to 0) then it means that the LSTM should remove that piece of information (e.g. the singular subject) in the corresponding component of $c^{\\langle t-1 \\rangle}$. If one of the values is 1, then it will keep the information. \n",
        "\n",
        "#### - Update gate\n",
        "\n",
        "Once we forget that the subject being discussed is singular, we need to find a way to update it to reflect that the new subject is now plural. Here is the formulat for the update gate: \n",
        "\n",
        "$$\\Gamma_u^{\\langle t \\rangle} = \\sigma(W_u[a^{\\langle t-1 \\rangle}, x^{\\{t\\}}] + b_u)\\tag{2} $$ \n",
        "\n",
        "Similar to the forget gate, here $\\Gamma_u^{\\langle t \\rangle}$ is again a vector of values between 0 and 1. This will be multiplied element-wise with $\\tilde{c}^{\\langle t \\rangle}$, in order to compute $c^{\\langle t \\rangle}$.\n",
        "\n",
        "#### - Updating the cell \n",
        "\n",
        "To update the new subject we need to create a new vector of numbers that we can add to our previous cell state. The equation we use is: \n",
        "\n",
        "$$ \\tilde{c}^{\\langle t \\rangle} = \\tanh(W_c[a^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}] + b_c)\\tag{3} $$\n",
        "\n",
        "Finally, the new cell state is: \n",
        "\n",
        "$$ c^{\\langle t \\rangle} = \\Gamma_f^{\\langle t \\rangle}* c^{\\langle t-1 \\rangle} + \\Gamma_u^{\\langle t \\rangle} *\\tilde{c}^{\\langle t \\rangle} \\tag{4} $$\n",
        "\n",
        "\n",
        "#### - Output gate\n",
        "\n",
        "To decide which outputs we will use, we will use the following two formulas: \n",
        "\n",
        "$$ \\Gamma_o^{\\langle t \\rangle}=  \\sigma(W_o[a^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}] + b_o)\\tag{5}$$ \n",
        "$$ a^{\\langle t \\rangle} = \\Gamma_o^{\\langle t \\rangle}* \\tanh(c^{\\langle t \\rangle})\\tag{6} $$\n",
        "\n",
        "Where in equation 5 you decide what to output using a sigmoid function and in equation 6 you multiply that by the $\\tanh$ of the previous state. "
      ]
    },
    {
      "metadata": {
        "id": "RbBIkGOleVZV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.1 - LSTM cell\n",
        "\n",
        "**Instructions**:\n",
        "1. Concatenate $a^{\\langle t-1 \\rangle}$ and $x^{\\langle t \\rangle}$ in a single matrix: $concat = \\begin{bmatrix} a^{\\langle t-1 \\rangle} \\\\ x^{\\langle t \\rangle} \\end{bmatrix}$\n",
        "2. Compute all the formulas 1-6. You can use `sigmoid()` and `np.tanh()`.\n",
        "3. Compute the prediction $y^{\\langle t \\rangle}$. You can use `softmax()`"
      ]
    },
    {
      "metadata": {
        "id": "W5lLIj0peYlJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.2 - Forward pass for LSTM\n",
        "\n",
        "Now that you have implemented one step of an LSTM, you can now iterate this over this using a for-loop to process a sequence of $T_x$ inputs. \n",
        "\n",
        "<img src=\"https://imgur.com/CFEgAAx.png\" style=\"width:500;height:300px;\">\n",
        "<caption><center> **Figure 5**: LSTM over multiple time-steps. </center></caption>\n",
        "\n",
        "**Exercise:** Implement `lstm_forward()` to run an LSTM over $T_x$ time-steps. \n",
        "\n",
        "**Note**: $c^{\\langle 0 \\rangle}$ is initialized with zeros."
      ]
    },
    {
      "metadata": {
        "id": "xyvTSphqeYWD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The forward passes for the basic RNN and the LSTM. When using a deep learning framework, implementing the forward pass is sufficient to build systems that achieve great performance. Now we will see how to do backpropagation in LSTM  and RNNS\n"
      ]
    },
    {
      "metadata": {
        "id": "4VE20Fh9eYdb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.3- LSTM backward pass"
      ]
    },
    {
      "metadata": {
        "id": "sg_ZpZwEejTh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.3.1 One Step backward\n",
        "\n",
        "The LSTM backward pass is slighltly more complicated than the forward one. We have provided you with all the equations for the LSTM backward pass below. (If you enjoy calculus exercises feel free to try deriving these from scratch yourself.) \n",
        "\n",
        "### 2.3.2 gate derivatives\n",
        "\n",
        "$$d \\Gamma_o^{\\langle t \\rangle} = da_{next}*\\tanh(c_{next}) * \\Gamma_o^{\\langle t \\rangle}*(1-\\Gamma_o^{\\langle t \\rangle})\\tag{7}$$\n",
        "\n",
        "$$d\\tilde c^{\\langle t \\rangle} = dc_{next}*\\Gamma_u^{\\langle t \\rangle}+ \\Gamma_o^{\\langle t \\rangle} (1-\\tanh(c_{next})^2) * i_t * da_{next} * \\tilde c^{\\langle t \\rangle} * (1-\\tanh(\\tilde c)^2) \\tag{8}$$\n",
        "\n",
        "$$d\\Gamma_u^{\\langle t \\rangle} = dc_{next}*\\tilde c^{\\langle t \\rangle} + \\Gamma_o^{\\langle t \\rangle} (1-\\tanh(c_{next})^2) * \\tilde c^{\\langle t \\rangle} * da_{next}*\\Gamma_u^{\\langle t \\rangle}*(1-\\Gamma_u^{\\langle t \\rangle})\\tag{9}$$\n",
        "\n",
        "$$d\\Gamma_f^{\\langle t \\rangle} = dc_{next}*\\tilde c_{prev} + \\Gamma_o^{\\langle t \\rangle} (1-\\tanh(c_{next})^2) * c_{prev} * da_{next}*\\Gamma_f^{\\langle t \\rangle}*(1-\\Gamma_f^{\\langle t \\rangle})\\tag{10}$$\n",
        "\n",
        "### 2.3.3 parameter derivatives \n",
        "\n",
        "$$ dW_f = d\\Gamma_f^{\\langle t \\rangle} * \\begin{pmatrix} a_{prev} \\\\ x_t\\end{pmatrix}^T \\tag{11} $$\n",
        "$$ dW_u = d\\Gamma_u^{\\langle t \\rangle} * \\begin{pmatrix} a_{prev} \\\\ x_t\\end{pmatrix}^T \\tag{12} $$\n",
        "$$ dW_c = d\\tilde c^{\\langle t \\rangle} * \\begin{pmatrix} a_{prev} \\\\ x_t\\end{pmatrix}^T \\tag{13} $$\n",
        "$$ dW_o = d\\Gamma_o^{\\langle t \\rangle} * \\begin{pmatrix} a_{prev} \\\\ x_t\\end{pmatrix}^T \\tag{14}$$\n",
        "\n",
        "To calculate $db_f, db_u, db_c, db_o$ you just need to sum across the horizontal (axis= 1) axis on $d\\Gamma_f^{\\langle t \\rangle}, d\\Gamma_u^{\\langle t \\rangle}, d\\tilde c^{\\langle t \\rangle}, d\\Gamma_o^{\\langle t \\rangle}$ respectively. Note that you should have the `keep_dims = True` option.\n",
        "\n",
        "Finally, you will compute the derivative with respect to the previous hidden state, previous memory state, and input.\n",
        "\n",
        "$$ da_{prev} = W_f^T*d\\Gamma_f^{\\langle t \\rangle} + W_u^T * d\\Gamma_u^{\\langle t \\rangle}+ W_c^T * d\\tilde c^{\\langle t \\rangle} + W_o^T * d\\Gamma_o^{\\langle t \\rangle} \\tag{15}$$\n",
        "Here, the weights for equations 13 are the first n_a, (i.e. $W_f = W_f[:n_a,:]$ etc...)\n",
        "\n",
        "$$ dc_{prev} = dc_{next}\\Gamma_f^{\\langle t \\rangle} + \\Gamma_o^{\\langle t \\rangle} * (1- \\tanh(c_{next})^2)*\\Gamma_f^{\\langle t \\rangle}*da_{next} \\tag{16}$$\n",
        "$$ dx^{\\langle t \\rangle} = W_f^T*d\\Gamma_f^{\\langle t \\rangle} + W_u^T * d\\Gamma_u^{\\langle t \\rangle}+ W_c^T * d\\tilde c_t + W_o^T * d\\Gamma_o^{\\langle t \\rangle}\\tag{17} $$\n",
        "where the weights for equation 15 are from n_a to the end, (i.e. $W_f = W_f[n_a:,:]$ etc...)\n"
      ]
    },
    {
      "metadata": {
        "id": "48IPf1r-ejXk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.3 Backward pass through the LSTM RNN\n",
        "\n",
        "This part is very similar to the `rnn_backward` function you implemented above. You will first create variables of the same dimension as your return variables. You will then iterate over all the time steps starting from the end and call the one step function you implemented for LSTM at each iteration. You will then update the parameters by summing them individually. Finally return a dictionary with the new gradients. \n",
        "\n",
        "**Instructions**: Implement the `lstm_backward` function. Create a for loop starting from $T_x$ and going backward. For each step call `lstm_cell_backward` and update the your old gradients by adding the new gradients to them. Note that `dxt` is not updated but is stored."
      ]
    },
    {
      "metadata": {
        "id": "x993luPGejcI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Questions and Answers(Q&A)"
      ]
    },
    {
      "metadata": {
        "id": "xtS4D1qTejk8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In LSTM Network (Understanding LSTMs), Why input gate and output gate use tanh? what is the intuition behind this?\n",
        "\n",
        "A: The reason for using tanh is that its range is between (-1,1) whereas the sigmoid function is (0,1).Actually tanh is extended version of sigmoid.\n",
        "$$ tanh = 2* \\sigma(2x) - 1 $$\n",
        "So, the gradient of the tanh is almost twice than the sigmoid gradient in the x range (-1.663,1.663) and is almost equal to sigmoid gradient in the x range (-inf,-1.633) U (1.663,inf). So the convergence is faster in the case of tanh because of larger gradients and is more resistant to the vanishing gradient problems."
      ]
    },
    {
      "metadata": {
        "id": "IvXjAHsbcKY7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Activity Recognition"
      ]
    },
    {
      "metadata": {
        "id": "zOHQ7a5_aqUD",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import h5py\n",
        "from tqdm import tqdm\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
        "from keras.models import Model, load_model, Sequential\n",
        "from keras.layers import Input, LSTM, Dense, Dropout\n",
        "from keras.utils import to_categorical\n",
        "from keras.applications.imagenet_utils import preprocess_input\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, TensorBoard,EarlyStopping\n",
        "from keras.utils.io_utils import HDF5Matrix\n",
        "\n",
        "SEQ_LEN = 30\n",
        "MAX_SEQ_LEN = 200\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M9-tCCZscPW6",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def get_data(path, if_pd=False):\n",
        "    \"\"\"Load our data from file.\"\"\"\n",
        "    df = pd.read_csv(path)\n",
        "    return df\n",
        "\n",
        "def get_class_dict(df):\n",
        "    class_name =  list(df['class'].unique())\n",
        "    index = np.arange(0, len(class_name))\n",
        "    label_index = dict(zip(class_name, index))\n",
        "    index_label = dict(zip(index, class_name))\n",
        "    return (label_index, index_label)\n",
        "    \n",
        "def clean_data(df):\n",
        "    mask = np.logical_and(df['frames'] >= SEQ_LEN, df['frames'] <= MAX_SEQ_LEN)\n",
        "    df = df[mask]\n",
        "    return df\n",
        "def split_train_test(df):\n",
        "    partition =  (df.groupby(['partition']))\n",
        "    un = df['partition'].unique()\n",
        "    train = partition.get_group(un[0])\n",
        "    test = partition.get_group(un[1])\n",
        "    return (train, test)\n",
        "\n",
        "def preprocess_image(img):\n",
        "    img = cv2.resize(img, (227,227))\n",
        "    return preprocess_input(img)\n",
        "    \n",
        "    \n",
        "def encode_video(row, model, label_index):\n",
        "    cap = cv2.VideoCapture(os.path.join(\"data\",\"UCF-101\",str(row[\"class\"].iloc[0]) ,str(row[\"video_name\"].iloc[0]) + \".avi\"))\n",
        "    images = []  \n",
        "    for i in range(SEQ_LEN):\n",
        "        ret, frame = cap.read()\n",
        "        frame = preprocess_image(frame)\n",
        "        images.append(frame)\n",
        "    \n",
        "    \n",
        "    features = model.predict(np.array(images))\n",
        "    index = label_index[row[\"class\"].iloc[0]]\n",
        "    y_onehot = to_categorical(index, len(label_index.keys()))\n",
        "    \n",
        "    return features, y_onehot\n",
        "\n",
        "def encode_dataset(data, model, label_index, phase):\n",
        "    input_f = []\n",
        "    output_y = []\n",
        "    required_classes = [\"ApplyEyeMakeup\" , \"ApplyLipstick\" , \"Archery\" , \"BabyCrawling\" , \"BalanceBeam\" ,\n",
        "                       \"BandMarching\" , \"BaseballPitch\" , \"Basketball\" , \"BasketballDunk\"]\n",
        "   \n",
        "    \n",
        "    for i in tqdm(range(data.shape[0])):\n",
        "    # Check whether the given row , is of a class that is required\n",
        "        if str(data.iloc[[i]][\"class\"].iloc[0]) in required_classes:\n",
        " \n",
        "            features,y =  encode_video(data.iloc[[i]], model, label_index)\n",
        "            input_f.append(features)\n",
        "            output_y.append(y)\n",
        "        \n",
        "    \n",
        "    f = h5py.File(phase+'_8'+'.h5', 'w')\n",
        "    f.create_dataset(phase, data=np.array(input_f))\n",
        "    f.create_dataset(phase+\"_labels\", data=np.array(output_y))\n",
        "    \n",
        "    del input_f[:]\n",
        "    del output_y[:]\n",
        "\n",
        "def lstm():\n",
        "    \"\"\"Build a simple LSTM network. We pass the extracted features from\n",
        "    our CNN to this model predomenently.\"\"\"\n",
        "    input_shape = (SEQ_LEN, 2048)\n",
        "    # Model.\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(2048, return_sequences=False,\n",
        "                   input_shape=input_shape,\n",
        "                   dropout=0.5))\n",
        "    model.add(Dense(512, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    #model.add(Dense(len(label_index.keys()), activation='softmax'))\n",
        "    model.add(Dense(99, activation='softmax'))\n",
        "    \n",
        "    checkpoint = ModelCheckpoint(filepath='models\\\\checkpoint-{epoch:02d}-{val_loss:.2f}.hdf5')\n",
        "    \n",
        "    tb_callback = TensorBoard(\n",
        "    log_dir=\"logs\",\n",
        "    histogram_freq=2,\n",
        "    write_graph=True\n",
        "    )\n",
        "    \n",
        "    early_stopping = EarlyStopping(monitor = 'val_loss',patience= 10)\n",
        "    \n",
        "    callback_list = [checkpoint, tb_callback]\n",
        "\n",
        "    optimizer = Adam(lr=1e-5, decay=1e-6)\n",
        "    metrics = ['accuracy', 'top_k_categorical_accuracy']\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer,metrics=metrics)\n",
        "    return model, callback_list\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8rQW1WYscPY7",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Get model with pretrained weights.\n",
        "    base_model = InceptionV3(\n",
        "    weights='imagenet',\n",
        "    include_top=True)\n",
        "    \n",
        "    \n",
        "    # We'll extract features at the final pool layer.\n",
        "    model = Model(\n",
        "        inputs=base_model.input,\n",
        "        outputs=base_model.get_layer('avg_pool').output)\n",
        "    \n",
        "    # Getting the data\n",
        "    df = get_data('.\\\\data\\\\data_file.csv')\n",
        "    \n",
        "    # Clean the data\n",
        "    df_clean = clean_data(df)\n",
        "    \n",
        "    # Creating index-label maps and inverse_maps\n",
        "    label_index, index_label = get_class_dict(df_clean)\n",
        "    \n",
        "    # Split the dataset into train and test\n",
        "    train, test = split_train_test(df_clean)\n",
        "    \n",
        "    # Encoding the dataset\n",
        "    encode_dataset(train, model, label_index, \"train\")\n",
        "    encode_dataset(test,model,label_index,\"test\")\n",
        "    \n",
        "    x_train = HDF5Matrix('train_8.h5', 'train')\n",
        "    y_train = HDF5Matrix('train_8.h5', 'train_labels')\n",
        "\n",
        "    x_test = HDF5Matrix('test_8.h5', 'test')\n",
        "    y_test = HDF5Matrix('test_8.h5', 'test_labels')\n",
        "    \n",
        "    model, callback_list = lstm()\n",
        "    model.fit(x_train, y_train, batch_size = BATCH_SIZE, epochs = EPOCHS,\n",
        "              verbose = 2,validation_data = (x_test, y_test),\n",
        "              shuffle = 'batch', callbacks=callback_list)\n",
        "    \n",
        "    #model.save(\"Activity_Recognition.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NnkJp3ljcPam",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NmMzLsUdlUhV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Sequence to Sequence Models - a general overview"
      ]
    },
    {
      "metadata": {
        "id": "dooS0ulglUhW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Many times, we might have to convert one sequence to another. Really?  Where?\n",
        "\n",
        "We do this in machine translation. For this purpose we use models known as sequence to sequence models. (**seq2seq**)\n",
        "\n",
        "If we take a high-level view, a seq2seq model has encoder, decoder and intermediate step as its main components:\n",
        "![alt text](https://cdn-images-1.medium.com/max/800/1*3lj8AGqfwEE5KCTJ-dXTvg.png)"
      ]
    },
    {
      "metadata": {
        "id": "SItyOliilUhY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A basic sequence-to-sequence model consists of two recurrent neural networks (RNNs): an encoder that processes the input and a decoder that generates the output. This basic architecture is depicted below.\n",
        "![alt text](https://www.tensorflow.org/images/basic_seq2seq.png)\n",
        "\n",
        "Each box in the picture above represents a cell of the RNN, most commonly a GRU cell or an LSTM cell. Encoder and decoder can share weights or, as is more common, use a different set of parameters."
      ]
    },
    {
      "metadata": {
        "id": "AjptK3QzlUhZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the basic model depicted above, every input has to be encoded into a fixed-size state vector, as that is the only thing passed to the decoder. To allow the decoder more direct access to the input, an **attention** mechanism was introduced. We'll look into details of the attention mechanism in the next part."
      ]
    },
    {
      "metadata": {
        "id": "2Mk4NHdFlUhZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Encoder\n",
        "\n",
        "Our input sequence is how are you. Each word from the input sequence is associated to a vector \n",
        "w∈Rd (via a lookup table). In our case, we have 3 words, thus our input will be transformed into $[w0,w1,w2]∈R^{d×3}$. Then, we simply run an LSTM over this sequence of vectors and store the last hidden state outputed by the LSTM: this will be our encoder representation e. Let’s write the hidden states $[e_0,e_1,e_2]$ (and thus $e=e_2$).\n",
        "![alt text](https://guillaumegenthial.github.io/assets/img2latex/seq2seq_vanilla_encoder.svg)\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "_EAxeXCLl0rG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Decoder\n",
        "\n",
        "Now that we have a vector e that captures the meaning of the input sequence, we’ll use it to generate the target sequence word by word. Feed to another LSTM cell: $e$ as hidden state and a special start of sentence vector $w_{s o s}$ as input. The LSTM computes the next hidden state $h_0 ∈ R h$ . Then, we apply some function $g : R^h ↦ R^V$ so that \n",
        "$s_0 := g ( h_0 ) ∈ R^V$ is a vector of the same size as the vocabulary.\n",
        "\n",
        "\\begin{equation}\n",
        "h_0 = LSTM ( e , w_{s o s} ) \n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "s_0 = g ( h_0 )\n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "p_0 = softmax ( s_0 )\n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "i_0 = argmax ( p_0 )$\n",
        "\\end{equation}\n",
        "\n",
        "Then, apply a softmax to $s_ 0$ to normalize it into a vector of probabilities $p_0 ∈ R^V$ . Now, each entry of $p_0$ will measure how likely is each word in the vocabulary. Let’s say that the word “comment” has the highest probability (and thus $i_0 = argmax ( p_0 )$ corresponds to the index of “comment”). Get a corresponding vector $w_{i_0} = w_{comment}$ and repeat the procedure: the LSTM will take $h_0$ as hidden state and $w_{comment}$ as input and will output a probability vector $p_1$ over the second word, etc.\n",
        "\n",
        "\\begin{equation}\n",
        "h_1 = LSTM ( h_0 , w_{i_0} ) \n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "s_1 = g ( h_1 )\n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "p_1 = softmax ( s_1 ) \n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "i _1 = argmax ( p_1 )\n",
        "\\end{equation}\n",
        "\n",
        "The decoding stops when the predicted word is a special end of sentence token."
      ]
    },
    {
      "metadata": {
        "id": "dlvMrO4hJKL5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://guillaumegenthial.github.io/assets/img2latex/seq2seq_vanilla_decoder.svg)"
      ]
    },
    {
      "metadata": {
        "id": "6Q9-F7KRQY9B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Attention !!!\n",
        "\n",
        "![alt text](https://guillaumegenthial.github.io/assets/img2latex/seq2seq_attention_mechanism_new.svg)"
      ]
    },
    {
      "metadata": {
        "id": "17fibl9MJieb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Seq2Seq with Attention \n",
        "\n",
        "The previous model has been refined over the past few years and greatly benefited from what is known as attention. Attention is a mechanism that forces the model to learn to focus (= to attend) on specific parts of the input sequence when decoding, instead of relying only on the hidden vector of the decoder’s LSTM. One way of performing attention is as follows. We slightly modify the reccurrence formula that we defined above by adding a new vector $c_t$ to the input of the LSTM\n",
        "\\begin{equation}\n",
        "h_t = LSTM ( h_{t − 1} , [ w_{i_{t − 1 }}, c_t ] )\n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "s_t = g ( h_t )\n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        " p_t = softmax ( s_t )\n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        " i_t = argmax ( p_t )\n",
        "\\end{equation}\n",
        "\n",
        " The vector c_t is the attention (or context) vector. We compute a new context vector at each decoding step. First, with a function $f ( h_{t − 1} , e_{t ′} ) ↦ α t ′ ∈ R$ , compute a score for each hidden state $e_{t'}$ of the encoder. Then, normalize the sequence of $αt′$ using a softmax and compute c t as the weighted average of the $e_{t ′}$.\n",
        "\n",
        "$α_t ′ = f ( h_{t − 1 }, e_{t ′} ) ∈ R$                 for all $t ′$  \n",
        "\n",
        "$\\vec{\\alpha} = softmax ( α ) $\n",
        "\n",
        "$c_t = n \\sum_{t'=0}^{n}  \\vec{\\alpha}_{t′} e_{t ′}$\n",
        "\n",
        "The choice of the function  $f$ varies"
      ]
    },
    {
      "metadata": {
        "id": "8-U2JYj2lUha",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# One of the main usage of a sequence to sequence model is in Neural Machine Translation"
      ]
    },
    {
      "metadata": {
        "id": "PP8rGeJWlUha",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Let's now execute a basic NMT with Tensorflow seq2seq\n",
        "### The code might not seem very trivial at the first go!!!"
      ]
    },
    {
      "metadata": {
        "id": "7DJuSwkLlUhc",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "The helper file\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import copy\n",
        "import numpy as np\n",
        "\n",
        "CODES = {'<unk>': 0,'<s>': 1, '</s>': 2}\n",
        "\n",
        "def load_data(path):\n",
        "    \"\"\"\n",
        "    Load Dataset from File\n",
        "    \"\"\"\n",
        "    input_file = os.path.join(path)\n",
        "    with open(input_file, 'r', encoding='utf-8') as f:\n",
        "        data = f.read()\n",
        "\n",
        "    return data\n",
        "\n",
        "def preprocess_and_save_data(source_path, target_path):\n",
        "    \"\"\"\n",
        "    Preprocess Text Data.  Save to to file.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Preprocess\n",
        "    source_text = load_data(source_path)\n",
        "    target_text = load_data(target_path)\n",
        "\n",
        "    source_text = source_text.lower()\n",
        "    target_text = target_text.lower()\n",
        "\n",
        "    source_vocab_to_int, source_int_to_vocab = create_lookup_tables(source_text)\n",
        "    \n",
        "    target_vocab_to_int, target_int_to_vocab = create_lookup_tables(target_text)\n",
        "    \n",
        "    source_text, target_text = text_to_ids(source_text, target_text, source_vocab_to_int, target_vocab_to_int)\n",
        "\n",
        "    # Save Data\n",
        "    pickle.dump((\n",
        "        (source_text, target_text),\n",
        "        (source_vocab_to_int, target_vocab_to_int),\n",
        "        (source_int_to_vocab, target_int_to_vocab)), open('preprocess.p', 'wb'))\n",
        "\n",
        "def load_preprocess():\n",
        "    \"\"\"\n",
        "    Load the Preprocessed Training data and return them in batches of <batch_size> or less\n",
        "    \"\"\"\n",
        "    return pickle.load(open('preprocess.p', mode='rb'))\n",
        "\n",
        "def create_lookup_tables(text):\n",
        "    \"\"\"\n",
        "    Create lookup tables for vocabulary\n",
        "    \"\"\"\n",
        "    vocab = set(text.split())\n",
        "    vocab_to_int = copy.copy(CODES)\n",
        "    \n",
        "    for v_i, v in enumerate(vocab, len(CODES)):\n",
        "        vocab_to_int[v] = v_i\n",
        "\n",
        "    int_to_vocab = {v_i: v for v, v_i in vocab_to_int.items()}\n",
        "\n",
        "    return vocab_to_int, int_to_vocab\n",
        "\n",
        "def save_params(params):\n",
        "    \"\"\"\n",
        "    Save parameters to file\n",
        "    \"\"\"\n",
        "    pickle.dump(params, open('params.p', 'wb'))\n",
        "\n",
        "def load_params():\n",
        "    \"\"\"\n",
        "    Load parameters from file\n",
        "    \"\"\"\n",
        "    return pickle.load(open('params.p', mode='rb'))\n",
        "\n",
        "def batch_data(source, target, batch_size):\n",
        "    \"\"\"\n",
        "    Batch source and target together\n",
        "    \"\"\"\n",
        "    for batch_i in range(0, len(source)//batch_size):\n",
        "        start_i = batch_i * batch_size\n",
        "        source_batch = source[start_i:start_i + batch_size]\n",
        "        target_batch = target[start_i:start_i + batch_size]\n",
        "        yield np.array(pad_sentence_batch(source_batch)), np.array(pad_sentence_batch(target_batch))\n",
        "\n",
        "def pad_sentence_batch(sentence_batch):\n",
        "    \"\"\"\n",
        "    Pad sentence with </s> id\n",
        "    \"\"\"\n",
        "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
        "    return [sentence + [CODES['</s>']] * (max_sentence - len(sentence))\n",
        "            for sentence in sentence_batch]\n",
        "\n",
        "def text_to_ids(source_text, target_text, source_vocab_to_int, target_vocab_to_int):\n",
        "    \"\"\"\n",
        "    Convert source and target text to proper word ids\n",
        "    :param source_text: String that contains all the source text.\n",
        "    :param target_text: String that contains all the target text.\n",
        "    :param source_vocab_to_int: Dictionary to go from the source words to an id\n",
        "    :param target_vocab_to_int: Dictionary to go from the target words to an id\n",
        "    :return: A tuple of lists (source_id_text, target_id_text)\n",
        "    \"\"\"\n",
        "    source_text_to_id = [[source_vocab_to_int[word] for word in line.split()] for line in source_text.split('\\n')]\n",
        "    target_text_to_id = [[target_vocab_to_int[word] for word in line.split()] for line in target_text.split('\\n')]\n",
        "    \n",
        "    return (source_text_to_id, target_text_to_id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D_2OnIVblUhf",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "source_path = 'data/small_vocab_fr'\n",
        "target_path = 'data/small_vocab_en'\n",
        "source_text = load_data(source_path)\n",
        "target_text = load_data(target_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d3isfDhclUhi",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "preprocess_and_save_data(source_path, target_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z43_PWP7lUhk",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from distutils.version import LooseVersion\n",
        "import warnings\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kSaIyps7lUhm",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "(source_int_text, target_int_text), (source_vocab_to_int, target_vocab_to_int), _ = load_preprocess()\n",
        "# pad_sentence_batch(source_int_text)\n",
        "source_vocab = len(source_vocab_to_int)\n",
        "target_vocab = len(target_vocab_to_int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x9Mp5XQwlUhn",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class Seq2seqHyperparams(object):\n",
        "    def __init__(self, hidden_units=256, n_layers_enconder=2,\n",
        "                 n_layers_decoder=2, num_encoder_symbols=source_vocab, \n",
        "                 num_decoder_symbols=target_vocab, learning_rate=0.01,\n",
        "                 embedding_size=15, max_gradient_norm=5.0, dtype=tf.float32,\n",
        "                 epochs=1, dropout=0.2, forget_bias=1.0,\n",
        "                 use_beam_search=True, beam_width=10, length_penalty_weight=0.0,\n",
        "                 use_attention=True, learning_rate_decay=False, \n",
        "                 use_bidirectional_enconder=False):\n",
        "    \n",
        "        self.hidden_units = hidden_units\n",
        "        self.n_layers_enconder = n_layers_enconder\n",
        "        self.n_layers_decoder = n_layers_decoder\n",
        "        self.num_encoder_symbols = num_encoder_symbols\n",
        "        self.num_decoder_symbols = num_decoder_symbols\n",
        "        self.learning_rate = learning_rate\n",
        "        self.embedding_size = embedding_size\n",
        "        self.max_gradient_norm = max_gradient_norm\n",
        "        self.dtype = dtype\n",
        "        self.dropout = dropout\n",
        "        self.forget_bias = forget_bias\n",
        "        self.use_beam_search = use_beam_search\n",
        "        self.beam_width = beam_width\n",
        "        self.length_penalty_weight = length_penalty_weight\n",
        "        self.use_attention = use_attention\n",
        "        self.learning_rate_decay = learning_rate_decay\n",
        "        self.use_bidirectional_enconder = use_bidirectional_enconder\n",
        "\n",
        "\n",
        "        # Extra vocabulary symbols\n",
        "        unk = '<unk>'\n",
        "        sos = '<s>'\n",
        "        eos = '</s>' # also function as PAD\n",
        "        self.extra_tokens = [unk, sos, eos]\n",
        "        self.unk_token = self.extra_tokens.index(unk) #unk_token = 0\n",
        "        self.start_token = self.extra_tokens.index(sos) # start_token = 1\n",
        "        self.end_token = self.extra_tokens.index(eos)   # end_token = 2\n",
        "\n",
        "hparams = Seq2seqHyperparams()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YZhpTVkVlUhq",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow.contrib.seq2seq as seq2seq\n",
        "from tensorflow.contrib.rnn import MultiRNNCell\n",
        "from tensorflow import layers\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "train_graph = tf.Graph()\n",
        "with train_graph.as_default():\n",
        "    \n",
        "    ### DEFINING PLACEHOLDERS ###\n",
        "\n",
        "    # encoder_inputs: [batch_size, max_time_steps]\n",
        "    encoder_inputs = tf.placeholder(dtype=tf.int32,\n",
        "                shape=(None, None), name='encoder_inputs')\n",
        "\n",
        "    # encoder_inputs_length: [batch_size]\n",
        "    encoder_inputs_length = tf.placeholder(\n",
        "                dtype=tf.int32, shape=(None,), name='encoder_inputs_length')\n",
        "\n",
        "    # get dynamic batch_size\n",
        "    batch_size = tf.shape(encoder_inputs)[0]\n",
        "\n",
        "    ### TRAIN MODE PLACEHOLDERS ###\n",
        "\n",
        "    # decoder_inputs: [batch_size, max_time_steps]\n",
        "    decoder_inputs = tf.placeholder(\n",
        "                    dtype=tf.int32, shape=(None, None), name='decoder_inputs')\n",
        "\n",
        "    # decoder_inputs_length: [batch_size]\n",
        "    decoder_inputs_length = tf.placeholder(\n",
        "                    dtype=tf.int32, shape=(None,), name='decoder_inputs_length')\n",
        "\n",
        "    decoder_start_token = tf.ones(\n",
        "                    shape=[batch_size, 1], dtype=tf.int32) * hparams.start_token\n",
        "    decoder_end_token = tf.ones(\n",
        "                    shape=[batch_size, 1], dtype=tf.int32) * hparams.end_token  \n",
        "\n",
        "\n",
        "    # decoder_inputs_train: [batch_size , max_time_steps + 1]\n",
        "    # insert sos symbol in front of each decoder input\n",
        "    decoder_inputs_train = tf.concat([decoder_start_token,\n",
        "                                          decoder_inputs], axis=1)\n",
        "\n",
        "    # decoder_inputs_length_train: [batch_size]\n",
        "    decoder_inputs_length_train = decoder_inputs_length + 1\n",
        "\n",
        "    # decoder_targets_train: [batch_size, max_time_steps + 1]\n",
        "    # insert eos symbol at the end of each decoder input\n",
        "    decoder_targets_train = tf.concat([decoder_inputs,\n",
        "                                           decoder_end_token], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TgiOZGrAlUht",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "with train_graph.as_default():\n",
        "    ## DEFINING ENCODER ##\n",
        "\n",
        "    encoder_embeddings = tf.Variable(tf.random_uniform([hparams.num_encoder_symbols, hparams.embedding_size], -1.0, 1.0),\n",
        "                                     dtype=hparams.dtype)\n",
        "\n",
        "    # Embedded_inputs: [batch_size, time_step, embedding_size]\n",
        "    encoder_inputs_embedded = tf.nn.embedding_lookup(\n",
        "        params=encoder_embeddings, ids=encoder_inputs)\n",
        "\n",
        "    if hparams.use_bidirectional_enconder:  #bidirectional encoder is not working!\n",
        "        \n",
        "        num_bi_layers = int(hparams.n_layers_enconder / 2)\n",
        "        num_residual_layers = hparams.n_layers_enconder - 1\n",
        "        num_bi_residual_layers = int(num_residual_layers / 2)\n",
        "        \n",
        "        print(num_bi_layers, num_residual_layers, num_bi_residual_layers)\n",
        "        \n",
        "        cell_list = []\n",
        "        for i in range(hparams.n_layers_enconder):\n",
        "            cell = tf.contrib.rnn.BasicLSTMCell(hparams.hidden_units, forget_bias=hparams.forget_bias)\n",
        "\n",
        "            if (i >= hparams.n_layers_enconder - num_residual_layers):\n",
        "                cell = tf.contrib.rnn.ResidualWrapper(cell, residual_fn=None)\n",
        "                if hparams.dropout > 0.0:\n",
        "                    cell = tf.contrib.rnn.DropoutWrapper(\n",
        "                        cell=cell, input_keep_prob=(1.0 - hparams.dropout))\n",
        "            \n",
        "            cell_list.append(cell)\n",
        "            \n",
        "        if len(cell_list) == 1:  # Single layer.\n",
        "            fw_cell = cell_list[0]\n",
        "            bw_cell = cell_list[0]\n",
        "        else:  # Multi layers\n",
        "            fw_cell = tf.contrib.rnn.MultiRNNCell(cell_list)\n",
        "            bw_cell = tf.contrib.rnn.MultiRNNCell(cell_list)\n",
        "\n",
        "        fw_cell = tf.contrib.rnn.BasicLSTMCell(hparams.n_layers_enconder)\n",
        "        bw_cell = tf.contrib.rnn.BasicLSTMCell(hparams.n_layers_enconder)\n",
        "\n",
        "        bi_outputs, bi_state = tf.nn.bidirectional_dynamic_rnn(\n",
        "                                                        fw_cell,\n",
        "                                                        bw_cell,\n",
        "                                                        encoder_inputs_embedded,\n",
        "                                                        dtype=dtype,\n",
        "                                                        sequence_length=encoder_inputs_length,\n",
        "                                                        time_major=False,\n",
        "                                                        swap_memory=True)\n",
        "        print(bi_outputs, \"\\n\\n\", bi_state)\n",
        "\n",
        "        encoder_outputs, bi_encoder_state = tf.concat(bi_outputs, -1), bi_state\n",
        "        \n",
        "        if num_bi_layers == 1:\n",
        "            encoder_last_state = bi_encoder_state\n",
        "        else:\n",
        "            # alternatively concat forward and backward states\n",
        "            encoder_state = []\n",
        "            for layer_id in range(num_bi_layers):\n",
        "                encoder_state.append(bi_encoder_state[0][layer_id])  # forward\n",
        "                encoder_state.append(bi_encoder_state[1][layer_id])  # backward\n",
        "            encoder_last_state = tuple(encoder_state)\n",
        "\n",
        "        encoder_state = bi_encoder_state\n",
        "        \n",
        "    else:\n",
        "        # Build RNN cell\n",
        "        cells = []\n",
        "        for _ in range(hparams.n_layers_enconder):\n",
        "            cell = tf.contrib.rnn.BasicLSTMCell(hparams.hidden_units, forget_bias=hparams.forget_bias)\n",
        "            if hparams.dropout > 0.0:\n",
        "                cell = tf.contrib.rnn.DropoutWrapper(\n",
        "                    cell=cell, input_keep_prob=(1.0 - hparams.dropout))\n",
        "            cells.append(cell)\n",
        "        if hparams.n_layers_enconder == 1:\n",
        "            encoder_cells = cells[0]\n",
        "        else:\n",
        "            encoder_cells = tf.contrib.rnn.MultiRNNCell(cells)\n",
        "\n",
        "        encoder_outputs, encoder_last_state = tf.nn.dynamic_rnn(\n",
        "            cell=encoder_cells, inputs=encoder_inputs_embedded,\n",
        "            sequence_length=encoder_inputs_length, dtype=hparams.dtype,\n",
        "            time_major=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dT15EeAzlUhw",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "with train_graph.as_default():\n",
        "    ### DEFINING DECODER ###\n",
        "\n",
        "    # Building decoder_cell\n",
        "    cells = []\n",
        "    # Build RNN cell\n",
        "    for _ in range(hparams.n_layers_decoder):\n",
        "        cell = tf.contrib.rnn.BasicLSTMCell(hparams.hidden_units, forget_bias=hparams.forget_bias)\n",
        "        if hparams.dropout > 0.0:\n",
        "            cell = tf.contrib.rnn.DropoutWrapper(\n",
        "                cell=cell, input_keep_prob=(1.0 - hparams.dropout))\n",
        "        cells.append(cell)\n",
        "    if hparams.n_layers_decoder == 1:\n",
        "        decoder_cells = cells[0]\n",
        "    else:\n",
        "        decoder_cells = tf.contrib.rnn.MultiRNNCell(cells)\n",
        "\n",
        "    if hparams.use_attention:\n",
        "        memory = encoder_outputs\n",
        "        \n",
        "        attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
        "            hparams.hidden_units,\n",
        "            memory,\n",
        "            memory_sequence_length=encoder_inputs_length,\n",
        "            normalize=True)\n",
        "        \n",
        "        decoder_cells_train = tf.contrib.seq2seq.AttentionWrapper(\n",
        "            decoder_cells,\n",
        "            attention_mechanism,\n",
        "            attention_layer_size=hparams.hidden_units,\n",
        "            alignment_history=False,\n",
        "            output_attention=True,\n",
        "            name=\"attention\")\n",
        "        \n",
        "        decoder_initial_state = decoder_cells_train.zero_state(batch_size, hparams.dtype).clone(\n",
        "          cell_state=encoder_last_state)\n",
        "        \n",
        "    else:\n",
        "        decoder_cells_train = decoder_cells\n",
        "        decoder_initial_state = encoder_last_state\n",
        "\n",
        "    decoder_embeddings = tf.Variable(tf.random_uniform([hparams.num_decoder_symbols, hparams.embedding_size], -1.0, 1.0), dtype=hparams.dtype)\n",
        "    \n",
        "    # decoder_inputs_embedded: [batch_size, max_time_step + 1, embedding_size]\n",
        "    decoder_inputs_embedded = tf.nn.embedding_lookup(\n",
        "        params=decoder_embeddings, ids=decoder_inputs_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9qNfyOWNlUhz",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "with train_graph.as_default():\n",
        "    ### TRAIN MODE ###\n",
        "    \n",
        "    # Helper to feed inputs for training: read inputs from dense ground truth vectors\n",
        "    training_helper = seq2seq.TrainingHelper(inputs=decoder_inputs_embedded,\n",
        "                                       sequence_length=decoder_inputs_length_train,\n",
        "                                       time_major=False,\n",
        "                                        name='training_helper')\n",
        "\n",
        "    training_decoder = seq2seq.BasicDecoder(cell=decoder_cells_train,\n",
        "                                       helper=training_helper,\n",
        "                                       initial_state=decoder_initial_state)\n",
        "\n",
        "    # decoder_outputs_train: BasicDecoderOutput\n",
        "    #                        namedtuple(rnn_outputs, sample_id)\n",
        "    # decoder_outputs_train.rnn_output: [batch_size, max_time_step + 1, num_decoder_symbols] if output_time_major=False\n",
        "    #                                   [max_time_step + 1, batch_size, num_decoder_symbols] if output_time_major=True\n",
        "    # decoder_outputs_train.sample_id: [batch_size], tf.int32\n",
        "    (decoder_outputs_train, decoder_last_state_train, \n",
        "         decoder_outputs_length_decode)  = seq2seq.dynamic_decode(decoder=training_decoder,\n",
        "                                                        output_time_major=False,\n",
        "                                                        swap_memory=True,\n",
        "                                                        impute_finished=True)\n",
        "\n",
        "    # More efficient to do the projection on the batch-time-concatenated tensor\n",
        "    # logits_train: [batch_size, max_time_step + 1, num_decoder_symbols]\n",
        "    \n",
        "    sample_id = decoder_outputs_train.sample_id\n",
        "    \n",
        "    output_layer = layers.Dense(hparams.num_decoder_symbols, name='output_projection')\n",
        "    logits_train = output_layer(decoder_outputs_train.rnn_output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p2j3PnCglUh1",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "with train_graph.as_default():\n",
        "    \n",
        "    ### LOSS, GRADIENT AND OPTIMIZATION ###\n",
        "    \n",
        "    if hparams.learning_rate_decay:\n",
        "        global_step = tf.Variable(0, trainable=False)\n",
        "\n",
        "        learning_rate = tf.constant(hparams.learning_rate)\n",
        "\n",
        "        #using luong10 decay scheme\n",
        "        decay_factor = 0.5\n",
        "        start_decay_step = int(hparams.epochs / 2)\n",
        "        decay_times = 10\n",
        "\n",
        "        remain_steps = hparams.epochs - start_decay_step\n",
        "        decay_steps = int(remain_steps / decay_times)\n",
        "\n",
        "        learning_rate = tf.cond(global_step < start_decay_step,\n",
        "                                lambda: hparams.learning_rate,\n",
        "                                lambda: tf.train.exponential_decay(\n",
        "                                    hparams.learning_rate,\n",
        "                                    (global_step - start_decay_step),\n",
        "                                    decay_steps, decay_factor, staircase=True),\n",
        "                                name=\"learning_rate_decay_cond\")\n",
        "    \n",
        "    # Maximum decoder time_steps in current batch\n",
        "    max_decoder_length = tf.reduce_max(decoder_inputs_length_train)\n",
        "    \n",
        "    # masks: masking for valid and padded time steps, [batch_size, max_time_step + 1]\n",
        "    target_weights = tf.sequence_mask(lengths=decoder_inputs_length_train, \n",
        "                             maxlen=max_decoder_length, dtype=hparams.dtype, name='masks')\n",
        "    \n",
        "    crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "        labels=decoder_targets_train, logits=logits_train)\n",
        "    \n",
        "    loss = (tf.reduce_sum(crossent * target_weights) /\n",
        "        tf.cast(batch_size, dtype=hparams.dtype))\n",
        "\n",
        "    trainable_params = tf.trainable_variables()\n",
        "    \n",
        "    opt = tf.train.AdamOptimizer(learning_rate=hparams.learning_rate)\n",
        "    \n",
        "    gradients = tf.gradients(loss, \n",
        "                             trainable_params)\n",
        "    \n",
        "    clip_gradients, gradient_norm = tf.clip_by_global_norm(gradients, hparams.max_gradient_norm)\n",
        "    \n",
        "    updates = opt.apply_gradients(\n",
        "            zip(clip_gradients, trainable_params))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WOUO15lnlUh3",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "with train_graph.as_default():\n",
        "\n",
        "    ### INFERENCE MODE ###\n",
        "    start_tokens = tf.fill([batch_size], hparams.start_token)\n",
        "    \n",
        "    decoder_initial_state_infer = tf.contrib.seq2seq.tile_batch(\n",
        "                  encoder_last_state, multiplier=hparams.beam_width)\n",
        "    \n",
        "    if hparams.use_attention:\n",
        "        memory = tf.contrib.seq2seq.tile_batch(\n",
        "          memory, multiplier=hparams.beam_width)\n",
        "        \n",
        "        source_sequence_length = tf.contrib.seq2seq.tile_batch(\n",
        "          encoder_inputs_length, multiplier=hparams.beam_width)\n",
        "        \n",
        "        encoder_last_state = tf.contrib.seq2seq.tile_batch(\n",
        "          encoder_last_state, multiplier=hparams.beam_width)\n",
        "        \n",
        "        batch_size = batch_size * hparams.beam_width\n",
        "        \n",
        "        attention_mechanism_infer = tf.contrib.seq2seq.BahdanauAttention(\n",
        "            hparams.hidden_units,\n",
        "            memory,\n",
        "            memory_sequence_length=source_sequence_length,\n",
        "            normalize=True)\n",
        "        \n",
        "        decoder_cells_infer = tf.contrib.seq2seq.AttentionWrapper(\n",
        "            decoder_cells,\n",
        "            attention_mechanism_infer,\n",
        "            attention_layer_size=hparams.hidden_units,\n",
        "            alignment_history=False,\n",
        "            output_attention=True,\n",
        "            name=\"attention_infer\")\n",
        "        \n",
        "        decoder_initial_state_infer = decoder_cells_infer.zero_state(batch_size, hparams.dtype).clone(\n",
        "          cell_state=encoder_last_state)\n",
        "    \n",
        "    if hparams.use_beam_search:\n",
        "\n",
        "        inference_decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
        "              cell=decoder_cells_infer,\n",
        "              embedding=decoder_embeddings,\n",
        "              start_tokens=start_tokens,\n",
        "              end_token=hparams.end_token,\n",
        "              initial_state=decoder_initial_state_infer,\n",
        "              beam_width=hparams.beam_width,\n",
        "              output_layer=output_layer,\n",
        "              length_penalty_weight=hparams.length_penalty_weight)\n",
        "        \n",
        "    else:\n",
        "        inference_helper = seq2seq.GreedyEmbeddingHelper(decoder_embeddings,\n",
        "                                                        start_tokens=start_tokens,\n",
        "                                                        end_token=hparams.end_token)\n",
        "\n",
        "        inference_decoder = seq2seq.BasicDecoder(cell=decoder_cells_infer,\n",
        "                                                 helper=inference_helper,\n",
        "                                                 initial_state=decoder_initial_state,\n",
        "                                                 output_layer=output_layer)\n",
        "    \n",
        "    maximum_iterations = tf.round(tf.reduce_max(encoder_inputs_length) * 2)\n",
        "    \n",
        "    (decoder_infer_outputs, decoder_infer_last_state,\n",
        "                 decoder_infer_outputs_length) = (seq2seq.dynamic_decode(\n",
        "                    decoder=inference_decoder,\n",
        "                    output_time_major=False,\n",
        "                    maximum_iterations=maximum_iterations))\n",
        "    \n",
        "    if hparams.use_beam_search:\n",
        "        decoder_pred_decode = decoder_infer_outputs.predicted_ids\n",
        "        tf.identity(decoder_pred_decode, 'decoder_pred_decode')\n",
        "    \n",
        "    else:\n",
        "        logits_infer = decoder_infer_outputs.rnn_output\n",
        "        sample_id_infer = decoder_infer_outputs.sample_id                                                                       "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5nVRFH0KlUh6",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#training parameters\n",
        "class TrainingHyperparams(object):\n",
        "    def __init__(self, epochs=20, batch_size=512):\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "train_hparams = TrainingHyperparams()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GWI_awXClUh9",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "outputId": "0ffa463c-1af9-4188-ded2-5bcb170631f2"
      },
      "cell_type": "code",
      "source": [
        "def sentence_to_seq(sentence, vocab_to_int):\n",
        "    \"\"\"\n",
        "    Convert a sentence to a sequence of ids\n",
        "    :param sentence: String\n",
        "    :param vocab_to_int: Dictionary to go from the words to an id\n",
        "    :return: List of word ids\n",
        "    \"\"\"\n",
        "    lower_case_words = [word.lower() for word in sentence.split()]\n",
        "    \n",
        "    word_id = [vocab_to_int.get(word, vocab_to_int['<unk>']) for word in lower_case_words]\n",
        "    \n",
        "    return word_id\n",
        "\n",
        "import time\n",
        "\n",
        "### TRAINING ###\n",
        "save_path = 'checkpoints/dev'\n",
        "\n",
        "train_source = source_int_text[train_hparams.batch_size:]\n",
        "train_target = target_int_text[train_hparams.batch_size:]\n",
        "\n",
        "valid_source = source_int_text[:train_hparams.batch_size]\n",
        "valid_target = target_int_text[:train_hparams.batch_size]\n",
        "\n",
        "get_accuracy_every = 30\n",
        "\n",
        "with tf.Session(graph=train_graph) as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    steps = 0\n",
        "    \n",
        "    for epoch_i in range(train_hparams.epochs):\n",
        "        \n",
        "        step = 0\n",
        "        for batch_i, (source_batch, target_batch) in enumerate(\n",
        "                batch_data(train_source, train_target, train_hparams.batch_size)):\n",
        "            start_time = time.time()\n",
        "            \n",
        "            source_batch_seq_lenght = []\n",
        "            for item in source_batch:\n",
        "                source_batch_seq_lenght.append(np.shape(item)[0])\n",
        "            \n",
        "            target_batch_seq_lenght = []\n",
        "            for item in target_batch:\n",
        "                target_batch_seq_lenght.append(np.shape(item)[0])\n",
        "                \n",
        "#             if (source_batch_seq_lenght[0] > 300): #OOM problems with datasets containing very large sentences\n",
        "#                 continue\n",
        "                \n",
        "            _, loss_val = sess.run(\n",
        "                [updates, loss],\n",
        "                {encoder_inputs: source_batch,\n",
        "                 decoder_inputs: target_batch,\n",
        "                encoder_inputs_length: source_batch_seq_lenght,\n",
        "                decoder_inputs_length: target_batch_seq_lenght})\n",
        "\n",
        "            print('Epoch {:>3} Batch {:>4}/{}, Loss: {:>6.3f}'\n",
        "                  .format(epoch_i, batch_i, len(source_int_text) // train_hparams.batch_size, loss_val))\n",
        "                \n",
        "            end_time = time.time()\n",
        "            \n",
        "    print(\"Training time: \", end_time - start_time)\n",
        "    # Save Model\n",
        "    saver = tf.train.Saver()\n",
        "    saver.save(sess, save_path)\n",
        "    print('Model Trained and Saved')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch   0 Batch    0/269, Loss: 97.877\n",
            "Epoch   0 Batch    1/269, Loss: 71.076\n",
            "Epoch   0 Batch    2/269, Loss: 114.789\n",
            "Epoch   0 Batch    3/269, Loss: 79.170\n",
            "Epoch   0 Batch    4/269, Loss: 101.692\n",
            "Epoch   0 Batch    5/269, Loss: 130.967\n",
            "Epoch   0 Batch    6/269, Loss: 79.163\n",
            "Epoch   0 Batch    7/269, Loss: 71.977\n",
            "Epoch   0 Batch    8/269, Loss: 85.193\n",
            "Epoch   0 Batch    9/269, Loss: 77.328\n",
            "Epoch   0 Batch   10/269, Loss: 77.700\n",
            "Epoch   0 Batch   11/269, Loss: 62.228\n",
            "Epoch   0 Batch   12/269, Loss: 58.238\n",
            "Epoch   0 Batch   13/269, Loss: 69.596\n",
            "Epoch   0 Batch   14/269, Loss: 61.465\n",
            "Epoch   0 Batch   15/269, Loss: 58.058\n",
            "Epoch   0 Batch   16/269, Loss: 56.366\n",
            "Epoch   0 Batch   17/269, Loss: 54.344\n",
            "Epoch   0 Batch   18/269, Loss: 52.447\n",
            "Epoch   0 Batch   19/269, Loss: 53.707\n",
            "Epoch   0 Batch   20/269, Loss: 49.734\n",
            "Epoch   0 Batch   21/269, Loss: 50.285\n",
            "Epoch   0 Batch   22/269, Loss: 56.243\n",
            "Epoch   0 Batch   23/269, Loss: 50.994\n",
            "Epoch   0 Batch   24/269, Loss: 50.204\n",
            "Epoch   0 Batch   25/269, Loss: 48.082\n",
            "Epoch   0 Batch   26/269, Loss: 46.900\n",
            "Epoch   0 Batch   27/269, Loss: 44.011\n",
            "Epoch   0 Batch   28/269, Loss: 45.238\n",
            "Epoch   0 Batch   29/269, Loss: 43.633\n",
            "Epoch   0 Batch   30/269, Loss: 44.279\n",
            "Epoch   0 Batch   31/269, Loss: 39.373\n",
            "Epoch   0 Batch   32/269, Loss: 41.731\n",
            "Epoch   0 Batch   33/269, Loss: 37.239\n",
            "Epoch   0 Batch   34/269, Loss: 37.863\n",
            "Epoch   0 Batch   35/269, Loss: 37.309\n",
            "Epoch   0 Batch   36/269, Loss: 34.464\n",
            "Epoch   0 Batch   37/269, Loss: 35.059\n",
            "Epoch   0 Batch   38/269, Loss: 32.321\n",
            "Epoch   0 Batch   39/269, Loss: 30.472\n",
            "Epoch   0 Batch   40/269, Loss: 29.467\n",
            "Epoch   0 Batch   41/269, Loss: 28.935\n",
            "Epoch   0 Batch   42/269, Loss: 27.649\n",
            "Epoch   0 Batch   43/269, Loss: 24.543\n",
            "Epoch   0 Batch   44/269, Loss: 24.135\n",
            "Epoch   0 Batch   45/269, Loss: 23.107\n",
            "Epoch   0 Batch   46/269, Loss: 22.814\n",
            "Epoch   0 Batch   47/269, Loss: 21.830\n",
            "Epoch   0 Batch   48/269, Loss: 21.299\n",
            "Epoch   0 Batch   49/269, Loss: 20.191\n",
            "Epoch   0 Batch   50/269, Loss: 20.397\n",
            "Epoch   0 Batch   51/269, Loss: 20.141\n",
            "Epoch   0 Batch   52/269, Loss: 18.591\n",
            "Epoch   0 Batch   53/269, Loss: 18.377\n",
            "Epoch   0 Batch   54/269, Loss: 17.431\n",
            "Epoch   0 Batch   55/269, Loss: 18.065\n",
            "Epoch   0 Batch   56/269, Loss: 17.408\n",
            "Epoch   0 Batch   57/269, Loss: 17.169\n",
            "Epoch   0 Batch   58/269, Loss: 16.505\n",
            "Epoch   0 Batch   59/269, Loss: 15.820\n",
            "Epoch   0 Batch   60/269, Loss: 15.875\n",
            "Epoch   0 Batch   61/269, Loss: 15.620\n",
            "Epoch   0 Batch   62/269, Loss: 15.803\n",
            "Epoch   0 Batch   63/269, Loss: 15.647\n",
            "Epoch   0 Batch   64/269, Loss: 15.254\n",
            "Epoch   0 Batch   65/269, Loss: 14.670\n",
            "Epoch   0 Batch   66/269, Loss: 15.186\n",
            "Epoch   0 Batch   67/269, Loss: 14.890\n",
            "Epoch   0 Batch   68/269, Loss: 15.189\n",
            "Epoch   0 Batch   69/269, Loss: 14.896\n",
            "Epoch   0 Batch   70/269, Loss: 14.707\n",
            "Epoch   0 Batch   71/269, Loss: 14.511\n",
            "Epoch   0 Batch   72/269, Loss: 14.655\n",
            "Epoch   0 Batch   73/269, Loss: 14.156\n",
            "Epoch   0 Batch   74/269, Loss: 13.814\n",
            "Epoch   0 Batch   75/269, Loss: 13.760\n",
            "Epoch   0 Batch   76/269, Loss: 14.022\n",
            "Epoch   0 Batch   77/269, Loss: 13.699\n",
            "Epoch   0 Batch   78/269, Loss: 13.838\n",
            "Epoch   0 Batch   79/269, Loss: 13.880\n",
            "Epoch   0 Batch   80/269, Loss: 13.848\n",
            "Epoch   0 Batch   81/269, Loss: 13.853\n",
            "Epoch   0 Batch   82/269, Loss: 13.339\n",
            "Epoch   0 Batch   83/269, Loss: 13.804\n",
            "Epoch   0 Batch   84/269, Loss: 13.507\n",
            "Epoch   0 Batch   85/269, Loss: 13.395\n",
            "Epoch   0 Batch   86/269, Loss: 13.412\n",
            "Epoch   0 Batch   87/269, Loss: 13.200\n",
            "Epoch   0 Batch   88/269, Loss: 12.947\n",
            "Epoch   0 Batch   89/269, Loss: 13.333\n",
            "Epoch   0 Batch   90/269, Loss: 13.296\n",
            "Epoch   0 Batch   91/269, Loss: 12.736\n",
            "Epoch   0 Batch   92/269, Loss: 13.020\n",
            "Epoch   0 Batch   93/269, Loss: 12.890\n",
            "Epoch   0 Batch   94/269, Loss: 12.963\n",
            "Epoch   0 Batch   95/269, Loss: 12.765\n",
            "Epoch   0 Batch   96/269, Loss: 12.781\n",
            "Epoch   0 Batch   97/269, Loss: 12.765\n",
            "Epoch   0 Batch   98/269, Loss: 12.838\n",
            "Epoch   0 Batch   99/269, Loss: 12.569\n",
            "Epoch   0 Batch  100/269, Loss: 12.428\n",
            "Epoch   0 Batch  101/269, Loss: 12.692\n",
            "Epoch   0 Batch  102/269, Loss: 12.599\n",
            "Epoch   0 Batch  103/269, Loss: 12.203\n",
            "Epoch   0 Batch  104/269, Loss: 12.390\n",
            "Epoch   0 Batch  105/269, Loss: 12.544\n",
            "Epoch   0 Batch  106/269, Loss: 12.199\n",
            "Epoch   0 Batch  107/269, Loss: 12.283\n",
            "Epoch   0 Batch  108/269, Loss: 12.052\n",
            "Epoch   0 Batch  109/269, Loss: 12.107\n",
            "Epoch   0 Batch  110/269, Loss: 11.887\n",
            "Epoch   0 Batch  111/269, Loss: 11.892\n",
            "Epoch   0 Batch  112/269, Loss: 12.000\n",
            "Epoch   0 Batch  113/269, Loss: 11.895\n",
            "Epoch   0 Batch  114/269, Loss: 11.775\n",
            "Epoch   0 Batch  115/269, Loss: 11.858\n",
            "Epoch   0 Batch  116/269, Loss: 11.633\n",
            "Epoch   0 Batch  117/269, Loss: 11.670\n",
            "Epoch   0 Batch  118/269, Loss: 11.558\n",
            "Epoch   0 Batch  119/269, Loss: 11.257\n",
            "Epoch   0 Batch  120/269, Loss: 11.425\n",
            "Epoch   0 Batch  121/269, Loss: 11.444\n",
            "Epoch   0 Batch  122/269, Loss: 11.146\n",
            "Epoch   0 Batch  123/269, Loss: 11.313\n",
            "Epoch   0 Batch  124/269, Loss: 11.408\n",
            "Epoch   0 Batch  125/269, Loss: 11.930\n",
            "Epoch   0 Batch  126/269, Loss: 11.279\n",
            "Epoch   0 Batch  127/269, Loss: 11.404\n",
            "Epoch   0 Batch  128/269, Loss: 11.168\n",
            "Epoch   0 Batch  129/269, Loss: 11.083\n",
            "Epoch   0 Batch  130/269, Loss: 10.865\n",
            "Epoch   0 Batch  131/269, Loss: 10.918\n",
            "Epoch   0 Batch  132/269, Loss: 11.113\n",
            "Epoch   0 Batch  133/269, Loss: 10.538\n",
            "Epoch   0 Batch  134/269, Loss: 10.448\n",
            "Epoch   0 Batch  135/269, Loss: 10.688\n",
            "Epoch   0 Batch  136/269, Loss: 10.465\n",
            "Epoch   0 Batch  137/269, Loss: 10.402\n",
            "Epoch   0 Batch  138/269, Loss: 10.382\n",
            "Epoch   0 Batch  139/269, Loss:  9.942\n",
            "Epoch   0 Batch  140/269, Loss: 10.094\n",
            "Epoch   0 Batch  141/269, Loss:  9.812\n",
            "Epoch   0 Batch  142/269, Loss:  9.491\n",
            "Epoch   0 Batch  143/269, Loss:  9.636\n",
            "Epoch   0 Batch  144/269, Loss:  9.337\n",
            "Epoch   0 Batch  145/269, Loss:  9.917\n",
            "Epoch   0 Batch  146/269, Loss:  9.996\n",
            "Epoch   0 Batch  147/269, Loss:  9.649\n",
            "Epoch   0 Batch  148/269, Loss:  9.473\n",
            "Epoch   0 Batch  149/269, Loss:  9.259\n",
            "Epoch   0 Batch  150/269, Loss:  9.254\n",
            "Epoch   0 Batch  151/269, Loss:  8.825\n",
            "Epoch   0 Batch  152/269, Loss:  9.044\n",
            "Epoch   0 Batch  153/269, Loss:  8.645\n",
            "Epoch   0 Batch  154/269, Loss:  8.256\n",
            "Epoch   0 Batch  155/269, Loss:  8.154\n",
            "Epoch   0 Batch  156/269, Loss:  8.335\n",
            "Epoch   0 Batch  157/269, Loss:  7.708\n",
            "Epoch   0 Batch  158/269, Loss:  7.709\n",
            "Epoch   0 Batch  159/269, Loss:  7.801\n",
            "Epoch   0 Batch  160/269, Loss:  7.529\n",
            "Epoch   0 Batch  161/269, Loss:  7.362\n",
            "Epoch   0 Batch  162/269, Loss:  8.065\n",
            "Epoch   0 Batch  163/269, Loss:  7.384\n",
            "Epoch   0 Batch  164/269, Loss:  6.987\n",
            "Epoch   0 Batch  165/269, Loss:  7.200\n",
            "Epoch   0 Batch  166/269, Loss:  7.711\n",
            "Epoch   0 Batch  167/269, Loss:  7.514\n",
            "Epoch   0 Batch  168/269, Loss:  6.999\n",
            "Epoch   0 Batch  169/269, Loss:  6.780\n",
            "Epoch   0 Batch  170/269, Loss:  6.398\n",
            "Epoch   0 Batch  171/269, Loss:  6.376\n",
            "Epoch   0 Batch  172/269, Loss:  6.193\n",
            "Epoch   0 Batch  173/269, Loss:  5.551\n",
            "Epoch   0 Batch  174/269, Loss:  5.461\n",
            "Epoch   0 Batch  175/269, Loss:  5.238\n",
            "Epoch   0 Batch  176/269, Loss:  4.820\n",
            "Epoch   0 Batch  177/269, Loss:  4.752\n",
            "Epoch   0 Batch  178/269, Loss:  4.268\n",
            "Epoch   0 Batch  179/269, Loss:  4.287\n",
            "Epoch   0 Batch  180/269, Loss:  3.676\n",
            "Epoch   0 Batch  181/269, Loss:  4.051\n",
            "Epoch   0 Batch  182/269, Loss:  3.571\n",
            "Epoch   0 Batch  183/269, Loss:  3.272\n",
            "Epoch   0 Batch  184/269, Loss:  2.973\n",
            "Epoch   0 Batch  185/269, Loss:  3.138\n",
            "Epoch   0 Batch  186/269, Loss:  2.932\n",
            "Epoch   0 Batch  187/269, Loss:  2.675\n",
            "Epoch   0 Batch  188/269, Loss:  2.696\n",
            "Epoch   0 Batch  189/269, Loss:  2.761\n",
            "Epoch   0 Batch  190/269, Loss:  2.321\n",
            "Epoch   0 Batch  191/269, Loss:  2.429\n",
            "Epoch   0 Batch  192/269, Loss:  2.287\n",
            "Epoch   0 Batch  193/269, Loss:  2.246\n",
            "Epoch   0 Batch  194/269, Loss:  2.117\n",
            "Epoch   0 Batch  195/269, Loss:  2.061\n",
            "Epoch   0 Batch  196/269, Loss:  1.856\n",
            "Epoch   0 Batch  197/269, Loss:  1.914\n",
            "Epoch   0 Batch  198/269, Loss:  1.814\n",
            "Epoch   0 Batch  199/269, Loss:  1.960\n",
            "Epoch   0 Batch  200/269, Loss:  1.710\n",
            "Epoch   0 Batch  201/269, Loss:  1.798\n",
            "Epoch   0 Batch  202/269, Loss:  1.821\n",
            "Epoch   0 Batch  203/269, Loss:  1.786\n",
            "Epoch   0 Batch  204/269, Loss:  1.610\n",
            "Epoch   0 Batch  205/269, Loss:  1.736\n",
            "Epoch   0 Batch  206/269, Loss:  1.742\n",
            "Epoch   0 Batch  207/269, Loss:  1.501\n",
            "Epoch   0 Batch  208/269, Loss:  1.588\n",
            "Epoch   0 Batch  209/269, Loss:  1.236\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch   0 Batch  210/269, Loss:  1.412\n",
            "Epoch   0 Batch  211/269, Loss:  1.338\n",
            "Epoch   0 Batch  212/269, Loss:  1.461\n",
            "Epoch   0 Batch  213/269, Loss:  1.526\n",
            "Epoch   0 Batch  214/269, Loss:  1.507\n",
            "Epoch   0 Batch  215/269, Loss:  1.403\n",
            "Epoch   0 Batch  216/269, Loss:  1.465\n",
            "Epoch   0 Batch  217/269, Loss:  1.329\n",
            "Epoch   0 Batch  218/269, Loss:  1.346\n",
            "Epoch   0 Batch  219/269, Loss:  1.348\n",
            "Epoch   0 Batch  220/269, Loss:  1.320\n",
            "Epoch   0 Batch  221/269, Loss:  1.263\n",
            "Epoch   0 Batch  222/269, Loss:  1.159\n",
            "Epoch   0 Batch  223/269, Loss:  1.210\n",
            "Epoch   0 Batch  224/269, Loss:  1.399\n",
            "Epoch   0 Batch  225/269, Loss:  1.172\n",
            "Epoch   0 Batch  226/269, Loss:  1.337\n",
            "Epoch   0 Batch  227/269, Loss:  1.211\n",
            "Epoch   0 Batch  228/269, Loss:  1.163\n",
            "Epoch   0 Batch  229/269, Loss:  1.116\n",
            "Epoch   0 Batch  230/269, Loss:  1.206\n",
            "Epoch   0 Batch  231/269, Loss:  1.157\n",
            "Epoch   0 Batch  232/269, Loss:  1.074\n",
            "Epoch   0 Batch  233/269, Loss:  1.142\n",
            "Epoch   0 Batch  234/269, Loss:  1.160\n",
            "Epoch   0 Batch  235/269, Loss:  1.175\n",
            "Epoch   0 Batch  236/269, Loss:  1.111\n",
            "Epoch   0 Batch  237/269, Loss:  1.049\n",
            "Epoch   0 Batch  238/269, Loss:  1.180\n",
            "Epoch   0 Batch  239/269, Loss:  1.026\n",
            "Epoch   0 Batch  240/269, Loss:  1.069\n",
            "Epoch   0 Batch  241/269, Loss:  1.234\n",
            "Epoch   0 Batch  242/269, Loss:  1.046\n",
            "Epoch   0 Batch  243/269, Loss:  0.966\n",
            "Epoch   0 Batch  244/269, Loss:  1.067\n",
            "Epoch   0 Batch  245/269, Loss:  1.089\n",
            "Epoch   0 Batch  246/269, Loss:  1.178\n",
            "Epoch   0 Batch  247/269, Loss:  1.130\n",
            "Epoch   0 Batch  248/269, Loss:  0.947\n",
            "Epoch   0 Batch  249/269, Loss:  1.007\n",
            "Epoch   0 Batch  250/269, Loss:  0.918\n",
            "Epoch   0 Batch  251/269, Loss:  0.962\n",
            "Epoch   0 Batch  252/269, Loss:  0.909\n",
            "Epoch   0 Batch  253/269, Loss:  0.993\n",
            "Epoch   0 Batch  254/269, Loss:  1.041\n",
            "Epoch   0 Batch  255/269, Loss:  1.088\n",
            "Epoch   0 Batch  256/269, Loss:  0.969\n",
            "Epoch   0 Batch  257/269, Loss:  1.076\n",
            "Epoch   0 Batch  258/269, Loss:  1.033\n",
            "Epoch   0 Batch  259/269, Loss:  1.010\n",
            "Epoch   0 Batch  260/269, Loss:  0.989\n",
            "Epoch   0 Batch  261/269, Loss:  0.939\n",
            "Epoch   0 Batch  262/269, Loss:  1.006\n",
            "Epoch   0 Batch  263/269, Loss:  1.060\n",
            "Epoch   0 Batch  264/269, Loss:  0.932\n",
            "Epoch   0 Batch  265/269, Loss:  0.934\n",
            "Epoch   0 Batch  266/269, Loss:  0.893\n",
            "Epoch   0 Batch  267/269, Loss:  1.028\n",
            "Epoch   1 Batch    0/269, Loss:  0.977\n",
            "Epoch   1 Batch    1/269, Loss:  0.842\n",
            "Epoch   1 Batch    2/269, Loss:  1.029\n",
            "Epoch   1 Batch    3/269, Loss:  0.958\n",
            "Epoch   1 Batch    4/269, Loss:  0.864\n",
            "Epoch   1 Batch    5/269, Loss:  0.866\n",
            "Epoch   1 Batch    6/269, Loss:  0.861\n",
            "Epoch   1 Batch    7/269, Loss:  0.979\n",
            "Epoch   1 Batch    8/269, Loss:  0.898\n",
            "Epoch   1 Batch    9/269, Loss:  0.914\n",
            "Epoch   1 Batch   10/269, Loss:  0.753\n",
            "Epoch   1 Batch   11/269, Loss:  0.953\n",
            "Epoch   1 Batch   12/269, Loss:  0.910\n",
            "Epoch   1 Batch   13/269, Loss:  0.843\n",
            "Epoch   1 Batch   14/269, Loss:  0.892\n",
            "Epoch   1 Batch   15/269, Loss:  0.951\n",
            "Epoch   1 Batch   16/269, Loss:  0.907\n",
            "Epoch   1 Batch   17/269, Loss:  0.866\n",
            "Epoch   1 Batch   18/269, Loss:  0.796\n",
            "Epoch   1 Batch   19/269, Loss:  0.885\n",
            "Epoch   1 Batch   20/269, Loss:  0.814\n",
            "Epoch   1 Batch   21/269, Loss:  0.883\n",
            "Epoch   1 Batch   22/269, Loss:  0.860\n",
            "Epoch   1 Batch   23/269, Loss:  0.940\n",
            "Epoch   1 Batch   24/269, Loss:  0.921\n",
            "Epoch   1 Batch   25/269, Loss:  0.828\n",
            "Epoch   1 Batch   26/269, Loss:  0.776\n",
            "Epoch   1 Batch   27/269, Loss:  0.864\n",
            "Epoch   1 Batch   28/269, Loss:  0.906\n",
            "Epoch   1 Batch   29/269, Loss:  0.801\n",
            "Epoch   1 Batch   30/269, Loss:  0.807\n",
            "Epoch   1 Batch   31/269, Loss:  0.882\n",
            "Epoch   1 Batch   32/269, Loss:  0.797\n",
            "Epoch   1 Batch   33/269, Loss:  0.875\n",
            "Epoch   1 Batch   34/269, Loss:  0.742\n",
            "Epoch   1 Batch   35/269, Loss:  0.959\n",
            "Epoch   1 Batch   36/269, Loss:  0.879\n",
            "Epoch   1 Batch   37/269, Loss:  0.758\n",
            "Epoch   1 Batch   38/269, Loss:  0.838\n",
            "Epoch   1 Batch   39/269, Loss:  0.804\n",
            "Epoch   1 Batch   40/269, Loss:  0.770\n",
            "Epoch   1 Batch   41/269, Loss:  0.793\n",
            "Epoch   1 Batch   42/269, Loss:  0.819\n",
            "Epoch   1 Batch   43/269, Loss:  0.821\n",
            "Epoch   1 Batch   44/269, Loss:  0.797\n",
            "Epoch   1 Batch   45/269, Loss:  0.844\n",
            "Epoch   1 Batch   46/269, Loss:  0.771\n",
            "Epoch   1 Batch   47/269, Loss:  0.706\n",
            "Epoch   1 Batch   48/269, Loss:  0.735\n",
            "Epoch   1 Batch   49/269, Loss:  0.783\n",
            "Epoch   1 Batch   50/269, Loss:  0.731\n",
            "Epoch   1 Batch   51/269, Loss:  0.721\n",
            "Epoch   1 Batch   52/269, Loss:  0.746\n",
            "Epoch   1 Batch   53/269, Loss:  0.795\n",
            "Epoch   1 Batch   54/269, Loss:  0.665\n",
            "Epoch   1 Batch   55/269, Loss:  0.699\n",
            "Epoch   1 Batch   56/269, Loss:  0.810\n",
            "Epoch   1 Batch   57/269, Loss:  0.777\n",
            "Epoch   1 Batch   58/269, Loss:  0.839\n",
            "Epoch   1 Batch   59/269, Loss:  0.687\n",
            "Epoch   1 Batch   60/269, Loss:  0.738\n",
            "Epoch   1 Batch   61/269, Loss:  0.724\n",
            "Epoch   1 Batch   62/269, Loss:  0.784\n",
            "Epoch   1 Batch   63/269, Loss:  0.774\n",
            "Epoch   1 Batch   64/269, Loss:  0.680\n",
            "Epoch   1 Batch   65/269, Loss:  0.703\n",
            "Epoch   1 Batch   66/269, Loss:  0.797\n",
            "Epoch   1 Batch   67/269, Loss:  0.780\n",
            "Epoch   1 Batch   68/269, Loss:  0.981\n",
            "Epoch   1 Batch   69/269, Loss:  0.760\n",
            "Epoch   1 Batch   70/269, Loss:  0.896\n",
            "Epoch   1 Batch   71/269, Loss:  0.799\n",
            "Epoch   1 Batch   72/269, Loss:  0.829\n",
            "Epoch   1 Batch   73/269, Loss:  0.817\n",
            "Epoch   1 Batch   74/269, Loss:  0.713\n",
            "Epoch   1 Batch   75/269, Loss:  0.801\n",
            "Epoch   1 Batch   76/269, Loss:  0.744\n",
            "Epoch   1 Batch   77/269, Loss:  0.682\n",
            "Epoch   1 Batch   78/269, Loss:  0.745\n",
            "Epoch   1 Batch   79/269, Loss:  0.759\n",
            "Epoch   1 Batch   80/269, Loss:  0.736\n",
            "Epoch   1 Batch   81/269, Loss:  0.702\n",
            "Epoch   1 Batch   82/269, Loss:  0.703\n",
            "Epoch   1 Batch   83/269, Loss:  0.749\n",
            "Epoch   1 Batch   84/269, Loss:  0.750\n",
            "Epoch   1 Batch   85/269, Loss:  0.661\n",
            "Epoch   1 Batch   86/269, Loss:  0.696\n",
            "Epoch   1 Batch   87/269, Loss:  0.690\n",
            "Epoch   1 Batch   88/269, Loss:  0.679\n",
            "Epoch   1 Batch   89/269, Loss:  0.713\n",
            "Epoch   1 Batch   90/269, Loss:  0.698\n",
            "Epoch   1 Batch   91/269, Loss:  0.692\n",
            "Epoch   1 Batch   92/269, Loss:  0.709\n",
            "Epoch   1 Batch   93/269, Loss:  0.662\n",
            "Epoch   1 Batch   94/269, Loss:  0.767\n",
            "Epoch   1 Batch   95/269, Loss:  0.679\n",
            "Epoch   1 Batch   96/269, Loss:  0.687\n",
            "Epoch   1 Batch   97/269, Loss:  0.669\n",
            "Epoch   1 Batch   98/269, Loss:  0.638\n",
            "Epoch   1 Batch   99/269, Loss:  0.711\n",
            "Epoch   1 Batch  100/269, Loss:  0.664\n",
            "Epoch   1 Batch  101/269, Loss:  0.680\n",
            "Epoch   1 Batch  102/269, Loss:  0.727\n",
            "Epoch   1 Batch  103/269, Loss:  0.729\n",
            "Epoch   1 Batch  104/269, Loss:  0.664\n",
            "Epoch   1 Batch  105/269, Loss:  0.685\n",
            "Epoch   1 Batch  106/269, Loss:  0.633\n",
            "Epoch   1 Batch  107/269, Loss:  0.671\n",
            "Epoch   1 Batch  108/269, Loss:  0.634\n",
            "Epoch   1 Batch  109/269, Loss:  0.778\n",
            "Epoch   1 Batch  110/269, Loss:  0.672\n",
            "Epoch   1 Batch  111/269, Loss:  0.679\n",
            "Epoch   1 Batch  112/269, Loss:  0.708\n",
            "Epoch   1 Batch  113/269, Loss:  0.687\n",
            "Epoch   1 Batch  114/269, Loss:  0.695\n",
            "Epoch   1 Batch  115/269, Loss:  0.686\n",
            "Epoch   1 Batch  116/269, Loss:  0.708\n",
            "Epoch   1 Batch  117/269, Loss:  0.656\n",
            "Epoch   1 Batch  118/269, Loss:  0.617\n",
            "Epoch   1 Batch  119/269, Loss:  0.740\n",
            "Epoch   1 Batch  120/269, Loss:  0.673\n",
            "Epoch   1 Batch  121/269, Loss:  0.702\n",
            "Epoch   1 Batch  122/269, Loss:  0.632\n",
            "Epoch   1 Batch  123/269, Loss:  0.658\n",
            "Epoch   1 Batch  124/269, Loss:  0.640\n",
            "Epoch   1 Batch  125/269, Loss:  0.738\n",
            "Epoch   1 Batch  126/269, Loss:  0.680\n",
            "Epoch   1 Batch  127/269, Loss:  0.652\n",
            "Epoch   1 Batch  128/269, Loss:  0.614\n",
            "Epoch   1 Batch  129/269, Loss:  0.572\n",
            "Epoch   1 Batch  130/269, Loss:  0.639\n",
            "Epoch   1 Batch  131/269, Loss:  0.663\n",
            "Epoch   1 Batch  132/269, Loss:  0.641\n",
            "Epoch   1 Batch  133/269, Loss:  0.606\n",
            "Epoch   1 Batch  134/269, Loss:  0.604\n",
            "Epoch   1 Batch  135/269, Loss:  0.649\n",
            "Epoch   1 Batch  136/269, Loss:  0.622\n",
            "Epoch   1 Batch  137/269, Loss:  0.688\n",
            "Epoch   1 Batch  138/269, Loss:  0.638\n",
            "Epoch   1 Batch  139/269, Loss:  0.672\n",
            "Epoch   1 Batch  140/269, Loss:  0.652\n",
            "Epoch   1 Batch  141/269, Loss:  0.696\n",
            "Epoch   1 Batch  142/269, Loss:  0.608\n",
            "Epoch   1 Batch  143/269, Loss:  0.546\n",
            "Epoch   1 Batch  144/269, Loss:  0.702\n",
            "Epoch   1 Batch  145/269, Loss:  0.617\n",
            "Epoch   1 Batch  146/269, Loss:  0.643\n",
            "Epoch   1 Batch  147/269, Loss:  0.709\n",
            "Epoch   1 Batch  148/269, Loss:  0.563\n",
            "Epoch   1 Batch  149/269, Loss:  0.612\n",
            "Epoch   1 Batch  150/269, Loss:  0.634\n",
            "Epoch   1 Batch  151/269, Loss:  0.597\n",
            "Epoch   1 Batch  152/269, Loss:  0.581\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch   1 Batch  153/269, Loss:  0.618\n",
            "Epoch   1 Batch  154/269, Loss:  0.625\n",
            "Epoch   1 Batch  155/269, Loss:  0.590\n",
            "Epoch   1 Batch  156/269, Loss:  0.617\n",
            "Epoch   1 Batch  157/269, Loss:  0.615\n",
            "Epoch   1 Batch  158/269, Loss:  0.555\n",
            "Epoch   1 Batch  159/269, Loss:  0.606\n",
            "Epoch   1 Batch  160/269, Loss:  0.620\n",
            "Epoch   1 Batch  161/269, Loss:  0.590\n",
            "Epoch   1 Batch  162/269, Loss:  0.575\n",
            "Epoch   1 Batch  163/269, Loss:  0.596\n",
            "Epoch   1 Batch  164/269, Loss:  0.663\n",
            "Epoch   1 Batch  165/269, Loss:  0.648\n",
            "Epoch   1 Batch  166/269, Loss:  0.604\n",
            "Epoch   1 Batch  167/269, Loss:  0.646\n",
            "Epoch   1 Batch  168/269, Loss:  0.603\n",
            "Epoch   1 Batch  169/269, Loss:  0.586\n",
            "Epoch   1 Batch  170/269, Loss:  0.552\n",
            "Epoch   1 Batch  171/269, Loss:  0.608\n",
            "Epoch   1 Batch  172/269, Loss:  0.687\n",
            "Epoch   1 Batch  173/269, Loss:  0.577\n",
            "Epoch   1 Batch  174/269, Loss:  0.604\n",
            "Epoch   1 Batch  175/269, Loss:  0.685\n",
            "Epoch   1 Batch  176/269, Loss:  0.612\n",
            "Epoch   1 Batch  177/269, Loss:  0.529\n",
            "Epoch   1 Batch  178/269, Loss:  0.591\n",
            "Epoch   1 Batch  179/269, Loss:  0.583\n",
            "Epoch   1 Batch  180/269, Loss:  0.545\n",
            "Epoch   1 Batch  181/269, Loss:  0.592\n",
            "Epoch   1 Batch  182/269, Loss:  0.585\n",
            "Epoch   1 Batch  183/269, Loss:  0.592\n",
            "Epoch   1 Batch  184/269, Loss:  0.584\n",
            "Epoch   1 Batch  185/269, Loss:  0.650\n",
            "Epoch   1 Batch  186/269, Loss:  0.645\n",
            "Epoch   1 Batch  187/269, Loss:  0.575\n",
            "Epoch   1 Batch  188/269, Loss:  0.603\n",
            "Epoch   1 Batch  189/269, Loss:  0.689\n",
            "Epoch   1 Batch  190/269, Loss:  0.580\n",
            "Epoch   1 Batch  191/269, Loss:  0.527\n",
            "Epoch   1 Batch  192/269, Loss:  0.613\n",
            "Epoch   1 Batch  193/269, Loss:  0.602\n",
            "Epoch   1 Batch  194/269, Loss:  0.562\n",
            "Epoch   1 Batch  195/269, Loss:  0.644\n",
            "Epoch   1 Batch  196/269, Loss:  0.653\n",
            "Epoch   1 Batch  197/269, Loss:  0.651\n",
            "Epoch   1 Batch  198/269, Loss:  0.632\n",
            "Epoch   1 Batch  199/269, Loss:  0.657\n",
            "Epoch   1 Batch  200/269, Loss:  0.602\n",
            "Epoch   1 Batch  201/269, Loss:  0.644\n",
            "Epoch   1 Batch  202/269, Loss:  0.688\n",
            "Epoch   1 Batch  203/269, Loss:  0.587\n",
            "Epoch   1 Batch  204/269, Loss:  0.613\n",
            "Epoch   1 Batch  205/269, Loss:  0.625\n",
            "Epoch   1 Batch  206/269, Loss:  0.652\n",
            "Epoch   1 Batch  207/269, Loss:  0.570\n",
            "Epoch   1 Batch  208/269, Loss:  0.625\n",
            "Epoch   1 Batch  209/269, Loss:  0.561\n",
            "Epoch   1 Batch  210/269, Loss:  0.552\n",
            "Epoch   1 Batch  211/269, Loss:  0.588\n",
            "Epoch   1 Batch  212/269, Loss:  0.598\n",
            "Epoch   1 Batch  213/269, Loss:  0.560\n",
            "Epoch   1 Batch  214/269, Loss:  0.561\n",
            "Epoch   1 Batch  215/269, Loss:  0.552\n",
            "Epoch   1 Batch  216/269, Loss:  0.656\n",
            "Epoch   1 Batch  217/269, Loss:  0.619\n",
            "Epoch   1 Batch  218/269, Loss:  0.642\n",
            "Epoch   1 Batch  219/269, Loss:  0.567\n",
            "Epoch   1 Batch  220/269, Loss:  0.561\n",
            "Epoch   1 Batch  221/269, Loss:  0.528\n",
            "Epoch   1 Batch  222/269, Loss:  0.551\n",
            "Epoch   1 Batch  223/269, Loss:  0.574\n",
            "Epoch   1 Batch  224/269, Loss:  0.604\n",
            "Epoch   1 Batch  225/269, Loss:  0.590\n",
            "Epoch   1 Batch  226/269, Loss:  0.573\n",
            "Epoch   1 Batch  227/269, Loss:  0.527\n",
            "Epoch   1 Batch  228/269, Loss:  0.513\n",
            "Epoch   1 Batch  229/269, Loss:  0.556\n",
            "Epoch   1 Batch  230/269, Loss:  0.640\n",
            "Epoch   1 Batch  231/269, Loss:  0.552\n",
            "Epoch   1 Batch  232/269, Loss:  0.563\n",
            "Epoch   1 Batch  233/269, Loss:  0.504\n",
            "Epoch   1 Batch  234/269, Loss:  0.589\n",
            "Epoch   1 Batch  235/269, Loss:  0.609\n",
            "Epoch   1 Batch  236/269, Loss:  0.591\n",
            "Epoch   1 Batch  237/269, Loss:  0.507\n",
            "Epoch   1 Batch  238/269, Loss:  0.565\n",
            "Epoch   1 Batch  239/269, Loss:  0.520\n",
            "Epoch   1 Batch  240/269, Loss:  0.542\n",
            "Epoch   1 Batch  241/269, Loss:  0.609\n",
            "Epoch   1 Batch  242/269, Loss:  0.522\n",
            "Epoch   1 Batch  243/269, Loss:  0.536\n",
            "Epoch   1 Batch  244/269, Loss:  0.594\n",
            "Epoch   1 Batch  245/269, Loss:  0.545\n",
            "Epoch   1 Batch  246/269, Loss:  0.620\n",
            "Epoch   1 Batch  247/269, Loss:  0.590\n",
            "Epoch   1 Batch  248/269, Loss:  0.539\n",
            "Epoch   1 Batch  249/269, Loss:  0.606\n",
            "Epoch   1 Batch  250/269, Loss:  0.547\n",
            "Epoch   1 Batch  251/269, Loss:  0.528\n",
            "Epoch   1 Batch  252/269, Loss:  0.516\n",
            "Epoch   1 Batch  253/269, Loss:  0.587\n",
            "Epoch   1 Batch  254/269, Loss:  0.604\n",
            "Epoch   1 Batch  255/269, Loss:  0.542\n",
            "Epoch   1 Batch  256/269, Loss:  0.583\n",
            "Epoch   1 Batch  257/269, Loss:  0.590\n",
            "Epoch   1 Batch  258/269, Loss:  0.638\n",
            "Epoch   1 Batch  259/269, Loss:  0.538\n",
            "Epoch   1 Batch  260/269, Loss:  0.525\n",
            "Epoch   1 Batch  261/269, Loss:  0.559\n",
            "Epoch   1 Batch  262/269, Loss:  0.622\n",
            "Epoch   1 Batch  263/269, Loss:  0.639\n",
            "Epoch   1 Batch  264/269, Loss:  0.514\n",
            "Epoch   1 Batch  265/269, Loss:  0.606\n",
            "Epoch   1 Batch  266/269, Loss:  0.519\n",
            "Epoch   1 Batch  267/269, Loss:  0.569\n",
            "Epoch   2 Batch    0/269, Loss:  0.605\n",
            "Epoch   2 Batch    1/269, Loss:  0.575\n",
            "Epoch   2 Batch    2/269, Loss:  0.638\n",
            "Epoch   2 Batch    3/269, Loss:  0.600\n",
            "Epoch   2 Batch    4/269, Loss:  0.528\n",
            "Epoch   2 Batch    5/269, Loss:  0.564\n",
            "Epoch   2 Batch    6/269, Loss:  0.576\n",
            "Epoch   2 Batch    7/269, Loss:  0.623\n",
            "Epoch   2 Batch    8/269, Loss:  0.553\n",
            "Epoch   2 Batch    9/269, Loss:  0.540\n",
            "Epoch   2 Batch   10/269, Loss:  0.514\n",
            "Epoch   2 Batch   11/269, Loss:  0.584\n",
            "Epoch   2 Batch   12/269, Loss:  0.573\n",
            "Epoch   2 Batch   13/269, Loss:  0.576\n",
            "Epoch   2 Batch   14/269, Loss:  0.561\n",
            "Epoch   2 Batch   15/269, Loss:  0.578\n",
            "Epoch   2 Batch   16/269, Loss:  0.558\n",
            "Epoch   2 Batch   17/269, Loss:  0.545\n",
            "Epoch   2 Batch   18/269, Loss:  0.546\n",
            "Epoch   2 Batch   19/269, Loss:  0.636\n",
            "Epoch   2 Batch   20/269, Loss:  0.532\n",
            "Epoch   2 Batch   21/269, Loss:  0.536\n",
            "Epoch   2 Batch   22/269, Loss:  0.577\n",
            "Epoch   2 Batch   23/269, Loss:  0.567\n",
            "Epoch   2 Batch   24/269, Loss:  0.562\n",
            "Epoch   2 Batch   25/269, Loss:  0.542\n",
            "Epoch   2 Batch   26/269, Loss:  0.578\n",
            "Epoch   2 Batch   27/269, Loss:  0.598\n",
            "Epoch   2 Batch   28/269, Loss:  0.632\n",
            "Epoch   2 Batch   29/269, Loss:  0.583\n",
            "Epoch   2 Batch   30/269, Loss:  0.546\n",
            "Epoch   2 Batch   31/269, Loss:  0.560\n",
            "Epoch   2 Batch   32/269, Loss:  0.577\n",
            "Epoch   2 Batch   33/269, Loss:  0.549\n",
            "Epoch   2 Batch   34/269, Loss:  0.504\n",
            "Epoch   2 Batch   35/269, Loss:  0.632\n",
            "Epoch   2 Batch   36/269, Loss:  0.614\n",
            "Epoch   2 Batch   37/269, Loss:  0.542\n",
            "Epoch   2 Batch   38/269, Loss:  0.546\n",
            "Epoch   2 Batch   39/269, Loss:  0.551\n",
            "Epoch   2 Batch   40/269, Loss:  0.549\n",
            "Epoch   2 Batch   41/269, Loss:  0.577\n",
            "Epoch   2 Batch   42/269, Loss:  0.552\n",
            "Epoch   2 Batch   43/269, Loss:  0.556\n",
            "Epoch   2 Batch   44/269, Loss:  0.524\n",
            "Epoch   2 Batch   45/269, Loss:  0.545\n",
            "Epoch   2 Batch   46/269, Loss:  0.536\n",
            "Epoch   2 Batch   47/269, Loss:  0.564\n",
            "Epoch   2 Batch   48/269, Loss:  0.488\n",
            "Epoch   2 Batch   49/269, Loss:  0.512\n",
            "Epoch   2 Batch   50/269, Loss:  0.542\n",
            "Epoch   2 Batch   51/269, Loss:  0.518\n",
            "Epoch   2 Batch   52/269, Loss:  0.558\n",
            "Epoch   2 Batch   53/269, Loss:  0.583\n",
            "Epoch   2 Batch   54/269, Loss:  0.446\n",
            "Epoch   2 Batch   55/269, Loss:  0.550\n",
            "Epoch   2 Batch   56/269, Loss:  0.511\n",
            "Epoch   2 Batch   57/269, Loss:  0.534\n",
            "Epoch   2 Batch   58/269, Loss:  0.589\n",
            "Epoch   2 Batch   59/269, Loss:  0.530\n",
            "Epoch   2 Batch   60/269, Loss:  0.561\n",
            "Epoch   2 Batch   61/269, Loss:  0.582\n",
            "Epoch   2 Batch   62/269, Loss:  0.542\n",
            "Epoch   2 Batch   63/269, Loss:  0.521\n",
            "Epoch   2 Batch   64/269, Loss:  0.607\n",
            "Epoch   2 Batch   65/269, Loss:  0.523\n",
            "Epoch   2 Batch   66/269, Loss:  0.544\n",
            "Epoch   2 Batch   67/269, Loss:  0.480\n",
            "Epoch   2 Batch   68/269, Loss:  0.682\n",
            "Epoch   2 Batch   69/269, Loss:  0.594\n",
            "Epoch   2 Batch   70/269, Loss:  0.578\n",
            "Epoch   2 Batch   71/269, Loss:  0.544\n",
            "Epoch   2 Batch   72/269, Loss:  0.577\n",
            "Epoch   2 Batch   73/269, Loss:  0.582\n",
            "Epoch   2 Batch   74/269, Loss:  0.484\n",
            "Epoch   2 Batch   75/269, Loss:  0.665\n",
            "Epoch   2 Batch   76/269, Loss:  0.587\n",
            "Epoch   2 Batch   77/269, Loss:  0.533\n",
            "Epoch   2 Batch   78/269, Loss:  0.554\n",
            "Epoch   2 Batch   79/269, Loss:  0.575\n",
            "Epoch   2 Batch   80/269, Loss:  0.586\n",
            "Epoch   2 Batch   81/269, Loss:  0.505\n",
            "Epoch   2 Batch   82/269, Loss:  0.509\n",
            "Epoch   2 Batch   83/269, Loss:  0.553\n",
            "Epoch   2 Batch   84/269, Loss:  0.590\n",
            "Epoch   2 Batch   85/269, Loss:  0.503\n",
            "Epoch   2 Batch   86/269, Loss:  0.552\n",
            "Epoch   2 Batch   87/269, Loss:  0.524\n",
            "Epoch   2 Batch   88/269, Loss:  0.504\n",
            "Epoch   2 Batch   89/269, Loss:  0.573\n",
            "Epoch   2 Batch   90/269, Loss:  0.541\n",
            "Epoch   2 Batch   91/269, Loss:  0.564\n",
            "Epoch   2 Batch   92/269, Loss:  0.546\n",
            "Epoch   2 Batch   93/269, Loss:  0.525\n",
            "Epoch   2 Batch   94/269, Loss:  0.565\n",
            "Epoch   2 Batch   95/269, Loss:  0.545\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch   2 Batch   96/269, Loss:  0.586\n",
            "Epoch   2 Batch   97/269, Loss:  0.552\n",
            "Epoch   2 Batch   98/269, Loss:  0.484\n",
            "Epoch   2 Batch   99/269, Loss:  0.514\n",
            "Epoch   2 Batch  100/269, Loss:  0.511\n",
            "Epoch   2 Batch  101/269, Loss:  0.584\n",
            "Epoch   2 Batch  102/269, Loss:  0.563\n",
            "Epoch   2 Batch  103/269, Loss:  0.583\n",
            "Epoch   2 Batch  104/269, Loss:  0.560\n",
            "Epoch   2 Batch  105/269, Loss:  0.541\n",
            "Epoch   2 Batch  106/269, Loss:  0.528\n",
            "Epoch   2 Batch  107/269, Loss:  0.547\n",
            "Epoch   2 Batch  108/269, Loss:  0.493\n",
            "Epoch   2 Batch  109/269, Loss:  0.595\n",
            "Epoch   2 Batch  110/269, Loss:  0.522\n",
            "Epoch   2 Batch  111/269, Loss:  0.530\n",
            "Epoch   2 Batch  112/269, Loss:  0.566\n",
            "Epoch   2 Batch  113/269, Loss:  0.542\n",
            "Epoch   2 Batch  114/269, Loss:  0.516\n",
            "Epoch   2 Batch  115/269, Loss:  0.505\n",
            "Epoch   2 Batch  116/269, Loss:  0.526\n",
            "Epoch   2 Batch  117/269, Loss:  0.593\n",
            "Epoch   2 Batch  118/269, Loss:  0.537\n",
            "Epoch   2 Batch  119/269, Loss:  0.526\n",
            "Epoch   2 Batch  120/269, Loss:  0.568\n",
            "Epoch   2 Batch  121/269, Loss:  0.510\n",
            "Epoch   2 Batch  122/269, Loss:  0.561\n",
            "Epoch   2 Batch  123/269, Loss:  0.528\n",
            "Epoch   2 Batch  124/269, Loss:  0.512\n",
            "Epoch   2 Batch  125/269, Loss:  0.588\n",
            "Epoch   2 Batch  126/269, Loss:  0.479\n",
            "Epoch   2 Batch  127/269, Loss:  0.538\n",
            "Epoch   2 Batch  128/269, Loss:  0.549\n",
            "Epoch   2 Batch  129/269, Loss:  0.518\n",
            "Epoch   2 Batch  130/269, Loss:  0.551\n",
            "Epoch   2 Batch  131/269, Loss:  0.571\n",
            "Epoch   2 Batch  132/269, Loss:  0.527\n",
            "Epoch   2 Batch  133/269, Loss:  0.522\n",
            "Epoch   2 Batch  134/269, Loss:  0.480\n",
            "Epoch   2 Batch  135/269, Loss:  0.570\n",
            "Epoch   2 Batch  136/269, Loss:  0.471\n",
            "Epoch   2 Batch  137/269, Loss:  0.491\n",
            "Epoch   2 Batch  138/269, Loss:  0.503\n",
            "Epoch   2 Batch  139/269, Loss:  0.526\n",
            "Epoch   2 Batch  140/269, Loss:  0.538\n",
            "Epoch   2 Batch  141/269, Loss:  0.564\n",
            "Epoch   2 Batch  142/269, Loss:  0.498\n",
            "Epoch   2 Batch  143/269, Loss:  0.497\n",
            "Epoch   2 Batch  144/269, Loss:  0.528\n",
            "Epoch   2 Batch  145/269, Loss:  0.471\n",
            "Epoch   2 Batch  146/269, Loss:  0.563\n",
            "Epoch   2 Batch  147/269, Loss:  0.560\n",
            "Epoch   2 Batch  148/269, Loss:  0.446\n",
            "Epoch   2 Batch  149/269, Loss:  0.458\n",
            "Epoch   2 Batch  150/269, Loss:  0.482\n",
            "Epoch   2 Batch  151/269, Loss:  0.495\n",
            "Epoch   2 Batch  152/269, Loss:  0.498\n",
            "Epoch   2 Batch  153/269, Loss:  0.501\n",
            "Epoch   2 Batch  154/269, Loss:  0.497\n",
            "Epoch   2 Batch  155/269, Loss:  0.519\n",
            "Epoch   2 Batch  156/269, Loss:  0.511\n",
            "Epoch   2 Batch  157/269, Loss:  0.476\n",
            "Epoch   2 Batch  158/269, Loss:  0.475\n",
            "Epoch   2 Batch  159/269, Loss:  0.509\n",
            "Epoch   2 Batch  160/269, Loss:  0.502\n",
            "Epoch   2 Batch  161/269, Loss:  0.504\n",
            "Epoch   2 Batch  162/269, Loss:  0.508\n",
            "Epoch   2 Batch  163/269, Loss:  0.535\n",
            "Epoch   2 Batch  164/269, Loss:  0.530\n",
            "Epoch   2 Batch  165/269, Loss:  0.604\n",
            "Epoch   2 Batch  166/269, Loss:  0.499\n",
            "Epoch   2 Batch  167/269, Loss:  0.515\n",
            "Epoch   2 Batch  168/269, Loss:  0.495\n",
            "Epoch   2 Batch  169/269, Loss:  0.516\n",
            "Epoch   2 Batch  170/269, Loss:  0.508\n",
            "Epoch   2 Batch  171/269, Loss:  0.509\n",
            "Epoch   2 Batch  172/269, Loss:  0.514\n",
            "Epoch   2 Batch  173/269, Loss:  0.517\n",
            "Epoch   2 Batch  174/269, Loss:  0.475\n",
            "Epoch   2 Batch  175/269, Loss:  0.671\n",
            "Epoch   2 Batch  176/269, Loss:  0.539\n",
            "Epoch   2 Batch  177/269, Loss:  0.488\n",
            "Epoch   2 Batch  178/269, Loss:  0.526\n",
            "Epoch   2 Batch  179/269, Loss:  0.476\n",
            "Epoch   2 Batch  180/269, Loss:  0.440\n",
            "Epoch   2 Batch  181/269, Loss:  0.509\n",
            "Epoch   2 Batch  182/269, Loss:  0.484\n",
            "Epoch   2 Batch  183/269, Loss:  0.526\n",
            "Epoch   2 Batch  184/269, Loss:  0.559\n",
            "Epoch   2 Batch  185/269, Loss:  0.529\n",
            "Epoch   2 Batch  186/269, Loss:  0.537\n",
            "Epoch   2 Batch  187/269, Loss:  0.488\n",
            "Epoch   2 Batch  188/269, Loss:  0.502\n",
            "Epoch   2 Batch  189/269, Loss:  0.532\n",
            "Epoch   2 Batch  190/269, Loss:  0.527\n",
            "Epoch   2 Batch  191/269, Loss:  0.501\n",
            "Epoch   2 Batch  192/269, Loss:  0.490\n",
            "Epoch   2 Batch  193/269, Loss:  0.505\n",
            "Epoch   2 Batch  194/269, Loss:  0.473\n",
            "Epoch   2 Batch  195/269, Loss:  0.543\n",
            "Epoch   2 Batch  196/269, Loss:  0.501\n",
            "Epoch   2 Batch  197/269, Loss:  0.573\n",
            "Epoch   2 Batch  198/269, Loss:  0.477\n",
            "Epoch   2 Batch  199/269, Loss:  0.527\n",
            "Epoch   2 Batch  200/269, Loss:  0.490\n",
            "Epoch   2 Batch  201/269, Loss:  0.538\n",
            "Epoch   2 Batch  202/269, Loss:  0.619\n",
            "Epoch   2 Batch  203/269, Loss:  0.506\n",
            "Epoch   2 Batch  204/269, Loss:  0.550\n",
            "Epoch   2 Batch  205/269, Loss:  0.545\n",
            "Epoch   2 Batch  206/269, Loss:  0.511\n",
            "Epoch   2 Batch  207/269, Loss:  0.494\n",
            "Epoch   2 Batch  208/269, Loss:  0.564\n",
            "Epoch   2 Batch  209/269, Loss:  0.459\n",
            "Epoch   2 Batch  210/269, Loss:  0.484\n",
            "Epoch   2 Batch  211/269, Loss:  0.506\n",
            "Epoch   2 Batch  212/269, Loss:  0.537\n",
            "Epoch   2 Batch  213/269, Loss:  0.567\n",
            "Epoch   2 Batch  214/269, Loss:  0.523\n",
            "Epoch   2 Batch  215/269, Loss:  0.465\n",
            "Epoch   2 Batch  216/269, Loss:  0.528\n",
            "Epoch   2 Batch  217/269, Loss:  0.529\n",
            "Epoch   2 Batch  218/269, Loss:  0.587\n",
            "Epoch   2 Batch  219/269, Loss:  0.532\n",
            "Epoch   2 Batch  220/269, Loss:  0.507\n",
            "Epoch   2 Batch  221/269, Loss:  0.444\n",
            "Epoch   2 Batch  222/269, Loss:  0.478\n",
            "Epoch   2 Batch  223/269, Loss:  0.552\n",
            "Epoch   2 Batch  224/269, Loss:  0.478\n",
            "Epoch   2 Batch  225/269, Loss:  0.523\n",
            "Epoch   2 Batch  226/269, Loss:  0.518\n",
            "Epoch   2 Batch  227/269, Loss:  0.480\n",
            "Epoch   2 Batch  228/269, Loss:  0.466\n",
            "Epoch   2 Batch  229/269, Loss:  0.470\n",
            "Epoch   2 Batch  230/269, Loss:  0.591\n",
            "Epoch   2 Batch  231/269, Loss:  0.462\n",
            "Epoch   2 Batch  232/269, Loss:  0.552\n",
            "Epoch   2 Batch  233/269, Loss:  0.507\n",
            "Epoch   2 Batch  234/269, Loss:  0.541\n",
            "Epoch   2 Batch  235/269, Loss:  0.537\n",
            "Epoch   2 Batch  236/269, Loss:  0.499\n",
            "Epoch   2 Batch  237/269, Loss:  0.471\n",
            "Epoch   2 Batch  238/269, Loss:  0.468\n",
            "Epoch   2 Batch  239/269, Loss:  0.533\n",
            "Epoch   2 Batch  240/269, Loss:  0.496\n",
            "Epoch   2 Batch  241/269, Loss:  0.530\n",
            "Epoch   2 Batch  242/269, Loss:  0.508\n",
            "Epoch   2 Batch  243/269, Loss:  0.497\n",
            "Epoch   2 Batch  244/269, Loss:  0.507\n",
            "Epoch   2 Batch  245/269, Loss:  0.533\n",
            "Epoch   2 Batch  246/269, Loss:  0.599\n",
            "Epoch   2 Batch  247/269, Loss:  0.503\n",
            "Epoch   2 Batch  248/269, Loss:  0.496\n",
            "Epoch   2 Batch  249/269, Loss:  0.495\n",
            "Epoch   2 Batch  250/269, Loss:  0.490\n",
            "Epoch   2 Batch  251/269, Loss:  0.481\n",
            "Epoch   2 Batch  252/269, Loss:  0.507\n",
            "Epoch   2 Batch  253/269, Loss:  0.501\n",
            "Epoch   2 Batch  254/269, Loss:  0.515\n",
            "Epoch   2 Batch  255/269, Loss:  0.487\n",
            "Epoch   2 Batch  256/269, Loss:  0.566\n",
            "Epoch   2 Batch  257/269, Loss:  0.537\n",
            "Epoch   2 Batch  258/269, Loss:  0.501\n",
            "Epoch   2 Batch  259/269, Loss:  0.490\n",
            "Epoch   2 Batch  260/269, Loss:  0.443\n",
            "Epoch   2 Batch  261/269, Loss:  0.478\n",
            "Epoch   2 Batch  262/269, Loss:  0.541\n",
            "Epoch   2 Batch  263/269, Loss:  0.527\n",
            "Epoch   2 Batch  264/269, Loss:  0.475\n",
            "Epoch   2 Batch  265/269, Loss:  0.487\n",
            "Epoch   2 Batch  266/269, Loss:  0.474\n",
            "Epoch   2 Batch  267/269, Loss:  0.474\n",
            "Epoch   3 Batch    0/269, Loss:  0.512\n",
            "Epoch   3 Batch    1/269, Loss:  0.501\n",
            "Epoch   3 Batch    2/269, Loss:  0.549\n",
            "Epoch   3 Batch    3/269, Loss:  0.499\n",
            "Epoch   3 Batch    4/269, Loss:  0.489\n",
            "Epoch   3 Batch    5/269, Loss:  0.481\n",
            "Epoch   3 Batch    6/269, Loss:  0.489\n",
            "Epoch   3 Batch    7/269, Loss:  0.519\n",
            "Epoch   3 Batch    8/269, Loss:  0.493\n",
            "Epoch   3 Batch    9/269, Loss:  0.440\n",
            "Epoch   3 Batch   10/269, Loss:  0.437\n",
            "Epoch   3 Batch   11/269, Loss:  0.470\n",
            "Epoch   3 Batch   12/269, Loss:  0.471\n",
            "Epoch   3 Batch   13/269, Loss:  0.515\n",
            "Epoch   3 Batch   14/269, Loss:  0.511\n",
            "Epoch   3 Batch   15/269, Loss:  0.527\n",
            "Epoch   3 Batch   16/269, Loss:  0.480\n",
            "Epoch   3 Batch   17/269, Loss:  0.500\n",
            "Epoch   3 Batch   18/269, Loss:  0.504\n",
            "Epoch   3 Batch   19/269, Loss:  0.516\n",
            "Epoch   3 Batch   20/269, Loss:  0.511\n",
            "Epoch   3 Batch   21/269, Loss:  0.510\n",
            "Epoch   3 Batch   22/269, Loss:  0.481\n",
            "Epoch   3 Batch   23/269, Loss:  0.557\n",
            "Epoch   3 Batch   24/269, Loss:  0.504\n",
            "Epoch   3 Batch   25/269, Loss:  0.540\n",
            "Epoch   3 Batch   26/269, Loss:  0.498\n",
            "Epoch   3 Batch   27/269, Loss:  0.509\n",
            "Epoch   3 Batch   28/269, Loss:  0.576\n",
            "Epoch   3 Batch   29/269, Loss:  0.501\n",
            "Epoch   3 Batch   30/269, Loss:  0.463\n",
            "Epoch   3 Batch   31/269, Loss:  0.484\n",
            "Epoch   3 Batch   32/269, Loss:  0.508\n",
            "Epoch   3 Batch   33/269, Loss:  0.519\n",
            "Epoch   3 Batch   34/269, Loss:  0.461\n",
            "Epoch   3 Batch   35/269, Loss:  0.543\n",
            "Epoch   3 Batch   36/269, Loss:  0.511\n",
            "Epoch   3 Batch   37/269, Loss:  0.489\n",
            "Epoch   3 Batch   38/269, Loss:  0.502\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch   3 Batch   39/269, Loss:  0.508\n",
            "Epoch   3 Batch   40/269, Loss:  0.479\n",
            "Epoch   3 Batch   41/269, Loss:  0.479\n",
            "Epoch   3 Batch   42/269, Loss:  0.487\n",
            "Epoch   3 Batch   43/269, Loss:  0.509\n",
            "Epoch   3 Batch   44/269, Loss:  0.502\n",
            "Epoch   3 Batch   45/269, Loss:  0.470\n",
            "Epoch   3 Batch   46/269, Loss:  0.467\n",
            "Epoch   3 Batch   47/269, Loss:  0.503\n",
            "Epoch   3 Batch   48/269, Loss:  0.501\n",
            "Epoch   3 Batch   49/269, Loss:  0.469\n",
            "Epoch   3 Batch   50/269, Loss:  0.479\n",
            "Epoch   3 Batch   51/269, Loss:  0.482\n",
            "Epoch   3 Batch   52/269, Loss:  0.552\n",
            "Epoch   3 Batch   53/269, Loss:  0.505\n",
            "Epoch   3 Batch   54/269, Loss:  0.411\n",
            "Epoch   3 Batch   55/269, Loss:  0.503\n",
            "Epoch   3 Batch   56/269, Loss:  0.440\n",
            "Epoch   3 Batch   57/269, Loss:  0.491\n",
            "Epoch   3 Batch   58/269, Loss:  0.550\n",
            "Epoch   3 Batch   59/269, Loss:  0.516\n",
            "Epoch   3 Batch   60/269, Loss:  0.496\n",
            "Epoch   3 Batch   61/269, Loss:  0.550\n",
            "Epoch   3 Batch   62/269, Loss:  0.428\n",
            "Epoch   3 Batch   63/269, Loss:  0.443\n",
            "Epoch   3 Batch   64/269, Loss:  0.495\n",
            "Epoch   3 Batch   65/269, Loss:  0.476\n",
            "Epoch   3 Batch   66/269, Loss:  0.520\n",
            "Epoch   3 Batch   67/269, Loss:  0.531\n",
            "Epoch   3 Batch   68/269, Loss:  0.587\n",
            "Epoch   3 Batch   69/269, Loss:  0.513\n",
            "Epoch   3 Batch   70/269, Loss:  0.560\n",
            "Epoch   3 Batch   71/269, Loss:  0.496\n",
            "Epoch   3 Batch   72/269, Loss:  0.516\n",
            "Epoch   3 Batch   73/269, Loss:  0.505\n",
            "Epoch   3 Batch   74/269, Loss:  0.469\n",
            "Epoch   3 Batch   75/269, Loss:  0.576\n",
            "Epoch   3 Batch   76/269, Loss:  0.543\n",
            "Epoch   3 Batch   77/269, Loss:  0.476\n",
            "Epoch   3 Batch   78/269, Loss:  0.561\n",
            "Epoch   3 Batch   79/269, Loss:  0.495\n",
            "Epoch   3 Batch   80/269, Loss:  0.486\n",
            "Epoch   3 Batch   81/269, Loss:  0.419\n",
            "Epoch   3 Batch   82/269, Loss:  0.464\n",
            "Epoch   3 Batch   83/269, Loss:  0.490\n",
            "Epoch   3 Batch   84/269, Loss:  0.541\n",
            "Epoch   3 Batch   85/269, Loss:  0.480\n",
            "Epoch   3 Batch   86/269, Loss:  0.506\n",
            "Epoch   3 Batch   87/269, Loss:  0.452\n",
            "Epoch   3 Batch   88/269, Loss:  0.431\n",
            "Epoch   3 Batch   89/269, Loss:  0.517\n",
            "Epoch   3 Batch   90/269, Loss:  0.544\n",
            "Epoch   3 Batch   91/269, Loss:  0.506\n",
            "Epoch   3 Batch   92/269, Loss:  0.500\n",
            "Epoch   3 Batch   93/269, Loss:  0.505\n",
            "Epoch   3 Batch   94/269, Loss:  0.491\n",
            "Epoch   3 Batch   95/269, Loss:  0.534\n",
            "Epoch   3 Batch   96/269, Loss:  0.522\n",
            "Epoch   3 Batch   97/269, Loss:  0.517\n",
            "Epoch   3 Batch   98/269, Loss:  0.447\n",
            "Epoch   3 Batch   99/269, Loss:  0.500\n",
            "Epoch   3 Batch  100/269, Loss:  0.436\n",
            "Epoch   3 Batch  101/269, Loss:  0.515\n",
            "Epoch   3 Batch  102/269, Loss:  0.550\n",
            "Epoch   3 Batch  103/269, Loss:  0.522\n",
            "Epoch   3 Batch  104/269, Loss:  0.475\n",
            "Epoch   3 Batch  105/269, Loss:  0.469\n",
            "Epoch   3 Batch  106/269, Loss:  0.485\n",
            "Epoch   3 Batch  107/269, Loss:  0.488\n",
            "Epoch   3 Batch  108/269, Loss:  0.432\n",
            "Epoch   3 Batch  109/269, Loss:  0.521\n",
            "Epoch   3 Batch  110/269, Loss:  0.507\n",
            "Epoch   3 Batch  111/269, Loss:  0.484\n",
            "Epoch   3 Batch  112/269, Loss:  0.505\n",
            "Epoch   3 Batch  113/269, Loss:  0.504\n",
            "Epoch   3 Batch  114/269, Loss:  0.492\n",
            "Epoch   3 Batch  115/269, Loss:  0.545\n",
            "Epoch   3 Batch  116/269, Loss:  0.493\n",
            "Epoch   3 Batch  117/269, Loss:  0.481\n",
            "Epoch   3 Batch  118/269, Loss:  0.495\n",
            "Epoch   3 Batch  119/269, Loss:  0.454\n",
            "Epoch   3 Batch  120/269, Loss:  0.554\n",
            "Epoch   3 Batch  121/269, Loss:  0.460\n",
            "Epoch   3 Batch  122/269, Loss:  0.538\n",
            "Epoch   3 Batch  123/269, Loss:  0.527\n",
            "Epoch   3 Batch  124/269, Loss:  0.481\n",
            "Epoch   3 Batch  125/269, Loss:  0.520\n",
            "Epoch   3 Batch  126/269, Loss:  0.453\n",
            "Epoch   3 Batch  127/269, Loss:  0.504\n",
            "Epoch   3 Batch  128/269, Loss:  0.473\n",
            "Epoch   3 Batch  129/269, Loss:  0.465\n",
            "Epoch   3 Batch  130/269, Loss:  0.472\n",
            "Epoch   3 Batch  131/269, Loss:  0.547\n",
            "Epoch   3 Batch  132/269, Loss:  0.493\n",
            "Epoch   3 Batch  133/269, Loss:  0.537\n",
            "Epoch   3 Batch  134/269, Loss:  0.469\n",
            "Epoch   3 Batch  135/269, Loss:  0.583\n",
            "Epoch   3 Batch  136/269, Loss:  0.434\n",
            "Epoch   3 Batch  137/269, Loss:  0.464\n",
            "Epoch   3 Batch  138/269, Loss:  0.482\n",
            "Epoch   3 Batch  139/269, Loss:  0.515\n",
            "Epoch   3 Batch  140/269, Loss:  0.507\n",
            "Epoch   3 Batch  141/269, Loss:  0.558\n",
            "Epoch   3 Batch  142/269, Loss:  0.490\n",
            "Epoch   3 Batch  143/269, Loss:  0.467\n",
            "Epoch   3 Batch  144/269, Loss:  0.501\n",
            "Epoch   3 Batch  145/269, Loss:  0.463\n",
            "Epoch   3 Batch  146/269, Loss:  0.482\n",
            "Epoch   3 Batch  147/269, Loss:  0.528\n",
            "Epoch   3 Batch  148/269, Loss:  0.459\n",
            "Epoch   3 Batch  149/269, Loss:  0.464\n",
            "Epoch   3 Batch  150/269, Loss:  0.525\n",
            "Epoch   3 Batch  151/269, Loss:  0.472\n",
            "Epoch   3 Batch  152/269, Loss:  0.475\n",
            "Epoch   3 Batch  153/269, Loss:  0.513\n",
            "Epoch   3 Batch  154/269, Loss:  0.510\n",
            "Epoch   3 Batch  155/269, Loss:  0.470\n",
            "Epoch   3 Batch  156/269, Loss:  0.498\n",
            "Epoch   3 Batch  157/269, Loss:  0.488\n",
            "Epoch   3 Batch  158/269, Loss:  0.433\n",
            "Epoch   3 Batch  159/269, Loss:  0.510\n",
            "Epoch   3 Batch  160/269, Loss:  0.494\n",
            "Epoch   3 Batch  161/269, Loss:  0.494\n",
            "Epoch   3 Batch  162/269, Loss:  0.492\n",
            "Epoch   3 Batch  163/269, Loss:  0.462\n",
            "Epoch   3 Batch  164/269, Loss:  0.513\n",
            "Epoch   3 Batch  165/269, Loss:  0.581\n",
            "Epoch   3 Batch  166/269, Loss:  0.478\n",
            "Epoch   3 Batch  167/269, Loss:  0.473\n",
            "Epoch   3 Batch  168/269, Loss:  0.466\n",
            "Epoch   3 Batch  169/269, Loss:  0.504\n",
            "Epoch   3 Batch  170/269, Loss:  0.511\n",
            "Epoch   3 Batch  171/269, Loss:  0.509\n",
            "Epoch   3 Batch  172/269, Loss:  0.475\n",
            "Epoch   3 Batch  173/269, Loss:  0.504\n",
            "Epoch   3 Batch  174/269, Loss:  0.497\n",
            "Epoch   3 Batch  175/269, Loss:  0.579\n",
            "Epoch   3 Batch  176/269, Loss:  0.533\n",
            "Epoch   3 Batch  177/269, Loss:  0.455\n",
            "Epoch   3 Batch  178/269, Loss:  0.467\n",
            "Epoch   3 Batch  179/269, Loss:  0.453\n",
            "Epoch   3 Batch  180/269, Loss:  0.436\n",
            "Epoch   3 Batch  181/269, Loss:  0.501\n",
            "Epoch   3 Batch  182/269, Loss:  0.419\n",
            "Epoch   3 Batch  183/269, Loss:  0.477\n",
            "Epoch   3 Batch  184/269, Loss:  0.523\n",
            "Epoch   3 Batch  185/269, Loss:  0.480\n",
            "Epoch   3 Batch  186/269, Loss:  0.514\n",
            "Epoch   3 Batch  187/269, Loss:  0.470\n",
            "Epoch   3 Batch  188/269, Loss:  0.437\n",
            "Epoch   3 Batch  189/269, Loss:  0.551\n",
            "Epoch   3 Batch  190/269, Loss:  0.511\n",
            "Epoch   3 Batch  191/269, Loss:  0.446\n",
            "Epoch   3 Batch  192/269, Loss:  0.478\n",
            "Epoch   3 Batch  193/269, Loss:  0.484\n",
            "Epoch   3 Batch  194/269, Loss:  0.454\n",
            "Epoch   3 Batch  195/269, Loss:  0.490\n",
            "Epoch   3 Batch  196/269, Loss:  0.456\n",
            "Epoch   3 Batch  197/269, Loss:  0.522\n",
            "Epoch   3 Batch  198/269, Loss:  0.480\n",
            "Epoch   3 Batch  199/269, Loss:  0.515\n",
            "Epoch   3 Batch  200/269, Loss:  0.491\n",
            "Epoch   3 Batch  201/269, Loss:  0.526\n",
            "Epoch   3 Batch  202/269, Loss:  0.553\n",
            "Epoch   3 Batch  203/269, Loss:  0.494\n",
            "Epoch   3 Batch  204/269, Loss:  0.528\n",
            "Epoch   3 Batch  205/269, Loss:  0.493\n",
            "Epoch   3 Batch  206/269, Loss:  0.513\n",
            "Epoch   3 Batch  207/269, Loss:  0.466\n",
            "Epoch   3 Batch  208/269, Loss:  0.539\n",
            "Epoch   3 Batch  209/269, Loss:  0.434\n",
            "Epoch   3 Batch  210/269, Loss:  0.460\n",
            "Epoch   3 Batch  211/269, Loss:  0.493\n",
            "Epoch   3 Batch  212/269, Loss:  0.478\n",
            "Epoch   3 Batch  213/269, Loss:  0.464\n",
            "Epoch   3 Batch  214/269, Loss:  0.503\n",
            "Epoch   3 Batch  215/269, Loss:  0.450\n",
            "Epoch   3 Batch  216/269, Loss:  0.528\n",
            "Epoch   3 Batch  217/269, Loss:  0.496\n",
            "Epoch   3 Batch  218/269, Loss:  0.545\n",
            "Epoch   3 Batch  219/269, Loss:  0.479\n",
            "Epoch   3 Batch  220/269, Loss:  0.473\n",
            "Epoch   3 Batch  221/269, Loss:  0.464\n",
            "Epoch   3 Batch  222/269, Loss:  0.453\n",
            "Epoch   3 Batch  223/269, Loss:  0.492\n",
            "Epoch   3 Batch  224/269, Loss:  0.461\n",
            "Epoch   3 Batch  225/269, Loss:  0.485\n",
            "Epoch   3 Batch  226/269, Loss:  0.519\n",
            "Epoch   3 Batch  227/269, Loss:  0.452\n",
            "Epoch   3 Batch  228/269, Loss:  0.472\n",
            "Epoch   3 Batch  229/269, Loss:  0.457\n",
            "Epoch   3 Batch  230/269, Loss:  0.572\n",
            "Epoch   3 Batch  231/269, Loss:  0.473\n",
            "Epoch   3 Batch  232/269, Loss:  0.502\n",
            "Epoch   3 Batch  233/269, Loss:  0.467\n",
            "Epoch   3 Batch  234/269, Loss:  0.534\n",
            "Epoch   3 Batch  235/269, Loss:  0.509\n",
            "Epoch   3 Batch  236/269, Loss:  0.467\n",
            "Epoch   3 Batch  237/269, Loss:  0.425\n",
            "Epoch   3 Batch  238/269, Loss:  0.514\n",
            "Epoch   3 Batch  239/269, Loss:  0.519\n",
            "Epoch   3 Batch  240/269, Loss:  0.459\n",
            "Epoch   3 Batch  241/269, Loss:  0.520\n",
            "Epoch   3 Batch  242/269, Loss:  0.523\n",
            "Epoch   3 Batch  243/269, Loss:  0.466\n",
            "Epoch   3 Batch  244/269, Loss:  0.463\n",
            "Epoch   3 Batch  245/269, Loss:  0.489\n",
            "Epoch   3 Batch  246/269, Loss:  0.595\n",
            "Epoch   3 Batch  247/269, Loss:  0.490\n",
            "Epoch   3 Batch  248/269, Loss:  0.501\n",
            "Epoch   3 Batch  249/269, Loss:  0.470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch   3 Batch  250/269, Loss:  0.488\n",
            "Epoch   3 Batch  251/269, Loss:  0.470\n",
            "Epoch   3 Batch  252/269, Loss:  0.485\n",
            "Epoch   3 Batch  253/269, Loss:  0.511\n",
            "Epoch   3 Batch  254/269, Loss:  0.516\n",
            "Epoch   3 Batch  255/269, Loss:  0.488\n",
            "Epoch   3 Batch  256/269, Loss:  0.486\n",
            "Epoch   3 Batch  257/269, Loss:  0.496\n",
            "Epoch   3 Batch  258/269, Loss:  0.488\n",
            "Epoch   3 Batch  259/269, Loss:  0.508\n",
            "Epoch   3 Batch  260/269, Loss:  0.421\n",
            "Epoch   3 Batch  261/269, Loss:  0.451\n",
            "Epoch   3 Batch  262/269, Loss:  0.509\n",
            "Epoch   3 Batch  263/269, Loss:  0.519\n",
            "Epoch   3 Batch  264/269, Loss:  0.489\n",
            "Epoch   3 Batch  265/269, Loss:  0.493\n",
            "Epoch   3 Batch  266/269, Loss:  0.483\n",
            "Epoch   3 Batch  267/269, Loss:  0.456\n",
            "Epoch   4 Batch    0/269, Loss:  0.516\n",
            "Epoch   4 Batch    1/269, Loss:  0.499\n",
            "Epoch   4 Batch    2/269, Loss:  0.588\n",
            "Epoch   4 Batch    3/269, Loss:  0.481\n",
            "Epoch   4 Batch    4/269, Loss:  0.458\n",
            "Epoch   4 Batch    5/269, Loss:  0.465\n",
            "Epoch   4 Batch    6/269, Loss:  0.485\n",
            "Epoch   4 Batch    7/269, Loss:  0.547\n",
            "Epoch   4 Batch    8/269, Loss:  0.503\n",
            "Epoch   4 Batch    9/269, Loss:  0.425\n",
            "Epoch   4 Batch   10/269, Loss:  0.433\n",
            "Epoch   4 Batch   11/269, Loss:  0.476\n",
            "Epoch   4 Batch   12/269, Loss:  0.440\n",
            "Epoch   4 Batch   13/269, Loss:  0.487\n",
            "Epoch   4 Batch   14/269, Loss:  0.504\n",
            "Epoch   4 Batch   15/269, Loss:  0.504\n",
            "Epoch   4 Batch   16/269, Loss:  0.455\n",
            "Epoch   4 Batch   17/269, Loss:  0.474\n",
            "Epoch   4 Batch   18/269, Loss:  0.492\n",
            "Epoch   4 Batch   19/269, Loss:  0.510\n",
            "Epoch   4 Batch   20/269, Loss:  0.504\n",
            "Epoch   4 Batch   21/269, Loss:  0.477\n",
            "Epoch   4 Batch   22/269, Loss:  0.492\n",
            "Epoch   4 Batch   23/269, Loss:  0.526\n",
            "Epoch   4 Batch   24/269, Loss:  0.494\n",
            "Epoch   4 Batch   25/269, Loss:  0.509\n",
            "Epoch   4 Batch   26/269, Loss:  0.469\n",
            "Epoch   4 Batch   27/269, Loss:  0.514\n",
            "Epoch   4 Batch   28/269, Loss:  0.507\n",
            "Epoch   4 Batch   29/269, Loss:  0.463\n",
            "Epoch   4 Batch   30/269, Loss:  0.459\n",
            "Epoch   4 Batch   31/269, Loss:  0.489\n",
            "Epoch   4 Batch   32/269, Loss:  0.468\n",
            "Epoch   4 Batch   33/269, Loss:  0.525\n",
            "Epoch   4 Batch   34/269, Loss:  0.446\n",
            "Epoch   4 Batch   35/269, Loss:  0.533\n",
            "Epoch   4 Batch   36/269, Loss:  0.499\n",
            "Epoch   4 Batch   37/269, Loss:  0.464\n",
            "Epoch   4 Batch   38/269, Loss:  0.471\n",
            "Epoch   4 Batch   39/269, Loss:  0.542\n",
            "Epoch   4 Batch   40/269, Loss:  0.480\n",
            "Epoch   4 Batch   41/269, Loss:  0.438\n",
            "Epoch   4 Batch   42/269, Loss:  0.504\n",
            "Epoch   4 Batch   43/269, Loss:  0.471\n",
            "Epoch   4 Batch   44/269, Loss:  0.432\n",
            "Epoch   4 Batch   45/269, Loss:  0.459\n",
            "Epoch   4 Batch   46/269, Loss:  0.446\n",
            "Epoch   4 Batch   47/269, Loss:  0.486\n",
            "Epoch   4 Batch   48/269, Loss:  0.498\n",
            "Epoch   4 Batch   49/269, Loss:  0.440\n",
            "Epoch   4 Batch   50/269, Loss:  0.432\n",
            "Epoch   4 Batch   51/269, Loss:  0.436\n",
            "Epoch   4 Batch   52/269, Loss:  0.527\n",
            "Epoch   4 Batch   53/269, Loss:  0.498\n",
            "Epoch   4 Batch   54/269, Loss:  0.403\n",
            "Epoch   4 Batch   55/269, Loss:  0.487\n",
            "Epoch   4 Batch   56/269, Loss:  0.429\n",
            "Epoch   4 Batch   57/269, Loss:  0.460\n",
            "Epoch   4 Batch   58/269, Loss:  0.546\n",
            "Epoch   4 Batch   59/269, Loss:  0.529\n",
            "Epoch   4 Batch   60/269, Loss:  0.476\n",
            "Epoch   4 Batch   61/269, Loss:  0.473\n",
            "Epoch   4 Batch   62/269, Loss:  0.417\n",
            "Epoch   4 Batch   63/269, Loss:  0.451\n",
            "Epoch   4 Batch   64/269, Loss:  0.501\n",
            "Epoch   4 Batch   65/269, Loss:  0.480\n",
            "Epoch   4 Batch   66/269, Loss:  0.508\n",
            "Epoch   4 Batch   67/269, Loss:  0.484\n",
            "Epoch   4 Batch   68/269, Loss:  0.607\n",
            "Epoch   4 Batch   69/269, Loss:  0.481\n",
            "Epoch   4 Batch   70/269, Loss:  0.506\n",
            "Epoch   4 Batch   71/269, Loss:  0.487\n",
            "Epoch   4 Batch   72/269, Loss:  0.491\n",
            "Epoch   4 Batch   73/269, Loss:  0.470\n",
            "Epoch   4 Batch   74/269, Loss:  0.465\n",
            "Epoch   4 Batch   75/269, Loss:  0.556\n",
            "Epoch   4 Batch   76/269, Loss:  0.537\n",
            "Epoch   4 Batch   77/269, Loss:  0.443\n",
            "Epoch   4 Batch   78/269, Loss:  0.530\n",
            "Epoch   4 Batch   79/269, Loss:  0.477\n",
            "Epoch   4 Batch   80/269, Loss:  0.469\n",
            "Epoch   4 Batch   81/269, Loss:  0.440\n",
            "Epoch   4 Batch   82/269, Loss:  0.429\n",
            "Epoch   4 Batch   83/269, Loss:  0.506\n",
            "Epoch   4 Batch   84/269, Loss:  0.521\n",
            "Epoch   4 Batch   85/269, Loss:  0.443\n",
            "Epoch   4 Batch   86/269, Loss:  0.472\n",
            "Epoch   4 Batch   87/269, Loss:  0.470\n",
            "Epoch   4 Batch   88/269, Loss:  0.408\n",
            "Epoch   4 Batch   89/269, Loss:  0.490\n",
            "Epoch   4 Batch   90/269, Loss:  0.512\n",
            "Epoch   4 Batch   91/269, Loss:  0.488\n",
            "Epoch   4 Batch   92/269, Loss:  0.519\n",
            "Epoch   4 Batch   93/269, Loss:  0.468\n",
            "Epoch   4 Batch   94/269, Loss:  0.523\n",
            "Epoch   4 Batch   95/269, Loss:  0.462\n",
            "Epoch   4 Batch   96/269, Loss:  0.521\n",
            "Epoch   4 Batch   97/269, Loss:  0.491\n",
            "Epoch   4 Batch   98/269, Loss:  0.430\n",
            "Epoch   4 Batch   99/269, Loss:  0.461\n",
            "Epoch   4 Batch  100/269, Loss:  0.432\n",
            "Epoch   4 Batch  101/269, Loss:  0.475\n",
            "Epoch   4 Batch  102/269, Loss:  0.496\n",
            "Epoch   4 Batch  103/269, Loss:  0.519\n",
            "Epoch   4 Batch  104/269, Loss:  0.481\n",
            "Epoch   4 Batch  105/269, Loss:  0.460\n",
            "Epoch   4 Batch  106/269, Loss:  0.501\n",
            "Epoch   4 Batch  107/269, Loss:  0.484\n",
            "Epoch   4 Batch  108/269, Loss:  0.476\n",
            "Epoch   4 Batch  109/269, Loss:  0.499\n",
            "Epoch   4 Batch  110/269, Loss:  0.473\n",
            "Epoch   4 Batch  111/269, Loss:  0.464\n",
            "Epoch   4 Batch  112/269, Loss:  0.505\n",
            "Epoch   4 Batch  113/269, Loss:  0.499\n",
            "Epoch   4 Batch  114/269, Loss:  0.466\n",
            "Epoch   4 Batch  115/269, Loss:  0.481\n",
            "Epoch   4 Batch  116/269, Loss:  0.482\n",
            "Epoch   4 Batch  117/269, Loss:  0.486\n",
            "Epoch   4 Batch  118/269, Loss:  0.485\n",
            "Epoch   4 Batch  119/269, Loss:  0.459\n",
            "Epoch   4 Batch  120/269, Loss:  0.501\n",
            "Epoch   4 Batch  121/269, Loss:  0.496\n",
            "Epoch   4 Batch  122/269, Loss:  0.475\n",
            "Epoch   4 Batch  123/269, Loss:  0.543\n",
            "Epoch   4 Batch  124/269, Loss:  0.476\n",
            "Epoch   4 Batch  125/269, Loss:  0.555\n",
            "Epoch   4 Batch  126/269, Loss:  0.444\n",
            "Epoch   4 Batch  127/269, Loss:  0.527\n",
            "Epoch   4 Batch  128/269, Loss:  0.497\n",
            "Epoch   4 Batch  129/269, Loss:  0.451\n",
            "Epoch   4 Batch  130/269, Loss:  0.480\n",
            "Epoch   4 Batch  131/269, Loss:  0.540\n",
            "Epoch   4 Batch  132/269, Loss:  0.460\n",
            "Epoch   4 Batch  133/269, Loss:  0.499\n",
            "Epoch   4 Batch  134/269, Loss:  0.464\n",
            "Epoch   4 Batch  135/269, Loss:  0.550\n",
            "Epoch   4 Batch  136/269, Loss:  0.453\n",
            "Epoch   4 Batch  137/269, Loss:  0.441\n",
            "Epoch   4 Batch  138/269, Loss:  0.508\n",
            "Epoch   4 Batch  139/269, Loss:  0.476\n",
            "Epoch   4 Batch  140/269, Loss:  0.500\n",
            "Epoch   4 Batch  141/269, Loss:  0.559\n",
            "Epoch   4 Batch  142/269, Loss:  0.464\n",
            "Epoch   4 Batch  143/269, Loss:  0.443\n",
            "Epoch   4 Batch  144/269, Loss:  0.462\n",
            "Epoch   4 Batch  145/269, Loss:  0.458\n",
            "Epoch   4 Batch  146/269, Loss:  0.479\n",
            "Epoch   4 Batch  147/269, Loss:  0.540\n",
            "Epoch   4 Batch  148/269, Loss:  0.432\n",
            "Epoch   4 Batch  149/269, Loss:  0.458\n",
            "Epoch   4 Batch  150/269, Loss:  0.528\n",
            "Epoch   4 Batch  151/269, Loss:  0.467\n",
            "Epoch   4 Batch  152/269, Loss:  0.447\n",
            "Epoch   4 Batch  153/269, Loss:  0.478\n",
            "Epoch   4 Batch  154/269, Loss:  0.489\n",
            "Epoch   4 Batch  155/269, Loss:  0.466\n",
            "Epoch   4 Batch  156/269, Loss:  0.478\n",
            "Epoch   4 Batch  157/269, Loss:  0.471\n",
            "Epoch   4 Batch  158/269, Loss:  0.453\n",
            "Epoch   4 Batch  159/269, Loss:  0.495\n",
            "Epoch   4 Batch  160/269, Loss:  0.501\n",
            "Epoch   4 Batch  161/269, Loss:  0.473\n",
            "Epoch   4 Batch  162/269, Loss:  0.501\n",
            "Epoch   4 Batch  163/269, Loss:  0.469\n",
            "Epoch   4 Batch  164/269, Loss:  0.514\n",
            "Epoch   4 Batch  165/269, Loss:  0.550\n",
            "Epoch   4 Batch  166/269, Loss:  0.492\n",
            "Epoch   4 Batch  167/269, Loss:  0.496\n",
            "Epoch   4 Batch  168/269, Loss:  0.470\n",
            "Epoch   4 Batch  169/269, Loss:  0.477\n",
            "Epoch   4 Batch  170/269, Loss:  0.470\n",
            "Epoch   4 Batch  171/269, Loss:  0.503\n",
            "Epoch   4 Batch  172/269, Loss:  0.500\n",
            "Epoch   4 Batch  173/269, Loss:  0.531\n",
            "Epoch   4 Batch  174/269, Loss:  0.463\n",
            "Epoch   4 Batch  175/269, Loss:  0.512\n",
            "Epoch   4 Batch  176/269, Loss:  0.480\n",
            "Epoch   4 Batch  177/269, Loss:  0.440\n",
            "Epoch   4 Batch  178/269, Loss:  0.484\n",
            "Epoch   4 Batch  179/269, Loss:  0.479\n",
            "Epoch   4 Batch  180/269, Loss:  0.445\n",
            "Epoch   4 Batch  181/269, Loss:  0.489\n",
            "Epoch   4 Batch  182/269, Loss:  0.429\n",
            "Epoch   4 Batch  183/269, Loss:  0.492\n",
            "Epoch   4 Batch  184/269, Loss:  0.520\n",
            "Epoch   4 Batch  185/269, Loss:  0.482\n",
            "Epoch   4 Batch  186/269, Loss:  0.490\n",
            "Epoch   4 Batch  187/269, Loss:  0.454\n",
            "Epoch   4 Batch  188/269, Loss:  0.419\n",
            "Epoch   4 Batch  189/269, Loss:  0.473\n",
            "Epoch   4 Batch  190/269, Loss:  0.483\n",
            "Epoch   4 Batch  191/269, Loss:  0.454\n",
            "Epoch   4 Batch  192/269, Loss:  0.485\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch   4 Batch  193/269, Loss:  0.510\n",
            "Epoch   4 Batch  194/269, Loss:  0.454\n",
            "Epoch   4 Batch  195/269, Loss:  0.477\n",
            "Epoch   4 Batch  196/269, Loss:  0.517\n",
            "Epoch   4 Batch  197/269, Loss:  0.501\n",
            "Epoch   4 Batch  198/269, Loss:  0.475\n",
            "Epoch   4 Batch  199/269, Loss:  0.460\n",
            "Epoch   4 Batch  200/269, Loss:  0.509\n",
            "Epoch   4 Batch  201/269, Loss:  0.530\n",
            "Epoch   4 Batch  202/269, Loss:  0.515\n",
            "Epoch   4 Batch  203/269, Loss:  0.453\n",
            "Epoch   4 Batch  204/269, Loss:  0.468\n",
            "Epoch   4 Batch  205/269, Loss:  0.524\n",
            "Epoch   4 Batch  206/269, Loss:  0.488\n",
            "Epoch   4 Batch  207/269, Loss:  0.483\n",
            "Epoch   4 Batch  208/269, Loss:  0.512\n",
            "Epoch   4 Batch  209/269, Loss:  0.426\n",
            "Epoch   4 Batch  210/269, Loss:  0.451\n",
            "Epoch   4 Batch  211/269, Loss:  0.455\n",
            "Epoch   4 Batch  212/269, Loss:  0.478\n",
            "Epoch   4 Batch  213/269, Loss:  0.461\n",
            "Epoch   4 Batch  214/269, Loss:  0.471\n",
            "Epoch   4 Batch  215/269, Loss:  0.464\n",
            "Epoch   4 Batch  216/269, Loss:  0.466\n",
            "Epoch   4 Batch  217/269, Loss:  0.480\n",
            "Epoch   4 Batch  218/269, Loss:  0.525\n",
            "Epoch   4 Batch  219/269, Loss:  0.443\n",
            "Epoch   4 Batch  220/269, Loss:  0.485\n",
            "Epoch   4 Batch  221/269, Loss:  0.446\n",
            "Epoch   4 Batch  222/269, Loss:  0.450\n",
            "Epoch   4 Batch  223/269, Loss:  0.477\n",
            "Epoch   4 Batch  224/269, Loss:  0.468\n",
            "Epoch   4 Batch  225/269, Loss:  0.522\n",
            "Epoch   4 Batch  226/269, Loss:  0.463\n",
            "Epoch   4 Batch  227/269, Loss:  0.395\n",
            "Epoch   4 Batch  228/269, Loss:  0.475\n",
            "Epoch   4 Batch  229/269, Loss:  0.440\n",
            "Epoch   4 Batch  230/269, Loss:  0.525\n",
            "Epoch   4 Batch  231/269, Loss:  0.421\n",
            "Epoch   4 Batch  232/269, Loss:  0.522\n",
            "Epoch   4 Batch  233/269, Loss:  0.464\n",
            "Epoch   4 Batch  234/269, Loss:  0.488\n",
            "Epoch   4 Batch  235/269, Loss:  0.441\n",
            "Epoch   4 Batch  236/269, Loss:  0.468\n",
            "Epoch   4 Batch  237/269, Loss:  0.421\n",
            "Epoch   4 Batch  238/269, Loss:  0.486\n",
            "Epoch   4 Batch  239/269, Loss:  0.486\n",
            "Epoch   4 Batch  240/269, Loss:  0.435\n",
            "Epoch   4 Batch  241/269, Loss:  0.470\n",
            "Epoch   4 Batch  242/269, Loss:  0.489\n",
            "Epoch   4 Batch  243/269, Loss:  0.446\n",
            "Epoch   4 Batch  244/269, Loss:  0.443\n",
            "Epoch   4 Batch  245/269, Loss:  0.470\n",
            "Epoch   4 Batch  246/269, Loss:  0.530\n",
            "Epoch   4 Batch  247/269, Loss:  0.465\n",
            "Epoch   4 Batch  248/269, Loss:  0.517\n",
            "Epoch   4 Batch  249/269, Loss:  0.489\n",
            "Epoch   4 Batch  250/269, Loss:  0.473\n",
            "Epoch   4 Batch  251/269, Loss:  0.459\n",
            "Epoch   4 Batch  252/269, Loss:  0.464\n",
            "Epoch   4 Batch  253/269, Loss:  0.484\n",
            "Epoch   4 Batch  254/269, Loss:  0.475\n",
            "Epoch   4 Batch  255/269, Loss:  0.475\n",
            "Epoch   4 Batch  256/269, Loss:  0.469\n",
            "Epoch   4 Batch  257/269, Loss:  0.487\n",
            "Epoch   4 Batch  258/269, Loss:  0.498\n",
            "Epoch   4 Batch  259/269, Loss:  0.447\n",
            "Epoch   4 Batch  260/269, Loss:  0.429\n",
            "Epoch   4 Batch  261/269, Loss:  0.468\n",
            "Epoch   4 Batch  262/269, Loss:  0.503\n",
            "Epoch   4 Batch  263/269, Loss:  0.498\n",
            "Epoch   4 Batch  264/269, Loss:  0.478\n",
            "Epoch   4 Batch  265/269, Loss:  0.472\n",
            "Epoch   4 Batch  266/269, Loss:  0.461\n",
            "Epoch   4 Batch  267/269, Loss:  0.430\n",
            "Epoch   5 Batch    0/269, Loss:  0.572\n",
            "Epoch   5 Batch    1/269, Loss:  0.506\n",
            "Epoch   5 Batch    2/269, Loss:  0.523\n",
            "Epoch   5 Batch    3/269, Loss:  0.459\n",
            "Epoch   5 Batch    4/269, Loss:  0.458\n",
            "Epoch   5 Batch    5/269, Loss:  0.446\n",
            "Epoch   5 Batch    6/269, Loss:  0.492\n",
            "Epoch   5 Batch    7/269, Loss:  0.523\n",
            "Epoch   5 Batch    8/269, Loss:  0.446\n",
            "Epoch   5 Batch    9/269, Loss:  0.430\n",
            "Epoch   5 Batch   10/269, Loss:  0.407\n",
            "Epoch   5 Batch   11/269, Loss:  0.457\n",
            "Epoch   5 Batch   12/269, Loss:  0.484\n",
            "Epoch   5 Batch   13/269, Loss:  0.483\n",
            "Epoch   5 Batch   14/269, Loss:  0.485\n",
            "Epoch   5 Batch   15/269, Loss:  0.505\n",
            "Epoch   5 Batch   16/269, Loss:  0.464\n",
            "Epoch   5 Batch   17/269, Loss:  0.484\n",
            "Epoch   5 Batch   18/269, Loss:  0.465\n",
            "Epoch   5 Batch   19/269, Loss:  0.513\n",
            "Epoch   5 Batch   20/269, Loss:  0.477\n",
            "Epoch   5 Batch   21/269, Loss:  0.465\n",
            "Epoch   5 Batch   22/269, Loss:  0.473\n",
            "Epoch   5 Batch   23/269, Loss:  0.509\n",
            "Epoch   5 Batch   24/269, Loss:  0.508\n",
            "Epoch   5 Batch   25/269, Loss:  0.507\n",
            "Epoch   5 Batch   26/269, Loss:  0.451\n",
            "Epoch   5 Batch   27/269, Loss:  0.508\n",
            "Epoch   5 Batch   28/269, Loss:  0.517\n",
            "Epoch   5 Batch   29/269, Loss:  0.473\n",
            "Epoch   5 Batch   30/269, Loss:  0.469\n",
            "Epoch   5 Batch   31/269, Loss:  0.488\n",
            "Epoch   5 Batch   32/269, Loss:  0.448\n",
            "Epoch   5 Batch   33/269, Loss:  0.481\n",
            "Epoch   5 Batch   34/269, Loss:  0.442\n",
            "Epoch   5 Batch   35/269, Loss:  0.514\n",
            "Epoch   5 Batch   36/269, Loss:  0.500\n",
            "Epoch   5 Batch   37/269, Loss:  0.465\n",
            "Epoch   5 Batch   38/269, Loss:  0.446\n",
            "Epoch   5 Batch   39/269, Loss:  0.490\n",
            "Epoch   5 Batch   40/269, Loss:  0.452\n",
            "Epoch   5 Batch   41/269, Loss:  0.454\n",
            "Epoch   5 Batch   42/269, Loss:  0.465\n",
            "Epoch   5 Batch   43/269, Loss:  0.503\n",
            "Epoch   5 Batch   44/269, Loss:  0.432\n",
            "Epoch   5 Batch   45/269, Loss:  0.464\n",
            "Epoch   5 Batch   46/269, Loss:  0.447\n",
            "Epoch   5 Batch   47/269, Loss:  0.498\n",
            "Epoch   5 Batch   48/269, Loss:  0.478\n",
            "Epoch   5 Batch   49/269, Loss:  0.450\n",
            "Epoch   5 Batch   50/269, Loss:  0.443\n",
            "Epoch   5 Batch   51/269, Loss:  0.458\n",
            "Epoch   5 Batch   52/269, Loss:  0.503\n",
            "Epoch   5 Batch   53/269, Loss:  0.496\n",
            "Epoch   5 Batch   54/269, Loss:  0.402\n",
            "Epoch   5 Batch   55/269, Loss:  0.520\n",
            "Epoch   5 Batch   56/269, Loss:  0.446\n",
            "Epoch   5 Batch   57/269, Loss:  0.488\n",
            "Epoch   5 Batch   58/269, Loss:  0.512\n",
            "Epoch   5 Batch   59/269, Loss:  0.488\n",
            "Epoch   5 Batch   60/269, Loss:  0.492\n",
            "Epoch   5 Batch   61/269, Loss:  0.489\n",
            "Epoch   5 Batch   62/269, Loss:  0.429\n",
            "Epoch   5 Batch   63/269, Loss:  0.438\n",
            "Epoch   5 Batch   64/269, Loss:  0.497\n",
            "Epoch   5 Batch   65/269, Loss:  0.490\n",
            "Epoch   5 Batch   66/269, Loss:  0.531\n",
            "Epoch   5 Batch   67/269, Loss:  0.486\n",
            "Epoch   5 Batch   68/269, Loss:  0.579\n",
            "Epoch   5 Batch   69/269, Loss:  0.496\n",
            "Epoch   5 Batch   70/269, Loss:  0.526\n",
            "Epoch   5 Batch   71/269, Loss:  0.458\n",
            "Epoch   5 Batch   72/269, Loss:  0.494\n",
            "Epoch   5 Batch   73/269, Loss:  0.488\n",
            "Epoch   5 Batch   74/269, Loss:  0.442\n",
            "Epoch   5 Batch   75/269, Loss:  0.582\n",
            "Epoch   5 Batch   76/269, Loss:  0.522\n",
            "Epoch   5 Batch   77/269, Loss:  0.455\n",
            "Epoch   5 Batch   78/269, Loss:  0.520\n",
            "Epoch   5 Batch   79/269, Loss:  0.454\n",
            "Epoch   5 Batch   80/269, Loss:  0.567\n",
            "Epoch   5 Batch   81/269, Loss:  0.437\n",
            "Epoch   5 Batch   82/269, Loss:  0.416\n",
            "Epoch   5 Batch   83/269, Loss:  0.505\n",
            "Epoch   5 Batch   84/269, Loss:  0.528\n",
            "Epoch   5 Batch   85/269, Loss:  0.433\n",
            "Epoch   5 Batch   86/269, Loss:  0.466\n",
            "Epoch   5 Batch   87/269, Loss:  0.439\n",
            "Epoch   5 Batch   88/269, Loss:  0.423\n",
            "Epoch   5 Batch   89/269, Loss:  0.473\n",
            "Epoch   5 Batch   90/269, Loss:  0.496\n",
            "Epoch   5 Batch   91/269, Loss:  0.504\n",
            "Epoch   5 Batch   92/269, Loss:  0.562\n",
            "Epoch   5 Batch   93/269, Loss:  0.494\n",
            "Epoch   5 Batch   94/269, Loss:  0.440\n",
            "Epoch   5 Batch   95/269, Loss:  0.474\n",
            "Epoch   5 Batch   96/269, Loss:  0.534\n",
            "Epoch   5 Batch   97/269, Loss:  0.482\n",
            "Epoch   5 Batch   98/269, Loss:  0.434\n",
            "Epoch   5 Batch   99/269, Loss:  0.484\n",
            "Epoch   5 Batch  100/269, Loss:  0.407\n",
            "Epoch   5 Batch  101/269, Loss:  0.512\n",
            "Epoch   5 Batch  102/269, Loss:  0.524\n",
            "Epoch   5 Batch  103/269, Loss:  0.492\n",
            "Epoch   5 Batch  104/269, Loss:  0.523\n",
            "Epoch   5 Batch  105/269, Loss:  0.472\n",
            "Epoch   5 Batch  106/269, Loss:  0.468\n",
            "Epoch   5 Batch  107/269, Loss:  0.465\n",
            "Epoch   5 Batch  108/269, Loss:  0.438\n",
            "Epoch   5 Batch  109/269, Loss:  0.531\n",
            "Epoch   5 Batch  110/269, Loss:  0.458\n",
            "Epoch   5 Batch  111/269, Loss:  0.454\n",
            "Epoch   5 Batch  112/269, Loss:  0.493\n",
            "Epoch   5 Batch  113/269, Loss:  0.483\n",
            "Epoch   5 Batch  114/269, Loss:  0.452\n",
            "Epoch   5 Batch  115/269, Loss:  0.518\n",
            "Epoch   5 Batch  116/269, Loss:  0.463\n",
            "Epoch   5 Batch  117/269, Loss:  0.495\n",
            "Epoch   5 Batch  118/269, Loss:  0.432\n",
            "Epoch   5 Batch  119/269, Loss:  0.466\n",
            "Epoch   5 Batch  120/269, Loss:  0.548\n",
            "Epoch   5 Batch  121/269, Loss:  0.520\n",
            "Epoch   5 Batch  122/269, Loss:  0.500\n",
            "Epoch   5 Batch  123/269, Loss:  0.474\n",
            "Epoch   5 Batch  124/269, Loss:  0.538\n",
            "Epoch   5 Batch  125/269, Loss:  0.536\n",
            "Epoch   5 Batch  126/269, Loss:  0.444\n",
            "Epoch   5 Batch  127/269, Loss:  0.554\n",
            "Epoch   5 Batch  128/269, Loss:  0.545\n",
            "Epoch   5 Batch  129/269, Loss:  0.451\n",
            "Epoch   5 Batch  130/269, Loss:  0.469\n",
            "Epoch   5 Batch  131/269, Loss:  0.481\n",
            "Epoch   5 Batch  132/269, Loss:  0.465\n",
            "Epoch   5 Batch  133/269, Loss:  0.503\n",
            "Epoch   5 Batch  134/269, Loss:  0.465\n",
            "Epoch   5 Batch  135/269, Loss:  0.606\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch   5 Batch  136/269, Loss:  0.482\n",
            "Epoch   5 Batch  137/269, Loss:  0.455\n",
            "Epoch   5 Batch  138/269, Loss:  0.500\n",
            "Epoch   5 Batch  139/269, Loss:  0.514\n",
            "Epoch   5 Batch  140/269, Loss:  0.494\n",
            "Epoch   5 Batch  141/269, Loss:  0.526\n",
            "Epoch   5 Batch  142/269, Loss:  0.442\n",
            "Epoch   5 Batch  143/269, Loss:  0.444\n",
            "Epoch   5 Batch  144/269, Loss:  0.495\n",
            "Epoch   5 Batch  145/269, Loss:  0.420\n",
            "Epoch   5 Batch  146/269, Loss:  0.506\n",
            "Epoch   5 Batch  147/269, Loss:  0.535\n",
            "Epoch   5 Batch  148/269, Loss:  0.437\n",
            "Epoch   5 Batch  149/269, Loss:  0.445\n",
            "Epoch   5 Batch  150/269, Loss:  0.498\n",
            "Epoch   5 Batch  151/269, Loss:  0.435\n",
            "Epoch   5 Batch  152/269, Loss:  0.452\n",
            "Epoch   5 Batch  153/269, Loss:  0.495\n",
            "Epoch   5 Batch  154/269, Loss:  0.469\n",
            "Epoch   5 Batch  155/269, Loss:  0.476\n",
            "Epoch   5 Batch  156/269, Loss:  0.455\n",
            "Epoch   5 Batch  157/269, Loss:  0.475\n",
            "Epoch   5 Batch  158/269, Loss:  0.425\n",
            "Epoch   5 Batch  159/269, Loss:  0.435\n",
            "Epoch   5 Batch  160/269, Loss:  0.528\n",
            "Epoch   5 Batch  161/269, Loss:  0.467\n",
            "Epoch   5 Batch  162/269, Loss:  0.447\n",
            "Epoch   5 Batch  163/269, Loss:  0.462\n",
            "Epoch   5 Batch  164/269, Loss:  0.498\n",
            "Epoch   5 Batch  165/269, Loss:  0.584\n",
            "Epoch   5 Batch  166/269, Loss:  0.507\n",
            "Epoch   5 Batch  167/269, Loss:  0.484\n",
            "Epoch   5 Batch  168/269, Loss:  0.470\n",
            "Epoch   5 Batch  169/269, Loss:  0.479\n",
            "Epoch   5 Batch  170/269, Loss:  0.470\n",
            "Epoch   5 Batch  171/269, Loss:  0.499\n",
            "Epoch   5 Batch  172/269, Loss:  0.540\n",
            "Epoch   5 Batch  173/269, Loss:  0.531\n",
            "Epoch   5 Batch  174/269, Loss:  0.488\n",
            "Epoch   5 Batch  175/269, Loss:  0.589\n",
            "Epoch   5 Batch  176/269, Loss:  0.473\n",
            "Epoch   5 Batch  177/269, Loss:  0.431\n",
            "Epoch   5 Batch  178/269, Loss:  0.491\n",
            "Epoch   5 Batch  179/269, Loss:  0.455\n",
            "Epoch   5 Batch  180/269, Loss:  0.415\n",
            "Epoch   5 Batch  181/269, Loss:  0.487\n",
            "Epoch   5 Batch  182/269, Loss:  0.461\n",
            "Epoch   5 Batch  183/269, Loss:  0.494\n",
            "Epoch   5 Batch  184/269, Loss:  0.510\n",
            "Epoch   5 Batch  185/269, Loss:  0.500\n",
            "Epoch   5 Batch  186/269, Loss:  0.472\n",
            "Epoch   5 Batch  187/269, Loss:  0.443\n",
            "Epoch   5 Batch  188/269, Loss:  0.452\n",
            "Epoch   5 Batch  189/269, Loss:  0.477\n",
            "Epoch   5 Batch  190/269, Loss:  0.505\n",
            "Epoch   5 Batch  191/269, Loss:  0.470\n",
            "Epoch   5 Batch  192/269, Loss:  0.447\n",
            "Epoch   5 Batch  193/269, Loss:  0.516\n",
            "Epoch   5 Batch  194/269, Loss:  0.450\n",
            "Epoch   5 Batch  195/269, Loss:  0.518\n",
            "Epoch   5 Batch  196/269, Loss:  0.464\n",
            "Epoch   5 Batch  197/269, Loss:  0.479\n",
            "Epoch   5 Batch  198/269, Loss:  0.464\n",
            "Epoch   5 Batch  199/269, Loss:  0.525\n",
            "Epoch   5 Batch  200/269, Loss:  0.513\n",
            "Epoch   5 Batch  201/269, Loss:  0.563\n",
            "Epoch   5 Batch  202/269, Loss:  0.540\n",
            "Epoch   5 Batch  203/269, Loss:  0.463\n",
            "Epoch   5 Batch  204/269, Loss:  0.496\n",
            "Epoch   5 Batch  205/269, Loss:  0.475\n",
            "Epoch   5 Batch  206/269, Loss:  0.509\n",
            "Epoch   5 Batch  207/269, Loss:  0.450\n",
            "Epoch   5 Batch  208/269, Loss:  0.469\n",
            "Epoch   5 Batch  209/269, Loss:  0.419\n",
            "Epoch   5 Batch  210/269, Loss:  0.487\n",
            "Epoch   5 Batch  211/269, Loss:  0.493\n",
            "Epoch   5 Batch  212/269, Loss:  0.473\n",
            "Epoch   5 Batch  213/269, Loss:  0.474\n",
            "Epoch   5 Batch  214/269, Loss:  0.517\n",
            "Epoch   5 Batch  215/269, Loss:  0.437\n",
            "Epoch   5 Batch  216/269, Loss:  0.508\n",
            "Epoch   5 Batch  217/269, Loss:  0.507\n",
            "Epoch   5 Batch  218/269, Loss:  0.540\n",
            "Epoch   5 Batch  219/269, Loss:  0.485\n",
            "Epoch   5 Batch  220/269, Loss:  0.458\n",
            "Epoch   5 Batch  221/269, Loss:  0.455\n",
            "Epoch   5 Batch  222/269, Loss:  0.475\n",
            "Epoch   5 Batch  223/269, Loss:  0.505\n",
            "Epoch   5 Batch  224/269, Loss:  0.447\n",
            "Epoch   5 Batch  225/269, Loss:  0.485\n",
            "Epoch   5 Batch  226/269, Loss:  0.480\n",
            "Epoch   5 Batch  227/269, Loss:  0.452\n",
            "Epoch   5 Batch  228/269, Loss:  0.472\n",
            "Epoch   5 Batch  229/269, Loss:  0.471\n",
            "Epoch   5 Batch  230/269, Loss:  0.527\n",
            "Epoch   5 Batch  231/269, Loss:  0.445\n",
            "Epoch   5 Batch  232/269, Loss:  0.486\n",
            "Epoch   5 Batch  233/269, Loss:  0.473\n",
            "Epoch   5 Batch  234/269, Loss:  0.462\n",
            "Epoch   5 Batch  235/269, Loss:  0.480\n",
            "Epoch   5 Batch  236/269, Loss:  0.450\n",
            "Epoch   5 Batch  237/269, Loss:  0.436\n",
            "Epoch   5 Batch  238/269, Loss:  0.472\n",
            "Epoch   5 Batch  239/269, Loss:  0.474\n",
            "Epoch   5 Batch  240/269, Loss:  0.431\n",
            "Epoch   5 Batch  241/269, Loss:  0.507\n",
            "Epoch   5 Batch  242/269, Loss:  0.462\n",
            "Epoch   5 Batch  243/269, Loss:  0.438\n",
            "Epoch   5 Batch  244/269, Loss:  0.512\n",
            "Epoch   5 Batch  245/269, Loss:  0.464\n",
            "Epoch   5 Batch  246/269, Loss:  0.583\n",
            "Epoch   5 Batch  247/269, Loss:  0.456\n",
            "Epoch   5 Batch  248/269, Loss:  0.508\n",
            "Epoch   5 Batch  249/269, Loss:  0.457\n",
            "Epoch   5 Batch  250/269, Loss:  0.465\n",
            "Epoch   5 Batch  251/269, Loss:  0.493\n",
            "Epoch   5 Batch  252/269, Loss:  0.457\n",
            "Epoch   5 Batch  253/269, Loss:  0.501\n",
            "Epoch   5 Batch  254/269, Loss:  0.500\n",
            "Epoch   5 Batch  255/269, Loss:  0.481\n",
            "Epoch   5 Batch  256/269, Loss:  0.454\n",
            "Epoch   5 Batch  257/269, Loss:  0.488\n",
            "Epoch   5 Batch  258/269, Loss:  0.529\n",
            "Epoch   5 Batch  259/269, Loss:  0.472\n",
            "Epoch   5 Batch  260/269, Loss:  0.443\n",
            "Epoch   5 Batch  261/269, Loss:  0.448\n",
            "Epoch   5 Batch  262/269, Loss:  0.497\n",
            "Epoch   5 Batch  263/269, Loss:  0.501\n",
            "Epoch   5 Batch  264/269, Loss:  0.436\n",
            "Epoch   5 Batch  265/269, Loss:  0.504\n",
            "Epoch   5 Batch  266/269, Loss:  0.458\n",
            "Epoch   5 Batch  267/269, Loss:  0.447\n",
            "Epoch   6 Batch    0/269, Loss:  0.512\n",
            "Epoch   6 Batch    1/269, Loss:  0.491\n",
            "Epoch   6 Batch    2/269, Loss:  0.544\n",
            "Epoch   6 Batch    3/269, Loss:  0.461\n",
            "Epoch   6 Batch    4/269, Loss:  0.438\n",
            "Epoch   6 Batch    5/269, Loss:  0.444\n",
            "Epoch   6 Batch    6/269, Loss:  0.502\n",
            "Epoch   6 Batch    7/269, Loss:  0.491\n",
            "Epoch   6 Batch    8/269, Loss:  0.494\n",
            "Epoch   6 Batch    9/269, Loss:  0.425\n",
            "Epoch   6 Batch   10/269, Loss:  0.438\n",
            "Epoch   6 Batch   11/269, Loss:  0.497\n",
            "Epoch   6 Batch   12/269, Loss:  0.479\n",
            "Epoch   6 Batch   13/269, Loss:  0.486\n",
            "Epoch   6 Batch   14/269, Loss:  0.481\n",
            "Epoch   6 Batch   15/269, Loss:  0.481\n",
            "Epoch   6 Batch   16/269, Loss:  0.440\n",
            "Epoch   6 Batch   17/269, Loss:  0.469\n",
            "Epoch   6 Batch   18/269, Loss:  0.451\n",
            "Epoch   6 Batch   19/269, Loss:  0.510\n",
            "Epoch   6 Batch   20/269, Loss:  0.465\n",
            "Epoch   6 Batch   21/269, Loss:  0.464\n",
            "Epoch   6 Batch   22/269, Loss:  0.444\n",
            "Epoch   6 Batch   23/269, Loss:  0.502\n",
            "Epoch   6 Batch   24/269, Loss:  0.477\n",
            "Epoch   6 Batch   25/269, Loss:  0.503\n",
            "Epoch   6 Batch   26/269, Loss:  0.472\n",
            "Epoch   6 Batch   27/269, Loss:  0.504\n",
            "Epoch   6 Batch   28/269, Loss:  0.500\n",
            "Epoch   6 Batch   29/269, Loss:  0.426\n",
            "Epoch   6 Batch   30/269, Loss:  0.445\n",
            "Epoch   6 Batch   31/269, Loss:  0.446\n",
            "Epoch   6 Batch   32/269, Loss:  0.441\n",
            "Epoch   6 Batch   33/269, Loss:  0.503\n",
            "Epoch   6 Batch   34/269, Loss:  0.440\n",
            "Epoch   6 Batch   35/269, Loss:  0.464\n",
            "Epoch   6 Batch   36/269, Loss:  0.474\n",
            "Epoch   6 Batch   37/269, Loss:  0.448\n",
            "Epoch   6 Batch   38/269, Loss:  0.451\n",
            "Epoch   6 Batch   39/269, Loss:  0.530\n",
            "Epoch   6 Batch   40/269, Loss:  0.441\n",
            "Epoch   6 Batch   41/269, Loss:  0.492\n",
            "Epoch   6 Batch   42/269, Loss:  0.547\n",
            "Epoch   6 Batch   43/269, Loss:  0.481\n",
            "Epoch   6 Batch   44/269, Loss:  0.506\n",
            "Epoch   6 Batch   45/269, Loss:  0.458\n",
            "Epoch   6 Batch   46/269, Loss:  0.461\n",
            "Epoch   6 Batch   47/269, Loss:  0.485\n",
            "Epoch   6 Batch   48/269, Loss:  0.526\n",
            "Epoch   6 Batch   49/269, Loss:  0.474\n",
            "Epoch   6 Batch   50/269, Loss:  0.490\n",
            "Epoch   6 Batch   51/269, Loss:  0.460\n",
            "Epoch   6 Batch   52/269, Loss:  0.537\n",
            "Epoch   6 Batch   53/269, Loss:  0.485\n",
            "Epoch   6 Batch   54/269, Loss:  0.422\n",
            "Epoch   6 Batch   55/269, Loss:  0.507\n",
            "Epoch   6 Batch   56/269, Loss:  0.499\n",
            "Epoch   6 Batch   57/269, Loss:  0.444\n",
            "Epoch   6 Batch   58/269, Loss:  0.543\n",
            "Epoch   6 Batch   59/269, Loss:  0.503\n",
            "Epoch   6 Batch   60/269, Loss:  0.477\n",
            "Epoch   6 Batch   61/269, Loss:  0.526\n",
            "Epoch   6 Batch   62/269, Loss:  0.476\n",
            "Epoch   6 Batch   63/269, Loss:  0.458\n",
            "Epoch   6 Batch   64/269, Loss:  0.514\n",
            "Epoch   6 Batch   65/269, Loss:  0.479\n",
            "Epoch   6 Batch   66/269, Loss:  0.510\n",
            "Epoch   6 Batch   67/269, Loss:  0.468\n",
            "Epoch   6 Batch   68/269, Loss:  0.590\n",
            "Epoch   6 Batch   69/269, Loss:  0.513\n",
            "Epoch   6 Batch   70/269, Loss:  0.527\n",
            "Epoch   6 Batch   71/269, Loss:  0.477\n",
            "Epoch   6 Batch   72/269, Loss:  0.548\n",
            "Epoch   6 Batch   73/269, Loss:  0.484\n",
            "Epoch   6 Batch   74/269, Loss:  0.466\n",
            "Epoch   6 Batch   75/269, Loss:  0.626\n",
            "Epoch   6 Batch   76/269, Loss:  0.532\n",
            "Epoch   6 Batch   77/269, Loss:  0.480\n",
            "Epoch   6 Batch   78/269, Loss:  0.528\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch   6 Batch   79/269, Loss:  0.476\n",
            "Epoch   6 Batch   80/269, Loss:  0.486\n",
            "Epoch   6 Batch   81/269, Loss:  0.429\n",
            "Epoch   6 Batch   82/269, Loss:  0.451\n",
            "Epoch   6 Batch   83/269, Loss:  0.509\n",
            "Epoch   6 Batch   84/269, Loss:  0.531\n",
            "Epoch   6 Batch   85/269, Loss:  0.485\n",
            "Epoch   6 Batch   86/269, Loss:  0.501\n",
            "Epoch   6 Batch   87/269, Loss:  0.466\n",
            "Epoch   6 Batch   88/269, Loss:  0.457\n",
            "Epoch   6 Batch   89/269, Loss:  0.533\n",
            "Epoch   6 Batch   90/269, Loss:  0.547\n",
            "Epoch   6 Batch   91/269, Loss:  0.510\n",
            "Epoch   6 Batch   92/269, Loss:  0.483\n",
            "Epoch   6 Batch   93/269, Loss:  0.484\n",
            "Epoch   6 Batch   94/269, Loss:  0.491\n",
            "Epoch   6 Batch   95/269, Loss:  0.494\n",
            "Epoch   6 Batch   96/269, Loss:  0.522\n",
            "Epoch   6 Batch   97/269, Loss:  0.493\n",
            "Epoch   6 Batch   98/269, Loss:  0.427\n",
            "Epoch   6 Batch   99/269, Loss:  0.517\n",
            "Epoch   6 Batch  100/269, Loss:  0.435\n",
            "Epoch   6 Batch  101/269, Loss:  0.499\n",
            "Epoch   6 Batch  102/269, Loss:  0.535\n",
            "Epoch   6 Batch  103/269, Loss:  0.549\n",
            "Epoch   6 Batch  104/269, Loss:  0.473\n",
            "Epoch   6 Batch  105/269, Loss:  0.462\n",
            "Epoch   6 Batch  106/269, Loss:  0.489\n",
            "Epoch   6 Batch  107/269, Loss:  0.468\n",
            "Epoch   6 Batch  108/269, Loss:  0.432\n",
            "Epoch   6 Batch  109/269, Loss:  0.606\n",
            "Epoch   6 Batch  110/269, Loss:  0.476\n",
            "Epoch   6 Batch  111/269, Loss:  0.490\n",
            "Epoch   6 Batch  112/269, Loss:  0.528\n",
            "Epoch   6 Batch  113/269, Loss:  0.520\n",
            "Epoch   6 Batch  114/269, Loss:  0.472\n",
            "Epoch   6 Batch  115/269, Loss:  0.507\n",
            "Epoch   6 Batch  116/269, Loss:  0.456\n",
            "Epoch   6 Batch  117/269, Loss:  0.483\n",
            "Epoch   6 Batch  118/269, Loss:  0.435\n",
            "Epoch   6 Batch  119/269, Loss:  0.451\n",
            "Epoch   6 Batch  120/269, Loss:  0.528\n",
            "Epoch   6 Batch  121/269, Loss:  0.447\n",
            "Epoch   6 Batch  122/269, Loss:  0.478\n",
            "Epoch   6 Batch  123/269, Loss:  0.521\n",
            "Epoch   6 Batch  124/269, Loss:  0.491\n",
            "Epoch   6 Batch  125/269, Loss:  0.589\n",
            "Epoch   6 Batch  126/269, Loss:  0.475\n",
            "Epoch   6 Batch  127/269, Loss:  0.517\n",
            "Epoch   6 Batch  128/269, Loss:  0.472\n",
            "Epoch   6 Batch  129/269, Loss:  0.447\n",
            "Epoch   6 Batch  130/269, Loss:  0.475\n",
            "Epoch   6 Batch  131/269, Loss:  0.468\n",
            "Epoch   6 Batch  132/269, Loss:  0.514\n",
            "Epoch   6 Batch  133/269, Loss:  0.494\n",
            "Epoch   6 Batch  134/269, Loss:  0.472\n",
            "Epoch   6 Batch  135/269, Loss:  0.499\n",
            "Epoch   6 Batch  136/269, Loss:  0.444\n",
            "Epoch   6 Batch  137/269, Loss:  0.464\n",
            "Epoch   6 Batch  138/269, Loss:  0.501\n",
            "Epoch   6 Batch  139/269, Loss:  0.495\n",
            "Epoch   6 Batch  140/269, Loss:  0.495\n",
            "Epoch   6 Batch  141/269, Loss:  0.530\n",
            "Epoch   6 Batch  142/269, Loss:  0.479\n",
            "Epoch   6 Batch  143/269, Loss:  0.560\n",
            "Epoch   6 Batch  144/269, Loss:  0.457\n",
            "Epoch   6 Batch  145/269, Loss:  0.465\n",
            "Epoch   6 Batch  146/269, Loss:  0.455\n",
            "Epoch   6 Batch  147/269, Loss:  0.497\n",
            "Epoch   6 Batch  148/269, Loss:  0.467\n",
            "Epoch   6 Batch  149/269, Loss:  0.482\n",
            "Epoch   6 Batch  150/269, Loss:  0.456\n",
            "Epoch   6 Batch  151/269, Loss:  0.436\n",
            "Epoch   6 Batch  152/269, Loss:  0.444\n",
            "Epoch   6 Batch  153/269, Loss:  0.476\n",
            "Epoch   6 Batch  154/269, Loss:  0.465\n",
            "Epoch   6 Batch  155/269, Loss:  0.490\n",
            "Epoch   6 Batch  156/269, Loss:  0.493\n",
            "Epoch   6 Batch  157/269, Loss:  0.501\n",
            "Epoch   6 Batch  158/269, Loss:  0.451\n",
            "Epoch   6 Batch  159/269, Loss:  0.451\n",
            "Epoch   6 Batch  160/269, Loss:  0.517\n",
            "Epoch   6 Batch  161/269, Loss:  0.512\n",
            "Epoch   6 Batch  162/269, Loss:  0.477\n",
            "Epoch   6 Batch  163/269, Loss:  0.457\n",
            "Epoch   6 Batch  164/269, Loss:  0.507\n",
            "Epoch   6 Batch  165/269, Loss:  0.557\n",
            "Epoch   6 Batch  166/269, Loss:  0.456\n",
            "Epoch   6 Batch  167/269, Loss:  0.455\n",
            "Epoch   6 Batch  168/269, Loss:  0.496\n",
            "Epoch   6 Batch  169/269, Loss:  0.489\n",
            "Epoch   6 Batch  170/269, Loss:  0.458\n",
            "Epoch   6 Batch  171/269, Loss:  0.537\n",
            "Epoch   6 Batch  172/269, Loss:  0.494\n",
            "Epoch   6 Batch  173/269, Loss:  0.509\n",
            "Epoch   6 Batch  174/269, Loss:  0.482\n",
            "Epoch   6 Batch  175/269, Loss:  0.536\n",
            "Epoch   6 Batch  176/269, Loss:  0.443\n",
            "Epoch   6 Batch  177/269, Loss:  0.426\n",
            "Epoch   6 Batch  178/269, Loss:  0.474\n",
            "Epoch   6 Batch  179/269, Loss:  0.500\n",
            "Epoch   6 Batch  180/269, Loss:  0.429\n",
            "Epoch   6 Batch  181/269, Loss:  0.492\n",
            "Epoch   6 Batch  182/269, Loss:  0.449\n",
            "Epoch   6 Batch  183/269, Loss:  0.469\n",
            "Epoch   6 Batch  184/269, Loss:  0.539\n",
            "Epoch   6 Batch  185/269, Loss:  0.492\n",
            "Epoch   6 Batch  186/269, Loss:  0.496\n",
            "Epoch   6 Batch  187/269, Loss:  0.455\n",
            "Epoch   6 Batch  188/269, Loss:  0.465\n",
            "Epoch   6 Batch  189/269, Loss:  0.525\n",
            "Epoch   6 Batch  190/269, Loss:  0.476\n",
            "Epoch   6 Batch  191/269, Loss:  0.460\n",
            "Epoch   6 Batch  192/269, Loss:  0.441\n",
            "Epoch   6 Batch  193/269, Loss:  0.459\n",
            "Epoch   6 Batch  194/269, Loss:  0.444\n",
            "Epoch   6 Batch  195/269, Loss:  0.573\n",
            "Epoch   6 Batch  196/269, Loss:  0.475\n",
            "Epoch   6 Batch  197/269, Loss:  0.483\n",
            "Epoch   6 Batch  198/269, Loss:  0.500\n",
            "Epoch   6 Batch  199/269, Loss:  0.516\n",
            "Epoch   6 Batch  200/269, Loss:  0.509\n",
            "Epoch   6 Batch  201/269, Loss:  0.537\n",
            "Epoch   6 Batch  202/269, Loss:  0.518\n",
            "Epoch   6 Batch  203/269, Loss:  0.452\n",
            "Epoch   6 Batch  204/269, Loss:  0.522\n",
            "Epoch   6 Batch  205/269, Loss:  0.522\n",
            "Epoch   6 Batch  206/269, Loss:  0.536\n",
            "Epoch   6 Batch  207/269, Loss:  0.548\n",
            "Epoch   6 Batch  208/269, Loss:  0.514\n",
            "Epoch   6 Batch  209/269, Loss:  0.466\n",
            "Epoch   6 Batch  210/269, Loss:  0.468\n",
            "Epoch   6 Batch  211/269, Loss:  0.475\n",
            "Epoch   6 Batch  212/269, Loss:  0.508\n",
            "Epoch   6 Batch  213/269, Loss:  0.482\n",
            "Epoch   6 Batch  214/269, Loss:  0.473\n",
            "Epoch   6 Batch  215/269, Loss:  0.468\n",
            "Epoch   6 Batch  216/269, Loss:  0.494\n",
            "Epoch   6 Batch  217/269, Loss:  0.503\n",
            "Epoch   6 Batch  218/269, Loss:  0.514\n",
            "Epoch   6 Batch  219/269, Loss:  0.484\n",
            "Epoch   6 Batch  220/269, Loss:  0.466\n",
            "Epoch   6 Batch  221/269, Loss:  0.459\n",
            "Epoch   6 Batch  222/269, Loss:  0.492\n",
            "Epoch   6 Batch  223/269, Loss:  0.478\n",
            "Epoch   6 Batch  224/269, Loss:  0.466\n",
            "Epoch   6 Batch  225/269, Loss:  0.475\n",
            "Epoch   6 Batch  226/269, Loss:  0.513\n",
            "Epoch   6 Batch  227/269, Loss:  0.458\n",
            "Epoch   6 Batch  228/269, Loss:  0.462\n",
            "Epoch   6 Batch  229/269, Loss:  0.430\n",
            "Epoch   6 Batch  230/269, Loss:  0.497\n",
            "Epoch   6 Batch  231/269, Loss:  0.454\n",
            "Epoch   6 Batch  232/269, Loss:  0.552\n",
            "Epoch   6 Batch  233/269, Loss:  0.518\n",
            "Epoch   6 Batch  234/269, Loss:  0.486\n",
            "Epoch   6 Batch  235/269, Loss:  0.512\n",
            "Epoch   6 Batch  236/269, Loss:  0.449\n",
            "Epoch   6 Batch  237/269, Loss:  0.454\n",
            "Epoch   6 Batch  238/269, Loss:  0.459\n",
            "Epoch   6 Batch  239/269, Loss:  0.526\n",
            "Epoch   6 Batch  240/269, Loss:  0.471\n",
            "Epoch   6 Batch  241/269, Loss:  0.526\n",
            "Epoch   6 Batch  242/269, Loss:  0.509\n",
            "Epoch   6 Batch  243/269, Loss:  0.490\n",
            "Epoch   6 Batch  244/269, Loss:  0.539\n",
            "Epoch   6 Batch  245/269, Loss:  0.524\n",
            "Epoch   6 Batch  246/269, Loss:  0.565\n",
            "Epoch   6 Batch  247/269, Loss:  0.513\n",
            "Epoch   6 Batch  248/269, Loss:  0.522\n",
            "Epoch   6 Batch  249/269, Loss:  0.470\n",
            "Epoch   6 Batch  250/269, Loss:  0.478\n",
            "Epoch   6 Batch  251/269, Loss:  0.513\n",
            "Epoch   6 Batch  252/269, Loss:  0.491\n",
            "Epoch   6 Batch  253/269, Loss:  0.513\n",
            "Epoch   6 Batch  254/269, Loss:  0.516\n",
            "Epoch   6 Batch  255/269, Loss:  0.453\n",
            "Epoch   6 Batch  256/269, Loss:  0.502\n",
            "Epoch   6 Batch  257/269, Loss:  0.552\n",
            "Epoch   6 Batch  258/269, Loss:  0.515\n",
            "Epoch   6 Batch  259/269, Loss:  0.528\n",
            "Epoch   6 Batch  260/269, Loss:  0.436\n",
            "Epoch   6 Batch  261/269, Loss:  0.473\n",
            "Epoch   6 Batch  262/269, Loss:  0.552\n",
            "Epoch   6 Batch  263/269, Loss:  0.539\n",
            "Epoch   6 Batch  264/269, Loss:  0.421\n",
            "Epoch   6 Batch  265/269, Loss:  0.493\n",
            "Epoch   6 Batch  266/269, Loss:  0.499\n",
            "Epoch   6 Batch  267/269, Loss:  0.487\n",
            "Epoch   7 Batch    0/269, Loss:  0.536\n",
            "Epoch   7 Batch    1/269, Loss:  0.487\n",
            "Epoch   7 Batch    2/269, Loss:  0.547\n",
            "Epoch   7 Batch    3/269, Loss:  0.532\n",
            "Epoch   7 Batch    4/269, Loss:  0.488\n",
            "Epoch   7 Batch    5/269, Loss:  0.510\n",
            "Epoch   7 Batch    6/269, Loss:  0.509\n",
            "Epoch   7 Batch    7/269, Loss:  0.499\n",
            "Epoch   7 Batch    8/269, Loss:  0.471\n",
            "Epoch   7 Batch    9/269, Loss:  0.485\n",
            "Epoch   7 Batch   10/269, Loss:  0.462\n",
            "Epoch   7 Batch   11/269, Loss:  0.521\n",
            "Epoch   7 Batch   12/269, Loss:  0.453\n",
            "Epoch   7 Batch   13/269, Loss:  0.485\n",
            "Epoch   7 Batch   14/269, Loss:  0.484\n",
            "Epoch   7 Batch   15/269, Loss:  0.504\n",
            "Epoch   7 Batch   16/269, Loss:  0.470\n",
            "Epoch   7 Batch   17/269, Loss:  0.489\n",
            "Epoch   7 Batch   18/269, Loss:  0.451\n",
            "Epoch   7 Batch   19/269, Loss:  0.515\n",
            "Epoch   7 Batch   20/269, Loss:  0.482\n",
            "Epoch   7 Batch   21/269, Loss:  0.507\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch   7 Batch   22/269, Loss:  0.493\n",
            "Epoch   7 Batch   23/269, Loss:  0.536\n",
            "Epoch   7 Batch   24/269, Loss:  0.515\n",
            "Epoch   7 Batch   25/269, Loss:  0.528\n",
            "Epoch   7 Batch   26/269, Loss:  0.493\n",
            "Epoch   7 Batch   27/269, Loss:  0.516\n",
            "Epoch   7 Batch   28/269, Loss:  0.533\n",
            "Epoch   7 Batch   29/269, Loss:  0.450\n",
            "Epoch   7 Batch   30/269, Loss:  0.499\n",
            "Epoch   7 Batch   31/269, Loss:  0.478\n",
            "Epoch   7 Batch   32/269, Loss:  0.518\n",
            "Epoch   7 Batch   33/269, Loss:  0.513\n",
            "Epoch   7 Batch   34/269, Loss:  0.440\n",
            "Epoch   7 Batch   35/269, Loss:  0.565\n",
            "Epoch   7 Batch   36/269, Loss:  0.519\n",
            "Epoch   7 Batch   37/269, Loss:  0.462\n",
            "Epoch   7 Batch   38/269, Loss:  0.473\n",
            "Epoch   7 Batch   39/269, Loss:  0.491\n",
            "Epoch   7 Batch   40/269, Loss:  0.475\n",
            "Epoch   7 Batch   41/269, Loss:  0.491\n",
            "Epoch   7 Batch   42/269, Loss:  0.497\n",
            "Epoch   7 Batch   43/269, Loss:  0.484\n",
            "Epoch   7 Batch   44/269, Loss:  0.473\n",
            "Epoch   7 Batch   45/269, Loss:  0.433\n",
            "Epoch   7 Batch   46/269, Loss:  0.448\n",
            "Epoch   7 Batch   47/269, Loss:  0.510\n",
            "Epoch   7 Batch   48/269, Loss:  0.498\n",
            "Epoch   7 Batch   49/269, Loss:  0.467\n",
            "Epoch   7 Batch   50/269, Loss:  0.429\n",
            "Epoch   7 Batch   51/269, Loss:  0.489\n",
            "Epoch   7 Batch   52/269, Loss:  0.547\n",
            "Epoch   7 Batch   53/269, Loss:  0.510\n",
            "Epoch   7 Batch   54/269, Loss:  0.443\n",
            "Epoch   7 Batch   55/269, Loss:  0.537\n",
            "Epoch   7 Batch   56/269, Loss:  0.414\n",
            "Epoch   7 Batch   57/269, Loss:  0.474\n",
            "Epoch   7 Batch   58/269, Loss:  0.527\n",
            "Epoch   7 Batch   59/269, Loss:  0.506\n",
            "Epoch   7 Batch   60/269, Loss:  0.486\n",
            "Epoch   7 Batch   61/269, Loss:  0.529\n",
            "Epoch   7 Batch   62/269, Loss:  0.413\n",
            "Epoch   7 Batch   63/269, Loss:  0.460\n",
            "Epoch   7 Batch   64/269, Loss:  0.521\n",
            "Epoch   7 Batch   65/269, Loss:  0.520\n",
            "Epoch   7 Batch   66/269, Loss:  0.520\n",
            "Epoch   7 Batch   67/269, Loss:  0.523\n",
            "Epoch   7 Batch   68/269, Loss:  0.592\n",
            "Epoch   7 Batch   69/269, Loss:  0.493\n",
            "Epoch   7 Batch   70/269, Loss:  0.566\n",
            "Epoch   7 Batch   71/269, Loss:  0.491\n",
            "Epoch   7 Batch   72/269, Loss:  0.534\n",
            "Epoch   7 Batch   73/269, Loss:  0.514\n",
            "Epoch   7 Batch   74/269, Loss:  0.491\n",
            "Epoch   7 Batch   75/269, Loss:  0.618\n",
            "Epoch   7 Batch   76/269, Loss:  0.523\n",
            "Epoch   7 Batch   77/269, Loss:  0.450\n",
            "Epoch   7 Batch   78/269, Loss:  0.514\n",
            "Epoch   7 Batch   79/269, Loss:  0.554\n",
            "Epoch   7 Batch   80/269, Loss:  0.452\n",
            "Epoch   7 Batch   81/269, Loss:  0.468\n",
            "Epoch   7 Batch   82/269, Loss:  0.408\n",
            "Epoch   7 Batch   83/269, Loss:  0.559\n",
            "Epoch   7 Batch   84/269, Loss:  0.514\n",
            "Epoch   7 Batch   85/269, Loss:  0.443\n",
            "Epoch   7 Batch   86/269, Loss:  0.512\n",
            "Epoch   7 Batch   87/269, Loss:  0.442\n",
            "Epoch   7 Batch   88/269, Loss:  0.445\n",
            "Epoch   7 Batch   89/269, Loss:  0.524\n",
            "Epoch   7 Batch   90/269, Loss:  0.515\n",
            "Epoch   7 Batch   91/269, Loss:  0.505\n",
            "Epoch   7 Batch   92/269, Loss:  0.518\n",
            "Epoch   7 Batch   93/269, Loss:  0.567\n",
            "Epoch   7 Batch   94/269, Loss:  0.463\n",
            "Epoch   7 Batch   95/269, Loss:  0.465\n",
            "Epoch   7 Batch   96/269, Loss:  0.504\n",
            "Epoch   7 Batch   97/269, Loss:  0.506\n",
            "Epoch   7 Batch   98/269, Loss:  0.447\n",
            "Epoch   7 Batch   99/269, Loss:  0.471\n",
            "Epoch   7 Batch  100/269, Loss:  0.475\n",
            "Epoch   7 Batch  101/269, Loss:  0.496\n",
            "Epoch   7 Batch  102/269, Loss:  0.522\n",
            "Epoch   7 Batch  103/269, Loss:  0.518\n",
            "Epoch   7 Batch  104/269, Loss:  0.493\n",
            "Epoch   7 Batch  105/269, Loss:  0.452\n",
            "Epoch   7 Batch  106/269, Loss:  0.452\n",
            "Epoch   7 Batch  107/269, Loss:  0.462\n",
            "Epoch   7 Batch  108/269, Loss:  0.445\n",
            "Epoch   7 Batch  109/269, Loss:  0.510\n",
            "Epoch   7 Batch  110/269, Loss:  0.475\n",
            "Epoch   7 Batch  111/269, Loss:  0.506\n",
            "Epoch   7 Batch  112/269, Loss:  0.505\n",
            "Epoch   7 Batch  113/269, Loss:  0.511\n",
            "Epoch   7 Batch  114/269, Loss:  0.466\n",
            "Epoch   7 Batch  115/269, Loss:  0.526\n",
            "Epoch   7 Batch  116/269, Loss:  0.463\n",
            "Epoch   7 Batch  117/269, Loss:  0.498\n",
            "Epoch   7 Batch  118/269, Loss:  0.441\n",
            "Epoch   7 Batch  119/269, Loss:  0.515\n",
            "Epoch   7 Batch  120/269, Loss:  0.570\n",
            "Epoch   7 Batch  121/269, Loss:  0.468\n",
            "Epoch   7 Batch  122/269, Loss:  0.505\n",
            "Epoch   7 Batch  123/269, Loss:  0.553\n",
            "Epoch   7 Batch  124/269, Loss:  0.479\n",
            "Epoch   7 Batch  125/269, Loss:  0.566\n",
            "Epoch   7 Batch  126/269, Loss:  0.426\n",
            "Epoch   7 Batch  127/269, Loss:  0.516\n",
            "Epoch   7 Batch  128/269, Loss:  0.464\n",
            "Epoch   7 Batch  129/269, Loss:  0.459\n",
            "Epoch   7 Batch  130/269, Loss:  0.563\n",
            "Epoch   7 Batch  131/269, Loss:  0.584\n",
            "Epoch   7 Batch  132/269, Loss:  0.494\n",
            "Epoch   7 Batch  133/269, Loss:  0.520\n",
            "Epoch   7 Batch  134/269, Loss:  0.501\n",
            "Epoch   7 Batch  135/269, Loss:  0.566\n",
            "Epoch   7 Batch  136/269, Loss:  0.485\n",
            "Epoch   7 Batch  137/269, Loss:  0.492\n",
            "Epoch   7 Batch  138/269, Loss:  0.511\n",
            "Epoch   7 Batch  139/269, Loss:  0.459\n",
            "Epoch   7 Batch  140/269, Loss:  0.472\n",
            "Epoch   7 Batch  141/269, Loss:  0.554\n",
            "Epoch   7 Batch  142/269, Loss:  0.497\n",
            "Epoch   7 Batch  143/269, Loss:  0.467\n",
            "Epoch   7 Batch  144/269, Loss:  0.487\n",
            "Epoch   7 Batch  145/269, Loss:  0.498\n",
            "Epoch   7 Batch  146/269, Loss:  0.476\n",
            "Epoch   7 Batch  147/269, Loss:  0.527\n",
            "Epoch   7 Batch  148/269, Loss:  0.442\n",
            "Epoch   7 Batch  149/269, Loss:  0.458\n",
            "Epoch   7 Batch  150/269, Loss:  0.506\n",
            "Epoch   7 Batch  151/269, Loss:  0.426\n",
            "Epoch   7 Batch  152/269, Loss:  0.475\n",
            "Epoch   7 Batch  153/269, Loss:  0.470\n",
            "Epoch   7 Batch  154/269, Loss:  0.471\n",
            "Epoch   7 Batch  155/269, Loss:  0.497\n",
            "Epoch   7 Batch  156/269, Loss:  0.469\n",
            "Epoch   7 Batch  157/269, Loss:  0.490\n",
            "Epoch   7 Batch  158/269, Loss:  0.471\n",
            "Epoch   7 Batch  159/269, Loss:  0.434\n",
            "Epoch   7 Batch  160/269, Loss:  0.476\n",
            "Epoch   7 Batch  161/269, Loss:  0.465\n",
            "Epoch   7 Batch  162/269, Loss:  0.492\n",
            "Epoch   7 Batch  163/269, Loss:  0.451\n",
            "Epoch   7 Batch  164/269, Loss:  0.526\n",
            "Epoch   7 Batch  165/269, Loss:  0.557\n",
            "Epoch   7 Batch  166/269, Loss:  0.536\n",
            "Epoch   7 Batch  167/269, Loss:  0.446\n",
            "Epoch   7 Batch  168/269, Loss:  0.500\n",
            "Epoch   7 Batch  169/269, Loss:  0.525\n",
            "Epoch   7 Batch  170/269, Loss:  0.484\n",
            "Epoch   7 Batch  171/269, Loss:  0.536\n",
            "Epoch   7 Batch  172/269, Loss:  0.518\n",
            "Epoch   7 Batch  173/269, Loss:  0.481\n",
            "Epoch   7 Batch  174/269, Loss:  0.467\n",
            "Epoch   7 Batch  175/269, Loss:  0.520\n",
            "Epoch   7 Batch  176/269, Loss:  0.532\n",
            "Epoch   7 Batch  177/269, Loss:  0.507\n",
            "Epoch   7 Batch  178/269, Loss:  0.549\n",
            "Epoch   7 Batch  179/269, Loss:  0.466\n",
            "Epoch   7 Batch  180/269, Loss:  0.428\n",
            "Epoch   7 Batch  181/269, Loss:  0.456\n",
            "Epoch   7 Batch  182/269, Loss:  0.470\n",
            "Epoch   7 Batch  183/269, Loss:  0.487\n",
            "Epoch   7 Batch  184/269, Loss:  0.527\n",
            "Epoch   7 Batch  185/269, Loss:  0.511\n",
            "Epoch   7 Batch  186/269, Loss:  0.521\n",
            "Epoch   7 Batch  187/269, Loss:  0.498\n",
            "Epoch   7 Batch  188/269, Loss:  0.458\n",
            "Epoch   7 Batch  189/269, Loss:  0.508\n",
            "Epoch   7 Batch  190/269, Loss:  0.521\n",
            "Epoch   7 Batch  191/269, Loss:  0.510\n",
            "Epoch   7 Batch  192/269, Loss:  0.490\n",
            "Epoch   7 Batch  193/269, Loss:  0.533\n",
            "Epoch   7 Batch  194/269, Loss:  0.494\n",
            "Epoch   7 Batch  195/269, Loss:  0.515\n",
            "Epoch   7 Batch  196/269, Loss:  0.511\n",
            "Epoch   7 Batch  197/269, Loss:  0.829\n",
            "Epoch   7 Batch  198/269, Loss:  3.986\n",
            "Epoch   7 Batch  199/269, Loss:  2.443\n",
            "Epoch   7 Batch  200/269, Loss:  0.789\n",
            "Epoch   7 Batch  201/269, Loss:  2.131\n",
            "Epoch   7 Batch  202/269, Loss:  0.953\n",
            "Epoch   7 Batch  203/269, Loss:  0.987\n",
            "Epoch   7 Batch  204/269, Loss:  1.238\n",
            "Epoch   7 Batch  205/269, Loss:  1.115\n",
            "Epoch   7 Batch  206/269, Loss:  1.487\n",
            "Epoch   7 Batch  207/269, Loss:  1.109\n",
            "Epoch   7 Batch  208/269, Loss:  1.318\n",
            "Epoch   7 Batch  209/269, Loss:  0.707\n",
            "Epoch   7 Batch  210/269, Loss:  1.814\n",
            "Epoch   7 Batch  211/269, Loss:  1.661\n",
            "Epoch   7 Batch  212/269, Loss:  1.886\n",
            "Epoch   7 Batch  213/269, Loss:  1.102\n",
            "Epoch   7 Batch  214/269, Loss:  1.630\n",
            "Epoch   7 Batch  215/269, Loss:  1.678\n",
            "Epoch   7 Batch  216/269, Loss:  1.615\n",
            "Epoch   7 Batch  217/269, Loss:  1.731\n",
            "Epoch   7 Batch  218/269, Loss:  1.647\n",
            "Epoch   7 Batch  219/269, Loss:  1.434\n",
            "Epoch   7 Batch  220/269, Loss:  1.771\n",
            "Epoch   7 Batch  221/269, Loss:  1.771\n",
            "Epoch   7 Batch  222/269, Loss:  1.908\n",
            "Epoch   7 Batch  223/269, Loss:  2.257\n",
            "Epoch   7 Batch  224/269, Loss:  2.897\n",
            "Epoch   7 Batch  225/269, Loss:  1.993\n",
            "Epoch   7 Batch  226/269, Loss:  2.541\n",
            "Epoch   7 Batch  227/269, Loss:  2.390\n",
            "Epoch   7 Batch  228/269, Loss:  2.079\n",
            "Epoch   7 Batch  229/269, Loss:  2.195\n",
            "Epoch   7 Batch  230/269, Loss:  2.523\n",
            "Epoch   7 Batch  231/269, Loss:  2.209\n",
            "Epoch   7 Batch  232/269, Loss:  2.117\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch   7 Batch  233/269, Loss:  2.963\n",
            "Epoch   7 Batch  234/269, Loss:  3.696\n",
            "Epoch   7 Batch  235/269, Loss:  2.874\n",
            "Epoch   7 Batch  236/269, Loss:  2.852\n",
            "Epoch   7 Batch  237/269, Loss:  3.171\n",
            "Epoch   7 Batch  238/269, Loss:  4.098\n",
            "Epoch   7 Batch  239/269, Loss:  2.845\n",
            "Epoch   7 Batch  240/269, Loss:  3.438\n",
            "Epoch   7 Batch  241/269, Loss:  4.609\n",
            "Epoch   7 Batch  242/269, Loss:  3.448\n",
            "Epoch   7 Batch  243/269, Loss:  6.055\n",
            "Epoch   7 Batch  244/269, Loss:  4.512\n",
            "Epoch   7 Batch  245/269, Loss:  4.812\n",
            "Epoch   7 Batch  246/269, Loss:  5.455\n",
            "Epoch   7 Batch  247/269, Loss:  7.026\n",
            "Epoch   7 Batch  248/269, Loss: 10.203\n",
            "Epoch   7 Batch  249/269, Loss: 16.374\n",
            "Epoch   7 Batch  250/269, Loss: 20.362\n",
            "Epoch   7 Batch  251/269, Loss: 28.700\n",
            "Epoch   7 Batch  252/269, Loss: 50.603\n",
            "Epoch   7 Batch  253/269, Loss: 104.095\n",
            "Epoch   7 Batch  254/269, Loss: 146.727\n",
            "Epoch   7 Batch  255/269, Loss: 160.308\n",
            "Epoch   7 Batch  256/269, Loss: 171.847\n",
            "Epoch   7 Batch  257/269, Loss: 180.655\n",
            "Epoch   7 Batch  258/269, Loss: 192.184\n",
            "Epoch   7 Batch  259/269, Loss: 242.390\n",
            "Epoch   7 Batch  260/269, Loss: 306.299\n",
            "Epoch   7 Batch  261/269, Loss: 462.149\n",
            "Epoch   7 Batch  262/269, Loss: 604.785\n",
            "Epoch   7 Batch  263/269, Loss: 633.879\n",
            "Epoch   7 Batch  264/269, Loss: 562.297\n",
            "Epoch   7 Batch  265/269, Loss: 494.762\n",
            "Epoch   7 Batch  266/269, Loss: 475.041\n",
            "Epoch   7 Batch  267/269, Loss: 485.380\n",
            "Epoch   8 Batch    0/269, Loss: 507.517\n",
            "Epoch   8 Batch    1/269, Loss: 514.398\n",
            "Epoch   8 Batch    2/269, Loss: 516.438\n",
            "Epoch   8 Batch    3/269, Loss: 470.904\n",
            "Epoch   8 Batch    4/269, Loss: 495.351\n",
            "Epoch   8 Batch    5/269, Loss: 542.421\n",
            "Epoch   8 Batch    6/269, Loss: 584.061\n",
            "Epoch   8 Batch    7/269, Loss: 593.918\n",
            "Epoch   8 Batch    8/269, Loss: 644.996\n",
            "Epoch   8 Batch    9/269, Loss: 805.038\n",
            "Epoch   8 Batch   10/269, Loss: 877.401\n",
            "Epoch   8 Batch   11/269, Loss: 893.896\n",
            "Epoch   8 Batch   12/269, Loss: 925.408\n",
            "Epoch   8 Batch   13/269, Loss: 1074.791\n",
            "Epoch   8 Batch   14/269, Loss: 1058.462\n",
            "Epoch   8 Batch   15/269, Loss: 1057.151\n",
            "Epoch   8 Batch   16/269, Loss: 1078.504\n",
            "Epoch   8 Batch   17/269, Loss: 1166.966\n",
            "Epoch   8 Batch   18/269, Loss: 1098.272\n",
            "Epoch   8 Batch   19/269, Loss: 1167.693\n",
            "Epoch   8 Batch   20/269, Loss: 1156.681\n",
            "Epoch   8 Batch   21/269, Loss: 1120.528\n",
            "Epoch   8 Batch   22/269, Loss: 1092.141\n",
            "Epoch   8 Batch   23/269, Loss: 1065.780\n",
            "Epoch   8 Batch   24/269, Loss: 1254.205\n",
            "Epoch   8 Batch   25/269, Loss: 1406.484\n",
            "Epoch   8 Batch   26/269, Loss: 1436.506\n",
            "Epoch   8 Batch   27/269, Loss: 1467.241\n",
            "Epoch   8 Batch   28/269, Loss: 1741.372\n",
            "Epoch   8 Batch   29/269, Loss: 1761.881\n",
            "Epoch   8 Batch   30/269, Loss: 1992.708\n",
            "Epoch   8 Batch   31/269, Loss: 1991.388\n",
            "Epoch   8 Batch   32/269, Loss: 2168.032\n",
            "Epoch   8 Batch   33/269, Loss: 2020.996\n",
            "Epoch   8 Batch   34/269, Loss: 1746.433\n",
            "Epoch   8 Batch   35/269, Loss: 1717.286\n",
            "Epoch   8 Batch   36/269, Loss: 1610.371\n",
            "Epoch   8 Batch   37/269, Loss: 1652.668\n",
            "Epoch   8 Batch   38/269, Loss: 1604.875\n",
            "Epoch   8 Batch   39/269, Loss: 1636.031\n",
            "Epoch   8 Batch   40/269, Loss: 1601.488\n",
            "Epoch   8 Batch   41/269, Loss: 1654.582\n",
            "Epoch   8 Batch   42/269, Loss: 2007.020\n",
            "Epoch   8 Batch   43/269, Loss: 2193.833\n",
            "Epoch   8 Batch   44/269, Loss: 2509.655\n",
            "Epoch   8 Batch   45/269, Loss: 2650.139\n",
            "Epoch   8 Batch   46/269, Loss: 2902.424\n",
            "Epoch   8 Batch   47/269, Loss: 3372.468\n",
            "Epoch   8 Batch   48/269, Loss: 3502.797\n",
            "Epoch   8 Batch   49/269, Loss: 3626.807\n",
            "Epoch   8 Batch   50/269, Loss: 3862.935\n",
            "Epoch   8 Batch   51/269, Loss: 3837.893\n",
            "Epoch   8 Batch   52/269, Loss: 3793.382\n",
            "Epoch   8 Batch   53/269, Loss: 3556.829\n",
            "Epoch   8 Batch   54/269, Loss: 3366.123\n",
            "Epoch   8 Batch   55/269, Loss: 3007.535\n",
            "Epoch   8 Batch   56/269, Loss: 3058.377\n",
            "Epoch   8 Batch   57/269, Loss: 2267.838\n",
            "Epoch   8 Batch   58/269, Loss: 1964.972\n",
            "Epoch   8 Batch   59/269, Loss: 1621.192\n",
            "Epoch   8 Batch   60/269, Loss: 1619.934\n",
            "Epoch   8 Batch   61/269, Loss: 1615.974\n",
            "Epoch   8 Batch   62/269, Loss: 1558.023\n",
            "Epoch   8 Batch   63/269, Loss: 1478.775\n",
            "Epoch   8 Batch   64/269, Loss: 1241.131\n",
            "Epoch   8 Batch   65/269, Loss: 1316.928\n",
            "Epoch   8 Batch   66/269, Loss: 1563.250\n",
            "Epoch   8 Batch   67/269, Loss: 1760.561\n",
            "Epoch   8 Batch   68/269, Loss: 1984.057\n",
            "Epoch   8 Batch   69/269, Loss: 2250.405\n",
            "Epoch   8 Batch   70/269, Loss: 2462.001\n",
            "Epoch   8 Batch   71/269, Loss: 2670.201\n",
            "Epoch   8 Batch   72/269, Loss: 2918.083\n",
            "Epoch   8 Batch   73/269, Loss: 3015.916\n",
            "Epoch   8 Batch   74/269, Loss: 3153.328\n",
            "Epoch   8 Batch   75/269, Loss: 3264.116\n",
            "Epoch   8 Batch   76/269, Loss: 3306.642\n",
            "Epoch   8 Batch   77/269, Loss: 3335.657\n",
            "Epoch   8 Batch   78/269, Loss: 3317.486\n",
            "Epoch   8 Batch   79/269, Loss: 3386.448\n",
            "Epoch   8 Batch   80/269, Loss: 3425.738\n",
            "Epoch   8 Batch   81/269, Loss: 3380.172\n",
            "Epoch   8 Batch   82/269, Loss: 3438.164\n",
            "Epoch   8 Batch   83/269, Loss: 3532.956\n",
            "Epoch   8 Batch   84/269, Loss: 3431.177\n",
            "Epoch   8 Batch   85/269, Loss: 3414.245\n",
            "Epoch   8 Batch   86/269, Loss: 3330.564\n",
            "Epoch   8 Batch   87/269, Loss: 3323.828\n",
            "Epoch   8 Batch   88/269, Loss: 3315.057\n",
            "Epoch   8 Batch   89/269, Loss: 3319.455\n",
            "Epoch   8 Batch   90/269, Loss: 3353.124\n",
            "Epoch   8 Batch   91/269, Loss: 3402.318\n",
            "Epoch   8 Batch   92/269, Loss: 3568.958\n",
            "Epoch   8 Batch   93/269, Loss: 3641.340\n",
            "Epoch   8 Batch   94/269, Loss: 3764.795\n",
            "Epoch   8 Batch   95/269, Loss: 3680.461\n",
            "Epoch   8 Batch   96/269, Loss: 3269.100\n",
            "Epoch   8 Batch   97/269, Loss: 2433.883\n",
            "Epoch   8 Batch   98/269, Loss: 1735.598\n",
            "Epoch   8 Batch   99/269, Loss: 1790.719\n",
            "Epoch   8 Batch  100/269, Loss: 1092.794\n",
            "Epoch   8 Batch  101/269, Loss: 1158.781\n",
            "Epoch   8 Batch  102/269, Loss: 1301.916\n",
            "Epoch   8 Batch  103/269, Loss: 1387.702\n",
            "Epoch   8 Batch  104/269, Loss: 1329.304\n",
            "Epoch   8 Batch  105/269, Loss: 1472.859\n",
            "Epoch   8 Batch  106/269, Loss: 1671.734\n",
            "Epoch   8 Batch  107/269, Loss: 1819.495\n",
            "Epoch   8 Batch  108/269, Loss: 1866.461\n",
            "Epoch   8 Batch  109/269, Loss: 1772.847\n",
            "Epoch   8 Batch  110/269, Loss: 1666.259\n",
            "Epoch   8 Batch  111/269, Loss: 1450.220\n",
            "Epoch   8 Batch  112/269, Loss: 1069.454\n",
            "Epoch   8 Batch  113/269, Loss: 904.551\n",
            "Epoch   8 Batch  114/269, Loss: 874.970\n",
            "Epoch   8 Batch  115/269, Loss: 1032.651\n",
            "Epoch   8 Batch  116/269, Loss: 928.704\n",
            "Epoch   8 Batch  117/269, Loss: 926.205\n",
            "Epoch   8 Batch  118/269, Loss: 1043.325\n",
            "Epoch   8 Batch  119/269, Loss: 1069.034\n",
            "Epoch   8 Batch  120/269, Loss: 1083.004\n",
            "Epoch   8 Batch  121/269, Loss: 1108.756\n",
            "Epoch   8 Batch  122/269, Loss: 1136.812\n",
            "Epoch   8 Batch  123/269, Loss: 1166.514\n",
            "Epoch   8 Batch  124/269, Loss: 1178.027\n",
            "Epoch   8 Batch  125/269, Loss: 1186.284\n",
            "Epoch   8 Batch  126/269, Loss: 1217.961\n",
            "Epoch   8 Batch  127/269, Loss: 1252.086\n",
            "Epoch   8 Batch  128/269, Loss: 1216.162\n",
            "Epoch   8 Batch  129/269, Loss: 1279.990\n",
            "Epoch   8 Batch  130/269, Loss: 1294.464\n",
            "Epoch   8 Batch  131/269, Loss: 1306.540\n",
            "Epoch   8 Batch  132/269, Loss: 1307.908\n",
            "Epoch   8 Batch  133/269, Loss: 1365.491\n",
            "Epoch   8 Batch  134/269, Loss: 1413.799\n",
            "Epoch   8 Batch  135/269, Loss: 1438.797\n",
            "Epoch   8 Batch  136/269, Loss: 1492.742\n",
            "Epoch   8 Batch  137/269, Loss: 1540.904\n",
            "Epoch   8 Batch  138/269, Loss: 1556.133\n",
            "Epoch   8 Batch  139/269, Loss: 1667.540\n",
            "Epoch   8 Batch  140/269, Loss: 1713.174\n",
            "Epoch   8 Batch  141/269, Loss: 1755.804\n",
            "Epoch   8 Batch  142/269, Loss: 1905.630\n",
            "Epoch   8 Batch  143/269, Loss: 1876.491\n",
            "Epoch   8 Batch  144/269, Loss: 1877.816\n",
            "Epoch   8 Batch  145/269, Loss: 1544.664\n",
            "Epoch   8 Batch  146/269, Loss: 1350.531\n",
            "Epoch   8 Batch  147/269, Loss: 1202.578\n",
            "Epoch   8 Batch  148/269, Loss: 1018.867\n",
            "Epoch   8 Batch  149/269, Loss: 910.104\n",
            "Epoch   8 Batch  150/269, Loss: 823.846\n",
            "Epoch   8 Batch  151/269, Loss: 868.385\n",
            "Epoch   8 Batch  152/269, Loss: 698.563\n",
            "Epoch   8 Batch  153/269, Loss: 696.803\n",
            "Epoch   8 Batch  154/269, Loss: 713.033\n",
            "Epoch   8 Batch  155/269, Loss: 784.960\n",
            "Epoch   8 Batch  156/269, Loss: 782.229\n",
            "Epoch   8 Batch  157/269, Loss: 791.247\n",
            "Epoch   8 Batch  158/269, Loss: 808.374\n",
            "Epoch   8 Batch  159/269, Loss: 844.668\n",
            "Epoch   8 Batch  160/269, Loss: 739.804\n",
            "Epoch   8 Batch  161/269, Loss: 722.488\n",
            "Epoch   8 Batch  162/269, Loss: 682.038\n",
            "Epoch   8 Batch  163/269, Loss: 640.847\n",
            "Epoch   8 Batch  164/269, Loss: 602.439\n",
            "Epoch   8 Batch  165/269, Loss: 602.198\n",
            "Epoch   8 Batch  166/269, Loss: 543.031\n",
            "Epoch   8 Batch  167/269, Loss: 496.159\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch   8 Batch  168/269, Loss: 522.161\n",
            "Epoch   8 Batch  169/269, Loss: 534.428\n",
            "Epoch   8 Batch  170/269, Loss: 488.100\n",
            "Epoch   8 Batch  171/269, Loss: 447.697\n",
            "Epoch   8 Batch  172/269, Loss: 427.743\n",
            "Epoch   8 Batch  173/269, Loss: 458.195\n",
            "Epoch   8 Batch  174/269, Loss: 467.561\n",
            "Epoch   8 Batch  175/269, Loss: 455.704\n",
            "Epoch   8 Batch  176/269, Loss: 503.707\n",
            "Epoch   8 Batch  177/269, Loss: 484.864\n",
            "Epoch   8 Batch  178/269, Loss: 442.825\n",
            "Epoch   8 Batch  179/269, Loss: 376.659\n",
            "Epoch   8 Batch  180/269, Loss: 355.878\n",
            "Epoch   8 Batch  181/269, Loss: 426.715\n",
            "Epoch   8 Batch  182/269, Loss: 374.878\n",
            "Epoch   8 Batch  183/269, Loss: 379.178\n",
            "Epoch   8 Batch  184/269, Loss: 378.013\n",
            "Epoch   8 Batch  185/269, Loss: 404.723\n",
            "Epoch   8 Batch  186/269, Loss: 353.802\n",
            "Epoch   8 Batch  187/269, Loss: 365.009\n",
            "Epoch   8 Batch  188/269, Loss: 348.650\n",
            "Epoch   8 Batch  189/269, Loss: 332.997\n",
            "Epoch   8 Batch  190/269, Loss: 332.925\n",
            "Epoch   8 Batch  191/269, Loss: 374.244\n",
            "Epoch   8 Batch  192/269, Loss: 428.331\n",
            "Epoch   8 Batch  193/269, Loss: 465.942\n",
            "Epoch   8 Batch  194/269, Loss: 493.556\n",
            "Epoch   8 Batch  195/269, Loss: 534.526\n",
            "Epoch   8 Batch  196/269, Loss: 553.576\n",
            "Epoch   8 Batch  197/269, Loss: 596.727\n",
            "Epoch   8 Batch  198/269, Loss: 644.833\n",
            "Epoch   8 Batch  199/269, Loss: 674.104\n",
            "Epoch   8 Batch  200/269, Loss: 713.479\n",
            "Epoch   8 Batch  201/269, Loss: 741.250\n",
            "Epoch   8 Batch  202/269, Loss: 783.207\n",
            "Epoch   8 Batch  203/269, Loss: 800.754\n",
            "Epoch   8 Batch  204/269, Loss: 814.511\n",
            "Epoch   8 Batch  205/269, Loss: 852.133\n",
            "Epoch   8 Batch  206/269, Loss: 850.641\n",
            "Epoch   8 Batch  207/269, Loss: 867.986\n",
            "Epoch   8 Batch  208/269, Loss: 883.141\n",
            "Epoch   8 Batch  209/269, Loss: 887.867\n",
            "Epoch   8 Batch  210/269, Loss: 897.629\n",
            "Epoch   8 Batch  211/269, Loss: 907.889\n",
            "Epoch   8 Batch  212/269, Loss: 895.610\n",
            "Epoch   8 Batch  213/269, Loss: 916.255\n",
            "Epoch   8 Batch  214/269, Loss: 916.845\n",
            "Epoch   8 Batch  215/269, Loss: 958.853\n",
            "Epoch   8 Batch  216/269, Loss: 938.079\n",
            "Epoch   8 Batch  217/269, Loss: 964.644\n",
            "Epoch   8 Batch  218/269, Loss: 983.669\n",
            "Epoch   8 Batch  219/269, Loss: 996.996\n",
            "Epoch   8 Batch  220/269, Loss: 1018.228\n",
            "Epoch   8 Batch  221/269, Loss: 1012.690\n",
            "Epoch   8 Batch  222/269, Loss: 1072.260\n",
            "Epoch   8 Batch  223/269, Loss: 1109.449\n",
            "Epoch   8 Batch  224/269, Loss: 1154.119\n",
            "Epoch   8 Batch  225/269, Loss: 1198.814\n",
            "Epoch   8 Batch  226/269, Loss: 1261.188\n",
            "Epoch   8 Batch  227/269, Loss: 1266.039\n",
            "Epoch   8 Batch  228/269, Loss: 1272.277\n",
            "Epoch   8 Batch  229/269, Loss: 1291.005\n",
            "Epoch   8 Batch  230/269, Loss: 1362.663\n",
            "Epoch   8 Batch  231/269, Loss: 1387.526\n",
            "Epoch   8 Batch  232/269, Loss: 1451.568\n",
            "Epoch   8 Batch  233/269, Loss: 1425.802\n",
            "Epoch   8 Batch  234/269, Loss: 1370.165\n",
            "Epoch   8 Batch  235/269, Loss: 1377.702\n",
            "Epoch   8 Batch  236/269, Loss: 1363.809\n",
            "Epoch   8 Batch  237/269, Loss: 1388.407\n",
            "Epoch   8 Batch  238/269, Loss: 1388.041\n",
            "Epoch   8 Batch  239/269, Loss: 1381.728\n",
            "Epoch   8 Batch  240/269, Loss: 1492.422\n",
            "Epoch   8 Batch  241/269, Loss: 1519.450\n",
            "Epoch   8 Batch  242/269, Loss: 1528.468\n",
            "Epoch   8 Batch  243/269, Loss: 1533.871\n",
            "Epoch   8 Batch  244/269, Loss: 1533.198\n",
            "Epoch   8 Batch  245/269, Loss: 1497.639\n",
            "Epoch   8 Batch  246/269, Loss: 1425.226\n",
            "Epoch   8 Batch  247/269, Loss: 1356.069\n",
            "Epoch   8 Batch  248/269, Loss: 1262.470\n",
            "Epoch   8 Batch  249/269, Loss: 1169.138\n",
            "Epoch   8 Batch  250/269, Loss: 1059.604\n",
            "Epoch   8 Batch  251/269, Loss: 1009.803\n",
            "Epoch   8 Batch  252/269, Loss: 970.448\n",
            "Epoch   8 Batch  253/269, Loss: 953.668\n",
            "Epoch   8 Batch  254/269, Loss: 956.105\n",
            "Epoch   8 Batch  255/269, Loss: 988.770\n",
            "Epoch   8 Batch  256/269, Loss: 1004.343\n",
            "Epoch   8 Batch  257/269, Loss: 1027.180\n",
            "Epoch   8 Batch  258/269, Loss: 1049.382\n",
            "Epoch   8 Batch  259/269, Loss: 1125.078\n",
            "Epoch   8 Batch  260/269, Loss: 1147.455\n",
            "Epoch   8 Batch  261/269, Loss: 1202.591\n",
            "Epoch   8 Batch  262/269, Loss: 1247.752\n",
            "Epoch   8 Batch  263/269, Loss: 1283.802\n",
            "Epoch   8 Batch  264/269, Loss: 1337.594\n",
            "Epoch   8 Batch  265/269, Loss: 1359.158\n",
            "Epoch   8 Batch  266/269, Loss: 1443.874\n",
            "Epoch   8 Batch  267/269, Loss: 1505.952\n",
            "Epoch   9 Batch    0/269, Loss: 1503.405\n",
            "Epoch   9 Batch    1/269, Loss: 1515.202\n",
            "Epoch   9 Batch    2/269, Loss: 1514.773\n",
            "Epoch   9 Batch    3/269, Loss: 1517.373\n",
            "Epoch   9 Batch    4/269, Loss: 1498.637\n",
            "Epoch   9 Batch    5/269, Loss: 1513.440\n",
            "Epoch   9 Batch    6/269, Loss: 1503.454\n",
            "Epoch   9 Batch    7/269, Loss: 1472.821\n",
            "Epoch   9 Batch    8/269, Loss: 1409.393\n",
            "Epoch   9 Batch    9/269, Loss: 1408.153\n",
            "Epoch   9 Batch   10/269, Loss: 1366.661\n",
            "Epoch   9 Batch   11/269, Loss: 1410.243\n",
            "Epoch   9 Batch   12/269, Loss: 1393.139\n",
            "Epoch   9 Batch   13/269, Loss: 1401.793\n",
            "Epoch   9 Batch   14/269, Loss: 1374.522\n",
            "Epoch   9 Batch   15/269, Loss: 1379.475\n",
            "Epoch   9 Batch   16/269, Loss: 1427.195\n",
            "Epoch   9 Batch   17/269, Loss: 1408.511\n",
            "Epoch   9 Batch   18/269, Loss: 1402.169\n",
            "Epoch   9 Batch   19/269, Loss: 1477.393\n",
            "Epoch   9 Batch   20/269, Loss: 1466.606\n",
            "Epoch   9 Batch   21/269, Loss: 1486.431\n",
            "Epoch   9 Batch   22/269, Loss: 1478.735\n",
            "Epoch   9 Batch   23/269, Loss: 1469.407\n",
            "Epoch   9 Batch   24/269, Loss: 1462.098\n",
            "Epoch   9 Batch   25/269, Loss: 1461.248\n",
            "Epoch   9 Batch   26/269, Loss: 1480.908\n",
            "Epoch   9 Batch   27/269, Loss: 1493.073\n",
            "Epoch   9 Batch   28/269, Loss: 1511.124\n",
            "Epoch   9 Batch   29/269, Loss: 1507.964\n",
            "Epoch   9 Batch   30/269, Loss: 1482.394\n",
            "Epoch   9 Batch   31/269, Loss: 1440.220\n",
            "Epoch   9 Batch   32/269, Loss: 1435.819\n",
            "Epoch   9 Batch   33/269, Loss: 1420.894\n",
            "Epoch   9 Batch   34/269, Loss: 1453.880\n",
            "Epoch   9 Batch   35/269, Loss: 1424.285\n",
            "Epoch   9 Batch   36/269, Loss: 1491.796\n",
            "Epoch   9 Batch   37/269, Loss: 1483.906\n",
            "Epoch   9 Batch   38/269, Loss: 1500.798\n",
            "Epoch   9 Batch   39/269, Loss: 1503.900\n",
            "Epoch   9 Batch   40/269, Loss: 1493.653\n",
            "Epoch   9 Batch   41/269, Loss: 1493.831\n",
            "Epoch   9 Batch   42/269, Loss: 1523.054\n",
            "Epoch   9 Batch   43/269, Loss: 1460.098\n",
            "Epoch   9 Batch   44/269, Loss: 1428.281\n",
            "Epoch   9 Batch   45/269, Loss: 1374.747\n",
            "Epoch   9 Batch   46/269, Loss: 1257.546\n",
            "Epoch   9 Batch   47/269, Loss: 1119.740\n",
            "Epoch   9 Batch   48/269, Loss: 922.404\n",
            "Epoch   9 Batch   49/269, Loss: 837.742\n",
            "Epoch   9 Batch   50/269, Loss: 975.799\n",
            "Epoch   9 Batch   51/269, Loss: 884.070\n",
            "Epoch   9 Batch   52/269, Loss: 828.453\n",
            "Epoch   9 Batch   53/269, Loss: 959.616\n",
            "Epoch   9 Batch   54/269, Loss: 1077.872\n",
            "Epoch   9 Batch   55/269, Loss: 1201.798\n",
            "Epoch   9 Batch   56/269, Loss: 1317.118\n",
            "Epoch   9 Batch   57/269, Loss: 1378.764\n",
            "Epoch   9 Batch   58/269, Loss: 1436.935\n",
            "Epoch   9 Batch   59/269, Loss: 1528.777\n",
            "Epoch   9 Batch   60/269, Loss: 1538.259\n",
            "Epoch   9 Batch   61/269, Loss: 1627.319\n",
            "Epoch   9 Batch   62/269, Loss: 1692.910\n",
            "Epoch   9 Batch   63/269, Loss: 1699.205\n",
            "Epoch   9 Batch   64/269, Loss: 1716.981\n",
            "Epoch   9 Batch   65/269, Loss: 1755.033\n",
            "Epoch   9 Batch   66/269, Loss: 1790.295\n",
            "Epoch   9 Batch   67/269, Loss: 1775.445\n",
            "Epoch   9 Batch   68/269, Loss: 1829.140\n",
            "Epoch   9 Batch   69/269, Loss: 1788.904\n",
            "Epoch   9 Batch   70/269, Loss: 1840.910\n",
            "Epoch   9 Batch   71/269, Loss: 1813.135\n",
            "Epoch   9 Batch   72/269, Loss: 1870.047\n",
            "Epoch   9 Batch   73/269, Loss: 1889.826\n",
            "Epoch   9 Batch   74/269, Loss: 1825.849\n",
            "Epoch   9 Batch   75/269, Loss: 1884.731\n",
            "Epoch   9 Batch   76/269, Loss: 1858.430\n",
            "Epoch   9 Batch   77/269, Loss: 1890.660\n",
            "Epoch   9 Batch   78/269, Loss: 1894.869\n",
            "Epoch   9 Batch   79/269, Loss: 1885.291\n",
            "Epoch   9 Batch   80/269, Loss: 1931.428\n",
            "Epoch   9 Batch   81/269, Loss: 1934.735\n",
            "Epoch   9 Batch   82/269, Loss: 1925.996\n",
            "Epoch   9 Batch   83/269, Loss: 1924.307\n",
            "Epoch   9 Batch   84/269, Loss: 1933.077\n",
            "Epoch   9 Batch   85/269, Loss: 1884.009\n",
            "Epoch   9 Batch   86/269, Loss: 1913.397\n",
            "Epoch   9 Batch   87/269, Loss: 1829.688\n",
            "Epoch   9 Batch   88/269, Loss: 1864.376\n",
            "Epoch   9 Batch   89/269, Loss: 1848.214\n",
            "Epoch   9 Batch   90/269, Loss: 1745.582\n",
            "Epoch   9 Batch   91/269, Loss: 1837.743\n",
            "Epoch   9 Batch   92/269, Loss: 1841.863\n",
            "Epoch   9 Batch   93/269, Loss: 1846.099\n",
            "Epoch   9 Batch   94/269, Loss: 1879.439\n",
            "Epoch   9 Batch   95/269, Loss: 1858.501\n",
            "Epoch   9 Batch   96/269, Loss: 1808.822\n",
            "Epoch   9 Batch   97/269, Loss: 1825.842\n",
            "Epoch   9 Batch   98/269, Loss: 1814.036\n",
            "Epoch   9 Batch   99/269, Loss: 1774.605\n",
            "Epoch   9 Batch  100/269, Loss: 1849.777\n",
            "Epoch   9 Batch  101/269, Loss: 1780.690\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch   9 Batch  102/269, Loss: 1831.671\n",
            "Epoch   9 Batch  103/269, Loss: 1905.039\n",
            "Epoch   9 Batch  104/269, Loss: 1852.136\n",
            "Epoch   9 Batch  105/269, Loss: 1899.399\n",
            "Epoch   9 Batch  106/269, Loss: 1866.879\n",
            "Epoch   9 Batch  107/269, Loss: 1829.458\n",
            "Epoch   9 Batch  108/269, Loss: 1856.031\n",
            "Epoch   9 Batch  109/269, Loss: 1906.039\n",
            "Epoch   9 Batch  110/269, Loss: 1890.621\n",
            "Epoch   9 Batch  111/269, Loss: 1866.631\n",
            "Epoch   9 Batch  112/269, Loss: 1886.669\n",
            "Epoch   9 Batch  113/269, Loss: 1904.845\n",
            "Epoch   9 Batch  114/269, Loss: 1890.614\n",
            "Epoch   9 Batch  115/269, Loss: 1788.668\n",
            "Epoch   9 Batch  116/269, Loss: 1873.078\n",
            "Epoch   9 Batch  117/269, Loss: 1838.296\n",
            "Epoch   9 Batch  118/269, Loss: 1863.858\n",
            "Epoch   9 Batch  119/269, Loss: 1885.140\n",
            "Epoch   9 Batch  120/269, Loss: 1811.626\n",
            "Epoch   9 Batch  121/269, Loss: 1851.054\n",
            "Epoch   9 Batch  122/269, Loss: 1849.797\n",
            "Epoch   9 Batch  123/269, Loss: 1785.819\n",
            "Epoch   9 Batch  124/269, Loss: 1797.184\n",
            "Epoch   9 Batch  125/269, Loss: 1782.729\n",
            "Epoch   9 Batch  126/269, Loss: 1776.327\n",
            "Epoch   9 Batch  127/269, Loss: 1782.530\n",
            "Epoch   9 Batch  128/269, Loss: 1798.801\n",
            "Epoch   9 Batch  129/269, Loss: 1768.807\n",
            "Epoch   9 Batch  130/269, Loss: 1698.893\n",
            "Epoch   9 Batch  131/269, Loss: 1787.647\n",
            "Epoch   9 Batch  132/269, Loss: 1724.976\n",
            "Epoch   9 Batch  133/269, Loss: 1760.334\n",
            "Epoch   9 Batch  134/269, Loss: 1771.098\n",
            "Epoch   9 Batch  135/269, Loss: 1692.079\n",
            "Epoch   9 Batch  136/269, Loss: 1694.241\n",
            "Epoch   9 Batch  137/269, Loss: 1731.437\n",
            "Epoch   9 Batch  138/269, Loss: 1711.155\n",
            "Epoch   9 Batch  139/269, Loss: 1760.127\n",
            "Epoch   9 Batch  140/269, Loss: 1714.968\n",
            "Epoch   9 Batch  141/269, Loss: 1679.750\n",
            "Epoch   9 Batch  142/269, Loss: 1690.853\n",
            "Epoch   9 Batch  143/269, Loss: 1678.720\n",
            "Epoch   9 Batch  144/269, Loss: 1700.212\n",
            "Epoch   9 Batch  145/269, Loss: 1675.885\n",
            "Epoch   9 Batch  146/269, Loss: 1648.443\n",
            "Epoch   9 Batch  147/269, Loss: 1678.581\n",
            "Epoch   9 Batch  148/269, Loss: 1625.631\n",
            "Epoch   9 Batch  149/269, Loss: 1649.165\n",
            "Epoch   9 Batch  150/269, Loss: 1641.527\n",
            "Epoch   9 Batch  151/269, Loss: 1705.968\n",
            "Epoch   9 Batch  152/269, Loss: 1657.925\n",
            "Epoch   9 Batch  153/269, Loss: 1630.386\n",
            "Epoch   9 Batch  154/269, Loss: 1560.979\n",
            "Epoch   9 Batch  155/269, Loss: 1648.438\n",
            "Epoch   9 Batch  156/269, Loss: 1624.826\n",
            "Epoch   9 Batch  157/269, Loss: 1600.659\n",
            "Epoch   9 Batch  158/269, Loss: 1616.157\n",
            "Epoch   9 Batch  159/269, Loss: 1649.402\n",
            "Epoch   9 Batch  160/269, Loss: 1651.198\n",
            "Epoch   9 Batch  161/269, Loss: 1668.857\n",
            "Epoch   9 Batch  162/269, Loss: 1683.696\n",
            "Epoch   9 Batch  163/269, Loss: 1693.394\n",
            "Epoch   9 Batch  164/269, Loss: 1724.740\n",
            "Epoch   9 Batch  165/269, Loss: 1712.781\n",
            "Epoch   9 Batch  166/269, Loss: 1800.390\n",
            "Epoch   9 Batch  167/269, Loss: 1764.837\n",
            "Epoch   9 Batch  168/269, Loss: 1769.932\n",
            "Epoch   9 Batch  169/269, Loss: 1731.483\n",
            "Epoch   9 Batch  170/269, Loss: 1749.035\n",
            "Epoch   9 Batch  171/269, Loss: 1712.288\n",
            "Epoch   9 Batch  172/269, Loss: 1719.490\n",
            "Epoch   9 Batch  173/269, Loss: 1707.113\n",
            "Epoch   9 Batch  174/269, Loss: 1721.201\n",
            "Epoch   9 Batch  175/269, Loss: 1632.039\n",
            "Epoch   9 Batch  176/269, Loss: 1628.675\n",
            "Epoch   9 Batch  177/269, Loss: 1698.782\n",
            "Epoch   9 Batch  178/269, Loss: 1641.859\n",
            "Epoch   9 Batch  179/269, Loss: 1652.119\n",
            "Epoch   9 Batch  180/269, Loss: 1689.061\n",
            "Epoch   9 Batch  181/269, Loss: 1732.272\n",
            "Epoch   9 Batch  182/269, Loss: 1686.167\n",
            "Epoch   9 Batch  183/269, Loss: 1761.804\n",
            "Epoch   9 Batch  184/269, Loss: 1605.953\n",
            "Epoch   9 Batch  185/269, Loss: 1690.978\n",
            "Epoch   9 Batch  186/269, Loss: 1565.879\n",
            "Epoch   9 Batch  187/269, Loss: 1676.150\n",
            "Epoch   9 Batch  188/269, Loss: 1664.787\n",
            "Epoch   9 Batch  189/269, Loss: 1669.615\n",
            "Epoch   9 Batch  190/269, Loss: 1638.996\n",
            "Epoch   9 Batch  191/269, Loss: 1605.351\n",
            "Epoch   9 Batch  192/269, Loss: 1646.892\n",
            "Epoch   9 Batch  193/269, Loss: 1638.284\n",
            "Epoch   9 Batch  194/269, Loss: 1624.871\n",
            "Epoch   9 Batch  195/269, Loss: 1649.609\n",
            "Epoch   9 Batch  196/269, Loss: 1651.528\n",
            "Epoch   9 Batch  197/269, Loss: 1607.342\n",
            "Epoch   9 Batch  198/269, Loss: 1617.692\n",
            "Epoch   9 Batch  199/269, Loss: 1640.767\n",
            "Epoch   9 Batch  200/269, Loss: 1583.932\n",
            "Epoch   9 Batch  201/269, Loss: 1651.682\n",
            "Epoch   9 Batch  202/269, Loss: 1679.409\n",
            "Epoch   9 Batch  203/269, Loss: 1726.832\n",
            "Epoch   9 Batch  204/269, Loss: 1725.473\n",
            "Epoch   9 Batch  205/269, Loss: 1785.797\n",
            "Epoch   9 Batch  206/269, Loss: 1734.814\n",
            "Epoch   9 Batch  207/269, Loss: 1779.297\n",
            "Epoch   9 Batch  208/269, Loss: 1763.363\n",
            "Epoch   9 Batch  209/269, Loss: 1733.078\n",
            "Epoch   9 Batch  210/269, Loss: 1769.979\n",
            "Epoch   9 Batch  211/269, Loss: 1754.601\n",
            "Epoch   9 Batch  212/269, Loss: 1777.808\n",
            "Epoch   9 Batch  213/269, Loss: 1738.662\n",
            "Epoch   9 Batch  214/269, Loss: 1754.788\n",
            "Epoch   9 Batch  215/269, Loss: 1819.675\n",
            "Epoch   9 Batch  216/269, Loss: 1704.230\n",
            "Epoch   9 Batch  217/269, Loss: 1690.125\n",
            "Epoch   9 Batch  218/269, Loss: 1682.802\n",
            "Epoch   9 Batch  219/269, Loss: 1722.797\n",
            "Epoch   9 Batch  220/269, Loss: 1771.181\n",
            "Epoch   9 Batch  221/269, Loss: 1743.671\n",
            "Epoch   9 Batch  222/269, Loss: 1753.798\n",
            "Epoch   9 Batch  223/269, Loss: 1747.614\n",
            "Epoch   9 Batch  224/269, Loss: 1779.180\n",
            "Epoch   9 Batch  225/269, Loss: 1734.515\n",
            "Epoch   9 Batch  226/269, Loss: 1802.428\n",
            "Epoch   9 Batch  227/269, Loss: 1894.178\n",
            "Epoch   9 Batch  228/269, Loss: 1733.503\n",
            "Epoch   9 Batch  229/269, Loss: 1791.660\n",
            "Epoch   9 Batch  230/269, Loss: 1747.256\n",
            "Epoch   9 Batch  231/269, Loss: 1684.312\n",
            "Epoch   9 Batch  232/269, Loss: 1624.803\n",
            "Epoch   9 Batch  233/269, Loss: 1698.296\n",
            "Epoch   9 Batch  234/269, Loss: 1663.871\n",
            "Epoch   9 Batch  235/269, Loss: 1698.446\n",
            "Epoch   9 Batch  236/269, Loss: 1681.891\n",
            "Epoch   9 Batch  237/269, Loss: 1676.866\n",
            "Epoch   9 Batch  238/269, Loss: 1697.171\n",
            "Epoch   9 Batch  239/269, Loss: 1703.009\n",
            "Epoch   9 Batch  240/269, Loss: 1791.015\n",
            "Epoch   9 Batch  241/269, Loss: 1761.669\n",
            "Epoch   9 Batch  242/269, Loss: 1684.389\n",
            "Epoch   9 Batch  243/269, Loss: 1720.921\n",
            "Epoch   9 Batch  244/269, Loss: 1708.291\n",
            "Epoch   9 Batch  245/269, Loss: 1649.158\n",
            "Epoch   9 Batch  246/269, Loss: 1742.537\n",
            "Epoch   9 Batch  247/269, Loss: 1634.455\n",
            "Epoch   9 Batch  248/269, Loss: 1714.657\n",
            "Epoch   9 Batch  249/269, Loss: 1699.648\n",
            "Epoch   9 Batch  250/269, Loss: 1672.717\n",
            "Epoch   9 Batch  251/269, Loss: 1670.331\n",
            "Epoch   9 Batch  252/269, Loss: 1582.260\n",
            "Epoch   9 Batch  253/269, Loss: 1576.757\n",
            "Epoch   9 Batch  254/269, Loss: 1554.816\n",
            "Epoch   9 Batch  255/269, Loss: 1592.350\n",
            "Epoch   9 Batch  256/269, Loss: 1476.928\n",
            "Epoch   9 Batch  257/269, Loss: 1456.439\n",
            "Epoch   9 Batch  258/269, Loss: 1429.199\n",
            "Epoch   9 Batch  259/269, Loss: 1419.430\n",
            "Epoch   9 Batch  260/269, Loss: 1377.922\n",
            "Epoch   9 Batch  261/269, Loss: 1331.017\n",
            "Epoch   9 Batch  262/269, Loss: 1388.705\n",
            "Epoch   9 Batch  263/269, Loss: 1386.939\n",
            "Epoch   9 Batch  264/269, Loss: 1376.546\n",
            "Epoch   9 Batch  265/269, Loss: 1320.419\n",
            "Epoch   9 Batch  266/269, Loss: 1390.111\n",
            "Epoch   9 Batch  267/269, Loss: 1402.875\n",
            "Epoch  10 Batch    0/269, Loss: 1379.955\n",
            "Epoch  10 Batch    1/269, Loss: 1385.978\n",
            "Epoch  10 Batch    2/269, Loss: 1417.767\n",
            "Epoch  10 Batch    3/269, Loss: 1415.761\n",
            "Epoch  10 Batch    4/269, Loss: 1366.084\n",
            "Epoch  10 Batch    5/269, Loss: 1357.629\n",
            "Epoch  10 Batch    6/269, Loss: 1360.774\n",
            "Epoch  10 Batch    7/269, Loss: 1362.723\n",
            "Epoch  10 Batch    8/269, Loss: 1331.948\n",
            "Epoch  10 Batch    9/269, Loss: 1327.542\n",
            "Epoch  10 Batch   10/269, Loss: 1300.704\n",
            "Epoch  10 Batch   11/269, Loss: 1343.282\n",
            "Epoch  10 Batch   12/269, Loss: 1319.489\n",
            "Epoch  10 Batch   13/269, Loss: 1336.211\n",
            "Epoch  10 Batch   14/269, Loss: 1303.505\n",
            "Epoch  10 Batch   15/269, Loss: 1325.272\n",
            "Epoch  10 Batch   16/269, Loss: 1342.083\n",
            "Epoch  10 Batch   17/269, Loss: 1350.871\n",
            "Epoch  10 Batch   18/269, Loss: 1307.324\n",
            "Epoch  10 Batch   19/269, Loss: 1332.627\n",
            "Epoch  10 Batch   20/269, Loss: 1328.293\n",
            "Epoch  10 Batch   21/269, Loss: 1350.043\n",
            "Epoch  10 Batch   22/269, Loss: 1421.036\n",
            "Epoch  10 Batch   23/269, Loss: 1410.854\n",
            "Epoch  10 Batch   24/269, Loss: 1398.145\n",
            "Epoch  10 Batch   25/269, Loss: 1399.762\n",
            "Epoch  10 Batch   26/269, Loss: 1431.515\n",
            "Epoch  10 Batch   27/269, Loss: 1445.125\n",
            "Epoch  10 Batch   28/269, Loss: 1426.148\n",
            "Epoch  10 Batch   29/269, Loss: 1455.369\n",
            "Epoch  10 Batch   30/269, Loss: 1462.103\n",
            "Epoch  10 Batch   31/269, Loss: 1508.233\n",
            "Epoch  10 Batch   32/269, Loss: 1510.438\n",
            "Epoch  10 Batch   33/269, Loss: 1536.997\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch  10 Batch   34/269, Loss: 1541.419\n",
            "Epoch  10 Batch   35/269, Loss: 1561.489\n",
            "Epoch  10 Batch   36/269, Loss: 1552.530\n",
            "Epoch  10 Batch   37/269, Loss: 1595.225\n",
            "Epoch  10 Batch   38/269, Loss: 1570.520\n",
            "Epoch  10 Batch   39/269, Loss: 1585.477\n",
            "Epoch  10 Batch   40/269, Loss: 1600.212\n",
            "Epoch  10 Batch   41/269, Loss: 1616.638\n",
            "Epoch  10 Batch   42/269, Loss: 1655.980\n",
            "Epoch  10 Batch   43/269, Loss: 1643.622\n",
            "Epoch  10 Batch   44/269, Loss: 1707.080\n",
            "Epoch  10 Batch   45/269, Loss: 1670.549\n",
            "Epoch  10 Batch   46/269, Loss: 1651.397\n",
            "Epoch  10 Batch   47/269, Loss: 1701.271\n",
            "Epoch  10 Batch   48/269, Loss: 1719.376\n",
            "Epoch  10 Batch   49/269, Loss: 1715.675\n",
            "Epoch  10 Batch   50/269, Loss: 1755.350\n",
            "Epoch  10 Batch   51/269, Loss: 1785.772\n",
            "Epoch  10 Batch   52/269, Loss: 1761.863\n",
            "Epoch  10 Batch   53/269, Loss: 1783.646\n",
            "Epoch  10 Batch   54/269, Loss: 1788.759\n",
            "Epoch  10 Batch   55/269, Loss: 1840.234\n",
            "Epoch  10 Batch   56/269, Loss: 1872.146\n",
            "Epoch  10 Batch   57/269, Loss: 1842.823\n",
            "Epoch  10 Batch   58/269, Loss: 1879.505\n",
            "Epoch  10 Batch   59/269, Loss: 1900.319\n",
            "Epoch  10 Batch   60/269, Loss: 1871.154\n",
            "Epoch  10 Batch   61/269, Loss: 1932.723\n",
            "Epoch  10 Batch   62/269, Loss: 1953.059\n",
            "Epoch  10 Batch   63/269, Loss: 1923.283\n",
            "Epoch  10 Batch   64/269, Loss: 1899.711\n",
            "Epoch  10 Batch   65/269, Loss: 1923.394\n",
            "Epoch  10 Batch   66/269, Loss: 1950.182\n",
            "Epoch  10 Batch   67/269, Loss: 1960.751\n",
            "Epoch  10 Batch   68/269, Loss: 2007.478\n",
            "Epoch  10 Batch   69/269, Loss: 1936.554\n",
            "Epoch  10 Batch   70/269, Loss: 1994.282\n",
            "Epoch  10 Batch   71/269, Loss: 1944.981\n",
            "Epoch  10 Batch   72/269, Loss: 1991.916\n",
            "Epoch  10 Batch   73/269, Loss: 1964.353\n",
            "Epoch  10 Batch   74/269, Loss: 1901.681\n",
            "Epoch  10 Batch   75/269, Loss: 1945.435\n",
            "Epoch  10 Batch   76/269, Loss: 1875.300\n",
            "Epoch  10 Batch   77/269, Loss: 1896.287\n",
            "Epoch  10 Batch   78/269, Loss: 1862.795\n",
            "Epoch  10 Batch   79/269, Loss: 1840.596\n",
            "Epoch  10 Batch   80/269, Loss: 1872.151\n",
            "Epoch  10 Batch   81/269, Loss: 1860.108\n",
            "Epoch  10 Batch   82/269, Loss: 1838.903\n",
            "Epoch  10 Batch   83/269, Loss: 1804.995\n",
            "Epoch  10 Batch   84/269, Loss: 1788.781\n",
            "Epoch  10 Batch   85/269, Loss: 1749.714\n",
            "Epoch  10 Batch   86/269, Loss: 1763.792\n",
            "Epoch  10 Batch   87/269, Loss: 1707.438\n",
            "Epoch  10 Batch   88/269, Loss: 1718.182\n",
            "Epoch  10 Batch   89/269, Loss: 1742.205\n",
            "Epoch  10 Batch   90/269, Loss: 1692.642\n",
            "Epoch  10 Batch   91/269, Loss: 1770.349\n",
            "Epoch  10 Batch   92/269, Loss: 1775.915\n",
            "Epoch  10 Batch   93/269, Loss: 1802.596\n",
            "Epoch  10 Batch   94/269, Loss: 1817.978\n",
            "Epoch  10 Batch   95/269, Loss: 1781.771\n",
            "Epoch  10 Batch   96/269, Loss: 1775.844\n",
            "Epoch  10 Batch   97/269, Loss: 1803.131\n",
            "Epoch  10 Batch   98/269, Loss: 1792.997\n",
            "Epoch  10 Batch   99/269, Loss: 1763.950\n",
            "Epoch  10 Batch  100/269, Loss: 1842.748\n",
            "Epoch  10 Batch  101/269, Loss: 1764.221\n",
            "Epoch  10 Batch  102/269, Loss: 1813.226\n",
            "Epoch  10 Batch  103/269, Loss: 1835.756\n",
            "Epoch  10 Batch  104/269, Loss: 1786.281\n",
            "Epoch  10 Batch  105/269, Loss: 1811.527\n",
            "Epoch  10 Batch  106/269, Loss: 1780.387\n",
            "Epoch  10 Batch  107/269, Loss: 1773.845\n",
            "Epoch  10 Batch  108/269, Loss: 1789.528\n",
            "Epoch  10 Batch  109/269, Loss: 1822.410\n",
            "Epoch  10 Batch  110/269, Loss: 1834.837\n",
            "Epoch  10 Batch  111/269, Loss: 1828.116\n",
            "Epoch  10 Batch  112/269, Loss: 1857.841\n",
            "Epoch  10 Batch  113/269, Loss: 1865.419\n",
            "Epoch  10 Batch  114/269, Loss: 1844.616\n",
            "Epoch  10 Batch  115/269, Loss: 1756.699\n",
            "Epoch  10 Batch  116/269, Loss: 1799.546\n",
            "Epoch  10 Batch  117/269, Loss: 1767.456\n",
            "Epoch  10 Batch  118/269, Loss: 1734.217\n",
            "Epoch  10 Batch  119/269, Loss: 1748.471\n",
            "Epoch  10 Batch  120/269, Loss: 1709.795\n",
            "Epoch  10 Batch  121/269, Loss: 1740.506\n",
            "Epoch  10 Batch  122/269, Loss: 1759.612\n",
            "Epoch  10 Batch  123/269, Loss: 1705.458\n",
            "Epoch  10 Batch  124/269, Loss: 1704.789\n",
            "Epoch  10 Batch  125/269, Loss: 1722.028\n",
            "Epoch  10 Batch  126/269, Loss: 1723.145\n",
            "Epoch  10 Batch  127/269, Loss: 1736.460\n",
            "Epoch  10 Batch  128/269, Loss: 1783.902\n",
            "Epoch  10 Batch  129/269, Loss: 1783.174\n",
            "Epoch  10 Batch  130/269, Loss: 1754.803\n",
            "Epoch  10 Batch  131/269, Loss: 1794.667\n",
            "Epoch  10 Batch  132/269, Loss: 1769.197\n",
            "Epoch  10 Batch  133/269, Loss: 1774.322\n",
            "Epoch  10 Batch  134/269, Loss: 1791.886\n",
            "Epoch  10 Batch  135/269, Loss: 1754.914\n",
            "Epoch  10 Batch  136/269, Loss: 1774.217\n",
            "Epoch  10 Batch  137/269, Loss: 1754.468\n",
            "Epoch  10 Batch  138/269, Loss: 1757.583\n",
            "Epoch  10 Batch  139/269, Loss: 1781.861\n",
            "Epoch  10 Batch  140/269, Loss: 1755.615\n",
            "Epoch  10 Batch  141/269, Loss: 1743.548\n",
            "Epoch  10 Batch  142/269, Loss: 1757.916\n",
            "Epoch  10 Batch  143/269, Loss: 1756.084\n",
            "Epoch  10 Batch  144/269, Loss: 1732.402\n",
            "Epoch  10 Batch  145/269, Loss: 1731.796\n",
            "Epoch  10 Batch  146/269, Loss: 1730.154\n",
            "Epoch  10 Batch  147/269, Loss: 1788.238\n",
            "Epoch  10 Batch  148/269, Loss: 1729.551\n",
            "Epoch  10 Batch  149/269, Loss: 1749.504\n",
            "Epoch  10 Batch  150/269, Loss: 1763.809\n",
            "Epoch  10 Batch  151/269, Loss: 1781.356\n",
            "Epoch  10 Batch  152/269, Loss: 1747.738\n",
            "Epoch  10 Batch  153/269, Loss: 1733.564\n",
            "Epoch  10 Batch  154/269, Loss: 1693.842\n",
            "Epoch  10 Batch  155/269, Loss: 1780.473\n",
            "Epoch  10 Batch  156/269, Loss: 1742.598\n",
            "Epoch  10 Batch  157/269, Loss: 1728.139\n",
            "Epoch  10 Batch  158/269, Loss: 1753.145\n",
            "Epoch  10 Batch  159/269, Loss: 1777.942\n",
            "Epoch  10 Batch  160/269, Loss: 1758.036\n",
            "Epoch  10 Batch  161/269, Loss: 1711.257\n",
            "Epoch  10 Batch  162/269, Loss: 1686.524\n",
            "Epoch  10 Batch  163/269, Loss: 1645.501\n",
            "Epoch  10 Batch  164/269, Loss: 1649.992\n",
            "Epoch  10 Batch  165/269, Loss: 1635.286\n",
            "Epoch  10 Batch  166/269, Loss: 1675.501\n",
            "Epoch  10 Batch  167/269, Loss: 1616.422\n",
            "Epoch  10 Batch  168/269, Loss: 1620.758\n",
            "Epoch  10 Batch  169/269, Loss: 1569.448\n",
            "Epoch  10 Batch  170/269, Loss: 1585.311\n",
            "Epoch  10 Batch  171/269, Loss: 1555.716\n",
            "Epoch  10 Batch  172/269, Loss: 1549.870\n",
            "Epoch  10 Batch  173/269, Loss: 1547.666\n",
            "Epoch  10 Batch  174/269, Loss: 1529.203\n",
            "Epoch  10 Batch  175/269, Loss: 1490.420\n",
            "Epoch  10 Batch  176/269, Loss: 1482.346\n",
            "Epoch  10 Batch  177/269, Loss: 1489.456\n",
            "Epoch  10 Batch  178/269, Loss: 1451.725\n",
            "Epoch  10 Batch  179/269, Loss: 1462.150\n",
            "Epoch  10 Batch  180/269, Loss: 1463.667\n",
            "Epoch  10 Batch  181/269, Loss: 1499.716\n",
            "Epoch  10 Batch  182/269, Loss: 1453.686\n",
            "Epoch  10 Batch  183/269, Loss: 1469.626\n",
            "Epoch  10 Batch  184/269, Loss: 1431.974\n",
            "Epoch  10 Batch  185/269, Loss: 1465.323\n",
            "Epoch  10 Batch  186/269, Loss: 1407.102\n",
            "Epoch  10 Batch  187/269, Loss: 1451.434\n",
            "Epoch  10 Batch  188/269, Loss: 1447.388\n",
            "Epoch  10 Batch  189/269, Loss: 1428.788\n",
            "Epoch  10 Batch  190/269, Loss: 1431.107\n",
            "Epoch  10 Batch  191/269, Loss: 1400.656\n",
            "Epoch  10 Batch  192/269, Loss: 1422.255\n",
            "Epoch  10 Batch  193/269, Loss: 1401.691\n",
            "Epoch  10 Batch  194/269, Loss: 1373.550\n",
            "Epoch  10 Batch  195/269, Loss: 1390.965\n",
            "Epoch  10 Batch  196/269, Loss: 1385.073\n",
            "Epoch  10 Batch  197/269, Loss: 1346.432\n",
            "Epoch  10 Batch  198/269, Loss: 1331.323\n",
            "Epoch  10 Batch  199/269, Loss: 1320.212\n",
            "Epoch  10 Batch  200/269, Loss: 1276.014\n",
            "Epoch  10 Batch  201/269, Loss: 1286.340\n",
            "Epoch  10 Batch  202/269, Loss: 1247.493\n",
            "Epoch  10 Batch  203/269, Loss: 1262.959\n",
            "Epoch  10 Batch  204/269, Loss: 1233.696\n",
            "Epoch  10 Batch  205/269, Loss: 1237.994\n",
            "Epoch  10 Batch  206/269, Loss: 1189.227\n",
            "Epoch  10 Batch  207/269, Loss: 1211.123\n",
            "Epoch  10 Batch  208/269, Loss: 1189.136\n",
            "Epoch  10 Batch  209/269, Loss: 1158.591\n",
            "Epoch  10 Batch  210/269, Loss: 1176.843\n",
            "Epoch  10 Batch  211/269, Loss: 1155.864\n",
            "Epoch  10 Batch  212/269, Loss: 1167.053\n",
            "Epoch  10 Batch  213/269, Loss: 1158.728\n",
            "Epoch  10 Batch  214/269, Loss: 1155.456\n",
            "Epoch  10 Batch  215/269, Loss: 1158.518\n",
            "Epoch  10 Batch  216/269, Loss: 1145.922\n",
            "Epoch  10 Batch  217/269, Loss: 1095.941\n",
            "Epoch  10 Batch  218/269, Loss: 1095.742\n",
            "Epoch  10 Batch  219/269, Loss: 1097.969\n",
            "Epoch  10 Batch  220/269, Loss: 1095.372\n",
            "Epoch  10 Batch  221/269, Loss: 1097.283\n",
            "Epoch  10 Batch  222/269, Loss: 1099.003\n",
            "Epoch  10 Batch  223/269, Loss: 1079.594\n",
            "Epoch  10 Batch  224/269, Loss: 1091.088\n",
            "Epoch  10 Batch  225/269, Loss: 1061.232\n",
            "Epoch  10 Batch  226/269, Loss: 1088.948\n",
            "Epoch  10 Batch  227/269, Loss: 1108.462\n",
            "Epoch  10 Batch  228/269, Loss: 1043.287\n",
            "Epoch  10 Batch  229/269, Loss: 1055.789\n",
            "Epoch  10 Batch  230/269, Loss: 1040.085\n",
            "Epoch  10 Batch  231/269, Loss: 1029.044\n",
            "Epoch  10 Batch  232/269, Loss: 1012.604\n",
            "Epoch  10 Batch  233/269, Loss: 1037.178\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch  10 Batch  234/269, Loss: 1003.412\n",
            "Epoch  10 Batch  235/269, Loss: 1025.502\n",
            "Epoch  10 Batch  236/269, Loss: 1013.197\n",
            "Epoch  10 Batch  237/269, Loss: 1012.048\n",
            "Epoch  10 Batch  238/269, Loss: 1022.730\n",
            "Epoch  10 Batch  239/269, Loss: 1002.817\n",
            "Epoch  10 Batch  240/269, Loss: 1014.037\n",
            "Epoch  10 Batch  241/269, Loss: 1014.183\n",
            "Epoch  10 Batch  242/269, Loss: 973.164\n",
            "Epoch  10 Batch  243/269, Loss: 994.750\n",
            "Epoch  10 Batch  244/269, Loss: 1003.805\n",
            "Epoch  10 Batch  245/269, Loss: 1001.695\n",
            "Epoch  10 Batch  246/269, Loss: 987.991\n",
            "Epoch  10 Batch  247/269, Loss: 956.070\n",
            "Epoch  10 Batch  248/269, Loss: 980.226\n",
            "Epoch  10 Batch  249/269, Loss: 969.370\n",
            "Epoch  10 Batch  250/269, Loss: 951.363\n",
            "Epoch  10 Batch  251/269, Loss: 947.496\n",
            "Epoch  10 Batch  252/269, Loss: 956.646\n",
            "Epoch  10 Batch  253/269, Loss: 952.710\n",
            "Epoch  10 Batch  254/269, Loss: 976.562\n",
            "Epoch  10 Batch  255/269, Loss: 1002.743\n",
            "Epoch  10 Batch  256/269, Loss: 964.031\n",
            "Epoch  10 Batch  257/269, Loss: 992.799\n",
            "Epoch  10 Batch  258/269, Loss: 1003.004\n",
            "Epoch  10 Batch  259/269, Loss: 1017.595\n",
            "Epoch  10 Batch  260/269, Loss: 1006.973\n",
            "Epoch  10 Batch  261/269, Loss: 1003.236\n",
            "Epoch  10 Batch  262/269, Loss: 1035.096\n",
            "Epoch  10 Batch  263/269, Loss: 1025.390\n",
            "Epoch  10 Batch  264/269, Loss: 1026.889\n",
            "Epoch  10 Batch  265/269, Loss: 999.145\n",
            "Epoch  10 Batch  266/269, Loss: 1035.170\n",
            "Epoch  10 Batch  267/269, Loss: 1047.546\n",
            "Epoch  11 Batch    0/269, Loss: 1024.878\n",
            "Epoch  11 Batch    1/269, Loss: 1021.052\n",
            "Epoch  11 Batch    2/269, Loss: 1039.310\n",
            "Epoch  11 Batch    3/269, Loss: 1059.906\n",
            "Epoch  11 Batch    4/269, Loss: 1019.512\n",
            "Epoch  11 Batch    5/269, Loss: 1056.204\n",
            "Epoch  11 Batch    6/269, Loss: 1064.948\n",
            "Epoch  11 Batch    7/269, Loss: 1087.298\n",
            "Epoch  11 Batch    8/269, Loss: 1071.414\n",
            "Epoch  11 Batch    9/269, Loss: 1073.548\n",
            "Epoch  11 Batch   10/269, Loss: 1053.507\n",
            "Epoch  11 Batch   11/269, Loss: 1096.388\n",
            "Epoch  11 Batch   12/269, Loss: 1077.552\n",
            "Epoch  11 Batch   13/269, Loss: 1124.617\n",
            "Epoch  11 Batch   14/269, Loss: 1102.990\n",
            "Epoch  11 Batch   15/269, Loss: 1082.130\n",
            "Epoch  11 Batch   16/269, Loss: 1124.524\n",
            "Epoch  11 Batch   17/269, Loss: 1101.890\n",
            "Epoch  11 Batch   18/269, Loss: 1079.151\n",
            "Epoch  11 Batch   19/269, Loss: 1124.221\n",
            "Epoch  11 Batch   20/269, Loss: 1083.799\n",
            "Epoch  11 Batch   21/269, Loss: 1083.544\n",
            "Epoch  11 Batch   22/269, Loss: 1131.976\n",
            "Epoch  11 Batch   23/269, Loss: 1159.579\n",
            "Epoch  11 Batch   24/269, Loss: 1099.972\n",
            "Epoch  11 Batch   25/269, Loss: 1111.817\n",
            "Epoch  11 Batch   26/269, Loss: 1159.986\n",
            "Epoch  11 Batch   27/269, Loss: 1127.124\n",
            "Epoch  11 Batch   28/269, Loss: 1066.548\n",
            "Epoch  11 Batch   29/269, Loss: 1108.232\n",
            "Epoch  11 Batch   30/269, Loss: 1125.298\n",
            "Epoch  11 Batch   31/269, Loss: 1147.321\n",
            "Epoch  11 Batch   32/269, Loss: 1131.486\n",
            "Epoch  11 Batch   33/269, Loss: 1146.238\n",
            "Epoch  11 Batch   34/269, Loss: 1135.821\n",
            "Epoch  11 Batch   35/269, Loss: 1140.516\n",
            "Epoch  11 Batch   36/269, Loss: 1156.885\n",
            "Epoch  11 Batch   37/269, Loss: 1163.590\n",
            "Epoch  11 Batch   38/269, Loss: 1162.085\n",
            "Epoch  11 Batch   39/269, Loss: 1150.221\n",
            "Epoch  11 Batch   40/269, Loss: 1130.858\n",
            "Epoch  11 Batch   41/269, Loss: 1159.775\n",
            "Epoch  11 Batch   42/269, Loss: 1182.947\n",
            "Epoch  11 Batch   43/269, Loss: 1157.494\n",
            "Epoch  11 Batch   44/269, Loss: 1200.712\n",
            "Epoch  11 Batch   45/269, Loss: 1171.406\n",
            "Epoch  11 Batch   46/269, Loss: 1145.237\n",
            "Epoch  11 Batch   47/269, Loss: 1171.457\n",
            "Epoch  11 Batch   48/269, Loss: 1146.419\n",
            "Epoch  11 Batch   49/269, Loss: 1130.168\n",
            "Epoch  11 Batch   50/269, Loss: 1110.184\n",
            "Epoch  11 Batch   51/269, Loss: 1107.360\n",
            "Epoch  11 Batch   52/269, Loss: 1110.972\n",
            "Epoch  11 Batch   53/269, Loss: 1098.969\n",
            "Epoch  11 Batch   54/269, Loss: 1064.972\n",
            "Epoch  11 Batch   55/269, Loss: 1070.396\n",
            "Epoch  11 Batch   56/269, Loss: 1101.769\n",
            "Epoch  11 Batch   57/269, Loss: 1085.253\n",
            "Epoch  11 Batch   58/269, Loss: 1065.894\n",
            "Epoch  11 Batch   59/269, Loss: 1078.150\n",
            "Epoch  11 Batch   60/269, Loss: 1043.003\n",
            "Epoch  11 Batch   61/269, Loss: 1070.500\n",
            "Epoch  11 Batch   62/269, Loss: 1070.002\n",
            "Epoch  11 Batch   63/269, Loss: 1044.836\n",
            "Epoch  11 Batch   64/269, Loss: 1026.521\n",
            "Epoch  11 Batch   65/269, Loss: 1043.731\n",
            "Epoch  11 Batch   66/269, Loss: 1037.340\n",
            "Epoch  11 Batch   67/269, Loss: 1023.212\n",
            "Epoch  11 Batch   68/269, Loss: 1068.319\n",
            "Epoch  11 Batch   69/269, Loss: 1025.978\n",
            "Epoch  11 Batch   70/269, Loss: 1048.626\n",
            "Epoch  11 Batch   71/269, Loss: 1011.057\n",
            "Epoch  11 Batch   72/269, Loss: 1073.208\n",
            "Epoch  11 Batch   73/269, Loss: 1046.854\n",
            "Epoch  11 Batch   74/269, Loss: 1019.240\n",
            "Epoch  11 Batch   75/269, Loss: 1027.994\n",
            "Epoch  11 Batch   76/269, Loss: 1005.762\n",
            "Epoch  11 Batch   77/269, Loss: 1017.848\n",
            "Epoch  11 Batch   78/269, Loss: 1020.007\n",
            "Epoch  11 Batch   79/269, Loss: 1000.741\n",
            "Epoch  11 Batch   80/269, Loss: 1006.673\n",
            "Epoch  11 Batch   81/269, Loss: 1015.620\n",
            "Epoch  11 Batch   82/269, Loss: 1011.497\n",
            "Epoch  11 Batch   83/269, Loss: 1016.102\n",
            "Epoch  11 Batch   84/269, Loss: 1010.399\n",
            "Epoch  11 Batch   85/269, Loss: 1000.123\n",
            "Epoch  11 Batch   86/269, Loss: 1002.818\n",
            "Epoch  11 Batch   87/269, Loss: 996.062\n",
            "Epoch  11 Batch   88/269, Loss: 998.499\n",
            "Epoch  11 Batch   89/269, Loss: 1010.861\n",
            "Epoch  11 Batch   90/269, Loss: 977.046\n",
            "Epoch  11 Batch   91/269, Loss: 999.106\n",
            "Epoch  11 Batch   92/269, Loss: 1001.616\n",
            "Epoch  11 Batch   93/269, Loss: 1011.754\n",
            "Epoch  11 Batch   94/269, Loss: 1023.144\n",
            "Epoch  11 Batch   95/269, Loss: 991.550\n",
            "Epoch  11 Batch   96/269, Loss: 987.286\n",
            "Epoch  11 Batch   97/269, Loss: 991.544\n",
            "Epoch  11 Batch   98/269, Loss: 984.533\n",
            "Epoch  11 Batch   99/269, Loss: 975.707\n",
            "Epoch  11 Batch  100/269, Loss: 993.397\n",
            "Epoch  11 Batch  101/269, Loss: 969.935\n",
            "Epoch  11 Batch  102/269, Loss: 989.312\n",
            "Epoch  11 Batch  103/269, Loss: 1008.399\n",
            "Epoch  11 Batch  104/269, Loss: 969.792\n",
            "Epoch  11 Batch  105/269, Loss: 997.610\n",
            "Epoch  11 Batch  106/269, Loss: 974.613\n",
            "Epoch  11 Batch  107/269, Loss: 946.717\n",
            "Epoch  11 Batch  108/269, Loss: 948.869\n",
            "Epoch  11 Batch  109/269, Loss: 985.141\n",
            "Epoch  11 Batch  110/269, Loss: 970.893\n",
            "Epoch  11 Batch  111/269, Loss: 984.214\n",
            "Epoch  11 Batch  112/269, Loss: 991.176\n",
            "Epoch  11 Batch  113/269, Loss: 1000.566\n",
            "Epoch  11 Batch  114/269, Loss: 998.449\n",
            "Epoch  11 Batch  115/269, Loss: 949.845\n",
            "Epoch  11 Batch  116/269, Loss: 979.797\n",
            "Epoch  11 Batch  117/269, Loss: 975.480\n",
            "Epoch  11 Batch  118/269, Loss: 966.032\n",
            "Epoch  11 Batch  119/269, Loss: 991.495\n",
            "Epoch  11 Batch  120/269, Loss: 970.493\n",
            "Epoch  11 Batch  121/269, Loss: 959.723\n",
            "Epoch  11 Batch  122/269, Loss: 995.692\n",
            "Epoch  11 Batch  123/269, Loss: 973.601\n",
            "Epoch  11 Batch  124/269, Loss: 974.129\n",
            "Epoch  11 Batch  125/269, Loss: 1006.160\n",
            "Epoch  11 Batch  126/269, Loss: 1011.143\n",
            "Epoch  11 Batch  127/269, Loss: 995.557\n",
            "Epoch  11 Batch  128/269, Loss: 1019.238\n",
            "Epoch  11 Batch  129/269, Loss: 1036.501\n",
            "Epoch  11 Batch  130/269, Loss: 981.069\n",
            "Epoch  11 Batch  131/269, Loss: 985.536\n",
            "Epoch  11 Batch  132/269, Loss: 991.032\n",
            "Epoch  11 Batch  133/269, Loss: 983.654\n",
            "Epoch  11 Batch  134/269, Loss: 991.540\n",
            "Epoch  11 Batch  135/269, Loss: 965.541\n",
            "Epoch  11 Batch  136/269, Loss: 976.356\n",
            "Epoch  11 Batch  137/269, Loss: 983.779\n",
            "Epoch  11 Batch  138/269, Loss: 981.729\n",
            "Epoch  11 Batch  139/269, Loss: 1007.293\n",
            "Epoch  11 Batch  140/269, Loss: 987.469\n",
            "Epoch  11 Batch  141/269, Loss: 989.483\n",
            "Epoch  11 Batch  142/269, Loss: 1004.394\n",
            "Epoch  11 Batch  143/269, Loss: 1021.360\n",
            "Epoch  11 Batch  144/269, Loss: 1034.403\n",
            "Epoch  11 Batch  145/269, Loss: 1040.044\n",
            "Epoch  11 Batch  146/269, Loss: 1054.218\n",
            "Epoch  11 Batch  147/269, Loss: 1106.938\n",
            "Epoch  11 Batch  148/269, Loss: 1096.533\n",
            "Epoch  11 Batch  149/269, Loss: 1092.765\n",
            "Epoch  11 Batch  150/269, Loss: 1110.788\n",
            "Epoch  11 Batch  151/269, Loss: 1159.692\n",
            "Epoch  11 Batch  152/269, Loss: 1124.797\n",
            "Epoch  11 Batch  153/269, Loss: 1152.205\n",
            "Epoch  11 Batch  154/269, Loss: 1085.842\n",
            "Epoch  11 Batch  155/269, Loss: 1196.121\n",
            "Epoch  11 Batch  156/269, Loss: 1151.606\n",
            "Epoch  11 Batch  157/269, Loss: 1169.343\n",
            "Epoch  11 Batch  158/269, Loss: 1196.376\n",
            "Epoch  11 Batch  159/269, Loss: 1212.064\n",
            "Epoch  11 Batch  160/269, Loss: 1223.569\n",
            "Epoch  11 Batch  161/269, Loss: 1218.445\n",
            "Epoch  11 Batch  162/269, Loss: 1241.067\n",
            "Epoch  11 Batch  163/269, Loss: 1217.677\n",
            "Epoch  11 Batch  164/269, Loss: 1252.755\n",
            "Epoch  11 Batch  165/269, Loss: 1222.547\n",
            "Epoch  11 Batch  166/269, Loss: 1272.180\n",
            "Epoch  11 Batch  167/269, Loss: 1245.620\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch  11 Batch  168/269, Loss: 1250.859\n",
            "Epoch  11 Batch  169/269, Loss: 1230.205\n",
            "Epoch  11 Batch  170/269, Loss: 1255.553\n",
            "Epoch  11 Batch  171/269, Loss: 1225.102\n",
            "Epoch  11 Batch  172/269, Loss: 1261.711\n",
            "Epoch  11 Batch  173/269, Loss: 1247.247\n",
            "Epoch  11 Batch  174/269, Loss: 1246.078\n",
            "Epoch  11 Batch  175/269, Loss: 1221.113\n",
            "Epoch  11 Batch  176/269, Loss: 1204.225\n",
            "Epoch  11 Batch  177/269, Loss: 1210.823\n",
            "Epoch  11 Batch  178/269, Loss: 1197.030\n",
            "Epoch  11 Batch  179/269, Loss: 1209.981\n",
            "Epoch  11 Batch  180/269, Loss: 1178.890\n",
            "Epoch  11 Batch  181/269, Loss: 1218.550\n",
            "Epoch  11 Batch  182/269, Loss: 1192.430\n",
            "Epoch  11 Batch  183/269, Loss: 1236.930\n",
            "Epoch  11 Batch  184/269, Loss: 1186.868\n",
            "Epoch  11 Batch  185/269, Loss: 1236.126\n",
            "Epoch  11 Batch  186/269, Loss: 1197.341\n",
            "Epoch  11 Batch  187/269, Loss: 1227.290\n",
            "Epoch  11 Batch  188/269, Loss: 1234.425\n",
            "Epoch  11 Batch  189/269, Loss: 1242.464\n",
            "Epoch  11 Batch  190/269, Loss: 1230.133\n",
            "Epoch  11 Batch  191/269, Loss: 1218.698\n",
            "Epoch  11 Batch  192/269, Loss: 1269.539\n",
            "Epoch  11 Batch  193/269, Loss: 1261.378\n",
            "Epoch  11 Batch  194/269, Loss: 1243.159\n",
            "Epoch  11 Batch  195/269, Loss: 1268.117\n",
            "Epoch  11 Batch  196/269, Loss: 1270.549\n",
            "Epoch  11 Batch  197/269, Loss: 1248.646\n",
            "Epoch  11 Batch  198/269, Loss: 1263.292\n",
            "Epoch  11 Batch  199/269, Loss: 1275.582\n",
            "Epoch  11 Batch  200/269, Loss: 1241.962\n",
            "Epoch  11 Batch  201/269, Loss: 1280.907\n",
            "Epoch  11 Batch  202/269, Loss: 1275.836\n",
            "Epoch  11 Batch  203/269, Loss: 1263.438\n",
            "Epoch  11 Batch  204/269, Loss: 1253.863\n",
            "Epoch  11 Batch  205/269, Loss: 1276.142\n",
            "Epoch  11 Batch  206/269, Loss: 1247.488\n",
            "Epoch  11 Batch  207/269, Loss: 1274.742\n",
            "Epoch  11 Batch  208/269, Loss: 1264.239\n",
            "Epoch  11 Batch  209/269, Loss: 1227.256\n",
            "Epoch  11 Batch  210/269, Loss: 1276.103\n",
            "Epoch  11 Batch  211/269, Loss: 1262.134\n",
            "Epoch  11 Batch  212/269, Loss: 1267.761\n",
            "Epoch  11 Batch  213/269, Loss: 1262.439\n",
            "Epoch  11 Batch  214/269, Loss: 1247.566\n",
            "Epoch  11 Batch  215/269, Loss: 1258.353\n",
            "Epoch  11 Batch  216/269, Loss: 1250.363\n",
            "Epoch  11 Batch  217/269, Loss: 1198.905\n",
            "Epoch  11 Batch  218/269, Loss: 1223.154\n",
            "Epoch  11 Batch  219/269, Loss: 1185.150\n",
            "Epoch  11 Batch  220/269, Loss: 1242.662\n",
            "Epoch  11 Batch  221/269, Loss: 1189.647\n",
            "Epoch  11 Batch  222/269, Loss: 1193.793\n",
            "Epoch  11 Batch  223/269, Loss: 1157.433\n",
            "Epoch  11 Batch  224/269, Loss: 1193.098\n",
            "Epoch  11 Batch  225/269, Loss: 1145.414\n",
            "Epoch  11 Batch  226/269, Loss: 1155.705\n",
            "Epoch  11 Batch  227/269, Loss: 1211.781\n",
            "Epoch  11 Batch  228/269, Loss: 1151.716\n",
            "Epoch  11 Batch  229/269, Loss: 1184.616\n",
            "Epoch  11 Batch  230/269, Loss: 1136.629\n",
            "Epoch  11 Batch  231/269, Loss: 1131.327\n",
            "Epoch  11 Batch  232/269, Loss: 1101.066\n",
            "Epoch  11 Batch  233/269, Loss: 1163.377\n",
            "Epoch  11 Batch  234/269, Loss: 1115.975\n",
            "Epoch  11 Batch  235/269, Loss: 1165.074\n",
            "Epoch  11 Batch  236/269, Loss: 1144.091\n",
            "Epoch  11 Batch  237/269, Loss: 1170.352\n",
            "Epoch  11 Batch  238/269, Loss: 1161.849\n",
            "Epoch  11 Batch  239/269, Loss: 1131.475\n",
            "Epoch  11 Batch  240/269, Loss: 1181.073\n",
            "Epoch  11 Batch  241/269, Loss: 1150.125\n",
            "Epoch  11 Batch  242/269, Loss: 1122.191\n",
            "Epoch  11 Batch  243/269, Loss: 1164.927\n",
            "Epoch  11 Batch  244/269, Loss: 1143.346\n",
            "Epoch  11 Batch  245/269, Loss: 1138.885\n",
            "Epoch  11 Batch  246/269, Loss: 1141.274\n",
            "Epoch  11 Batch  247/269, Loss: 1119.006\n",
            "Epoch  11 Batch  248/269, Loss: 1130.003\n",
            "Epoch  11 Batch  249/269, Loss: 1143.486\n",
            "Epoch  11 Batch  250/269, Loss: 1137.001\n",
            "Epoch  11 Batch  251/269, Loss: 1135.610\n",
            "Epoch  11 Batch  252/269, Loss: 1111.198\n",
            "Epoch  11 Batch  253/269, Loss: 1126.226\n",
            "Epoch  11 Batch  254/269, Loss: 1134.083\n",
            "Epoch  11 Batch  255/269, Loss: 1139.588\n",
            "Epoch  11 Batch  256/269, Loss: 1110.910\n",
            "Epoch  11 Batch  257/269, Loss: 1126.460\n",
            "Epoch  11 Batch  258/269, Loss: 1130.577\n",
            "Epoch  11 Batch  259/269, Loss: 1120.983\n",
            "Epoch  11 Batch  260/269, Loss: 1087.897\n",
            "Epoch  11 Batch  261/269, Loss: 1065.010\n",
            "Epoch  11 Batch  262/269, Loss: 1094.750\n",
            "Epoch  11 Batch  263/269, Loss: 1072.126\n",
            "Epoch  11 Batch  264/269, Loss: 1070.972\n",
            "Epoch  11 Batch  265/269, Loss: 1051.595\n",
            "Epoch  11 Batch  266/269, Loss: 1043.886\n",
            "Epoch  11 Batch  267/269, Loss: 1072.529\n",
            "Epoch  12 Batch    0/269, Loss: 1031.229\n",
            "Epoch  12 Batch    1/269, Loss: 1050.046\n",
            "Epoch  12 Batch    2/269, Loss: 1052.568\n",
            "Epoch  12 Batch    3/269, Loss: 1019.742\n",
            "Epoch  12 Batch    4/269, Loss: 1011.781\n",
            "Epoch  12 Batch    5/269, Loss: 1015.367\n",
            "Epoch  12 Batch    6/269, Loss: 1016.542\n",
            "Epoch  12 Batch    7/269, Loss: 1007.818\n",
            "Epoch  12 Batch    8/269, Loss: 1007.086\n",
            "Epoch  12 Batch    9/269, Loss: 989.302\n",
            "Epoch  12 Batch   10/269, Loss: 982.634\n",
            "Epoch  12 Batch   11/269, Loss: 1011.009\n",
            "Epoch  12 Batch   12/269, Loss: 1025.900\n",
            "Epoch  12 Batch   13/269, Loss: 1007.227\n",
            "Epoch  12 Batch   14/269, Loss: 1019.517\n",
            "Epoch  12 Batch   15/269, Loss: 1035.887\n",
            "Epoch  12 Batch   16/269, Loss: 1080.251\n",
            "Epoch  12 Batch   17/269, Loss: 1065.141\n",
            "Epoch  12 Batch   18/269, Loss: 1044.099\n",
            "Epoch  12 Batch   19/269, Loss: 1077.635\n",
            "Epoch  12 Batch   20/269, Loss: 1080.140\n",
            "Epoch  12 Batch   21/269, Loss: 1099.765\n",
            "Epoch  12 Batch   22/269, Loss: 1126.257\n",
            "Epoch  12 Batch   23/269, Loss: 1134.679\n",
            "Epoch  12 Batch   24/269, Loss: 1115.848\n",
            "Epoch  12 Batch   25/269, Loss: 1143.326\n",
            "Epoch  12 Batch   26/269, Loss: 1166.792\n",
            "Epoch  12 Batch   27/269, Loss: 1147.104\n",
            "Epoch  12 Batch   28/269, Loss: 1113.795\n",
            "Epoch  12 Batch   29/269, Loss: 1137.165\n",
            "Epoch  12 Batch   30/269, Loss: 1171.342\n",
            "Epoch  12 Batch   31/269, Loss: 1192.801\n",
            "Epoch  12 Batch   32/269, Loss: 1175.378\n",
            "Epoch  12 Batch   33/269, Loss: 1160.408\n",
            "Epoch  12 Batch   34/269, Loss: 1191.071\n",
            "Epoch  12 Batch   35/269, Loss: 1170.921\n",
            "Epoch  12 Batch   36/269, Loss: 1200.077\n",
            "Epoch  12 Batch   37/269, Loss: 1196.000\n",
            "Epoch  12 Batch   38/269, Loss: 1208.915\n",
            "Epoch  12 Batch   39/269, Loss: 1182.976\n",
            "Epoch  12 Batch   40/269, Loss: 1173.496\n",
            "Epoch  12 Batch   41/269, Loss: 1203.134\n",
            "Epoch  12 Batch   42/269, Loss: 1227.117\n",
            "Epoch  12 Batch   43/269, Loss: 1211.655\n",
            "Epoch  12 Batch   44/269, Loss: 1246.973\n",
            "Epoch  12 Batch   45/269, Loss: 1226.456\n",
            "Epoch  12 Batch   46/269, Loss: 1200.534\n",
            "Epoch  12 Batch   47/269, Loss: 1252.544\n",
            "Epoch  12 Batch   48/269, Loss: 1214.485\n",
            "Epoch  12 Batch   49/269, Loss: 1229.232\n",
            "Epoch  12 Batch   50/269, Loss: 1234.719\n",
            "Epoch  12 Batch   51/269, Loss: 1222.063\n",
            "Epoch  12 Batch   52/269, Loss: 1222.032\n",
            "Epoch  12 Batch   53/269, Loss: 1223.048\n",
            "Epoch  12 Batch   54/269, Loss: 1213.751\n",
            "Epoch  12 Batch   55/269, Loss: 1241.827\n",
            "Epoch  12 Batch   56/269, Loss: 1235.345\n",
            "Epoch  12 Batch   57/269, Loss: 1232.365\n",
            "Epoch  12 Batch   58/269, Loss: 1208.359\n",
            "Epoch  12 Batch   59/269, Loss: 1212.407\n",
            "Epoch  12 Batch   60/269, Loss: 1171.737\n",
            "Epoch  12 Batch   61/269, Loss: 1205.035\n",
            "Epoch  12 Batch   62/269, Loss: 1197.474\n",
            "Epoch  12 Batch   63/269, Loss: 1161.894\n",
            "Epoch  12 Batch   64/269, Loss: 1134.610\n",
            "Epoch  12 Batch   65/269, Loss: 1151.923\n",
            "Epoch  12 Batch   66/269, Loss: 1154.293\n",
            "Epoch  12 Batch   67/269, Loss: 1109.265\n",
            "Epoch  12 Batch   68/269, Loss: 1143.648\n",
            "Epoch  12 Batch   69/269, Loss: 1101.541\n",
            "Epoch  12 Batch   70/269, Loss: 1146.464\n",
            "Epoch  12 Batch   71/269, Loss: 1067.646\n",
            "Epoch  12 Batch   72/269, Loss: 1127.903\n",
            "Epoch  12 Batch   73/269, Loss: 1090.241\n",
            "Epoch  12 Batch   74/269, Loss: 1046.216\n",
            "Epoch  12 Batch   75/269, Loss: 1128.849\n",
            "Epoch  12 Batch   76/269, Loss: 1060.067\n",
            "Epoch  12 Batch   77/269, Loss: 1125.677\n",
            "Epoch  12 Batch   78/269, Loss: 1073.671\n",
            "Epoch  12 Batch   79/269, Loss: 1050.601\n",
            "Epoch  12 Batch   80/269, Loss: 1068.778\n",
            "Epoch  12 Batch   81/269, Loss: 1055.275\n",
            "Epoch  12 Batch   82/269, Loss: 1058.284\n",
            "Epoch  12 Batch   83/269, Loss: 1036.209\n",
            "Epoch  12 Batch   84/269, Loss: 1045.082\n",
            "Epoch  12 Batch   85/269, Loss: 1020.339\n",
            "Epoch  12 Batch   86/269, Loss: 1027.226\n",
            "Epoch  12 Batch   87/269, Loss: 975.778\n",
            "Epoch  12 Batch   88/269, Loss: 1045.768\n",
            "Epoch  12 Batch   89/269, Loss: 1036.906\n",
            "Epoch  12 Batch   90/269, Loss: 972.529\n",
            "Epoch  12 Batch   91/269, Loss: 1040.186\n",
            "Epoch  12 Batch   92/269, Loss: 1031.102\n",
            "Epoch  12 Batch   93/269, Loss: 1057.815\n",
            "Epoch  12 Batch   94/269, Loss: 1053.680\n",
            "Epoch  12 Batch   95/269, Loss: 1012.848\n",
            "Epoch  12 Batch   96/269, Loss: 1021.212\n",
            "Epoch  12 Batch   97/269, Loss: 1023.011\n",
            "Epoch  12 Batch   98/269, Loss: 1019.416\n",
            "Epoch  12 Batch   99/269, Loss: 995.148\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch  12 Batch  100/269, Loss: 1030.605\n",
            "Epoch  12 Batch  101/269, Loss: 969.336\n",
            "Epoch  12 Batch  102/269, Loss: 996.439\n",
            "Epoch  12 Batch  103/269, Loss: 1043.563\n",
            "Epoch  12 Batch  104/269, Loss: 993.331\n",
            "Epoch  12 Batch  105/269, Loss: 1003.047\n",
            "Epoch  12 Batch  106/269, Loss: 968.456\n",
            "Epoch  12 Batch  107/269, Loss: 949.546\n",
            "Epoch  12 Batch  108/269, Loss: 972.987\n",
            "Epoch  12 Batch  109/269, Loss: 983.565\n",
            "Epoch  12 Batch  110/269, Loss: 968.761\n",
            "Epoch  12 Batch  111/269, Loss: 962.189\n",
            "Epoch  12 Batch  112/269, Loss: 968.224\n",
            "Epoch  12 Batch  113/269, Loss: 986.787\n",
            "Epoch  12 Batch  114/269, Loss: 988.540\n",
            "Epoch  12 Batch  115/269, Loss: 910.351\n",
            "Epoch  12 Batch  116/269, Loss: 963.504\n",
            "Epoch  12 Batch  117/269, Loss: 950.375\n",
            "Epoch  12 Batch  118/269, Loss: 947.700\n",
            "Epoch  12 Batch  119/269, Loss: 940.202\n",
            "Epoch  12 Batch  120/269, Loss: 919.010\n",
            "Epoch  12 Batch  121/269, Loss: 942.477\n",
            "Epoch  12 Batch  122/269, Loss: 958.728\n",
            "Epoch  12 Batch  123/269, Loss: 925.083\n",
            "Epoch  12 Batch  124/269, Loss: 919.444\n",
            "Epoch  12 Batch  125/269, Loss: 931.489\n",
            "Epoch  12 Batch  126/269, Loss: 938.495\n",
            "Epoch  12 Batch  127/269, Loss: 927.436\n",
            "Epoch  12 Batch  128/269, Loss: 941.220\n",
            "Epoch  12 Batch  129/269, Loss: 938.223\n",
            "Epoch  12 Batch  130/269, Loss: 885.583\n",
            "Epoch  12 Batch  131/269, Loss: 906.315\n",
            "Epoch  12 Batch  132/269, Loss: 913.941\n",
            "Epoch  12 Batch  133/269, Loss: 918.823\n",
            "Epoch  12 Batch  134/269, Loss: 902.708\n",
            "Epoch  12 Batch  135/269, Loss: 874.200\n",
            "Epoch  12 Batch  136/269, Loss: 888.132\n",
            "Epoch  12 Batch  137/269, Loss: 899.036\n",
            "Epoch  12 Batch  138/269, Loss: 898.149\n",
            "Epoch  12 Batch  139/269, Loss: 913.728\n",
            "Epoch  12 Batch  140/269, Loss: 886.702\n",
            "Epoch  12 Batch  141/269, Loss: 872.700\n",
            "Epoch  12 Batch  142/269, Loss: 893.749\n",
            "Epoch  12 Batch  143/269, Loss: 893.746\n",
            "Epoch  12 Batch  144/269, Loss: 898.931\n",
            "Epoch  12 Batch  145/269, Loss: 882.367\n",
            "Epoch  12 Batch  146/269, Loss: 904.732\n",
            "Epoch  12 Batch  147/269, Loss: 935.843\n",
            "Epoch  12 Batch  148/269, Loss: 907.982\n",
            "Epoch  12 Batch  149/269, Loss: 913.268\n",
            "Epoch  12 Batch  150/269, Loss: 918.562\n",
            "Epoch  12 Batch  151/269, Loss: 932.648\n",
            "Epoch  12 Batch  152/269, Loss: 891.578\n",
            "Epoch  12 Batch  153/269, Loss: 894.788\n",
            "Epoch  12 Batch  154/269, Loss: 849.862\n",
            "Epoch  12 Batch  155/269, Loss: 914.538\n",
            "Epoch  12 Batch  156/269, Loss: 874.193\n",
            "Epoch  12 Batch  157/269, Loss: 887.873\n",
            "Epoch  12 Batch  158/269, Loss: 887.399\n",
            "Epoch  12 Batch  159/269, Loss: 903.296\n",
            "Epoch  12 Batch  160/269, Loss: 895.713\n",
            "Epoch  12 Batch  161/269, Loss: 870.392\n",
            "Epoch  12 Batch  162/269, Loss: 879.345\n",
            "Epoch  12 Batch  163/269, Loss: 868.375\n",
            "Epoch  12 Batch  164/269, Loss: 882.547\n",
            "Epoch  12 Batch  165/269, Loss: 862.245\n",
            "Epoch  12 Batch  166/269, Loss: 898.726\n",
            "Epoch  12 Batch  167/269, Loss: 871.073\n",
            "Epoch  12 Batch  168/269, Loss: 873.926\n",
            "Epoch  12 Batch  169/269, Loss: 852.604\n",
            "Epoch  12 Batch  170/269, Loss: 866.392\n",
            "Epoch  12 Batch  171/269, Loss: 859.198\n",
            "Epoch  12 Batch  172/269, Loss: 865.216\n",
            "Epoch  12 Batch  173/269, Loss: 882.371\n",
            "Epoch  12 Batch  174/269, Loss: 877.393\n",
            "Epoch  12 Batch  175/269, Loss: 863.428\n",
            "Epoch  12 Batch  176/269, Loss: 866.603\n",
            "Epoch  12 Batch  177/269, Loss: 890.483\n",
            "Epoch  12 Batch  178/269, Loss: 858.516\n",
            "Epoch  12 Batch  179/269, Loss: 877.128\n",
            "Epoch  12 Batch  180/269, Loss: 884.969\n",
            "Epoch  12 Batch  181/269, Loss: 910.850\n",
            "Epoch  12 Batch  182/269, Loss: 908.829\n",
            "Epoch  12 Batch  183/269, Loss: 961.046\n",
            "Epoch  12 Batch  184/269, Loss: 886.670\n",
            "Epoch  12 Batch  185/269, Loss: 928.877\n",
            "Epoch  12 Batch  186/269, Loss: 889.766\n",
            "Epoch  12 Batch  187/269, Loss: 931.288\n",
            "Epoch  12 Batch  188/269, Loss: 939.796\n",
            "Epoch  12 Batch  189/269, Loss: 952.113\n",
            "Epoch  12 Batch  190/269, Loss: 917.820\n",
            "Epoch  12 Batch  191/269, Loss: 905.702\n",
            "Epoch  12 Batch  192/269, Loss: 925.752\n",
            "Epoch  12 Batch  193/269, Loss: 938.514\n",
            "Epoch  12 Batch  194/269, Loss: 912.598\n",
            "Epoch  12 Batch  195/269, Loss: 921.310\n",
            "Epoch  12 Batch  196/269, Loss: 940.625\n",
            "Epoch  12 Batch  197/269, Loss: 919.895\n",
            "Epoch  12 Batch  198/269, Loss: 911.042\n",
            "Epoch  12 Batch  199/269, Loss: 949.138\n",
            "Epoch  12 Batch  200/269, Loss: 916.798\n",
            "Epoch  12 Batch  201/269, Loss: 961.386\n",
            "Epoch  12 Batch  202/269, Loss: 949.297\n",
            "Epoch  12 Batch  203/269, Loss: 943.492\n",
            "Epoch  12 Batch  204/269, Loss: 929.102\n",
            "Epoch  12 Batch  205/269, Loss: 964.721\n",
            "Epoch  12 Batch  206/269, Loss: 937.156\n",
            "Epoch  12 Batch  207/269, Loss: 987.629\n",
            "Epoch  12 Batch  208/269, Loss: 946.930\n",
            "Epoch  12 Batch  209/269, Loss: 932.418\n",
            "Epoch  12 Batch  210/269, Loss: 980.338\n",
            "Epoch  12 Batch  211/269, Loss: 959.923\n",
            "Epoch  12 Batch  212/269, Loss: 984.163\n",
            "Epoch  12 Batch  213/269, Loss: 972.298\n",
            "Epoch  12 Batch  214/269, Loss: 958.533\n",
            "Epoch  12 Batch  215/269, Loss: 1001.236\n",
            "Epoch  12 Batch  216/269, Loss: 918.491\n",
            "Epoch  12 Batch  217/269, Loss: 905.216\n",
            "Epoch  12 Batch  218/269, Loss: 923.090\n",
            "Epoch  12 Batch  219/269, Loss: 920.664\n",
            "Epoch  12 Batch  220/269, Loss: 946.477\n",
            "Epoch  12 Batch  221/269, Loss: 940.475\n",
            "Epoch  12 Batch  222/269, Loss: 937.778\n",
            "Epoch  12 Batch  223/269, Loss: 924.045\n",
            "Epoch  12 Batch  224/269, Loss: 948.877\n",
            "Epoch  12 Batch  225/269, Loss: 920.999\n",
            "Epoch  12 Batch  226/269, Loss: 960.840\n",
            "Epoch  12 Batch  227/269, Loss: 1039.302\n",
            "Epoch  12 Batch  228/269, Loss: 935.863\n",
            "Epoch  12 Batch  229/269, Loss: 966.253\n",
            "Epoch  12 Batch  230/269, Loss: 926.346\n",
            "Epoch  12 Batch  231/269, Loss: 910.848\n",
            "Epoch  12 Batch  232/269, Loss: 888.427\n",
            "Epoch  12 Batch  233/269, Loss: 921.089\n",
            "Epoch  12 Batch  234/269, Loss: 916.406\n",
            "Epoch  12 Batch  235/269, Loss: 963.439\n",
            "Epoch  12 Batch  236/269, Loss: 950.472\n",
            "Epoch  12 Batch  237/269, Loss: 948.583\n",
            "Epoch  12 Batch  238/269, Loss: 987.874\n",
            "Epoch  12 Batch  239/269, Loss: 955.516\n",
            "Epoch  12 Batch  240/269, Loss: 1020.015\n",
            "Epoch  12 Batch  241/269, Loss: 999.245\n",
            "Epoch  12 Batch  242/269, Loss: 969.222\n",
            "Epoch  12 Batch  243/269, Loss: 955.709\n",
            "Epoch  12 Batch  244/269, Loss: 983.796\n",
            "Epoch  12 Batch  245/269, Loss: 935.379\n",
            "Epoch  12 Batch  246/269, Loss: 998.820\n",
            "Epoch  12 Batch  247/269, Loss: 944.038\n",
            "Epoch  12 Batch  248/269, Loss: 994.254\n",
            "Epoch  12 Batch  249/269, Loss: 1007.258\n",
            "Epoch  12 Batch  250/269, Loss: 974.511\n",
            "Epoch  12 Batch  251/269, Loss: 992.017\n",
            "Epoch  12 Batch  252/269, Loss: 959.349\n",
            "Epoch  12 Batch  253/269, Loss: 942.451\n",
            "Epoch  12 Batch  254/269, Loss: 990.108\n",
            "Epoch  12 Batch  255/269, Loss: 1013.669\n",
            "Epoch  12 Batch  256/269, Loss: 931.907\n",
            "Epoch  12 Batch  257/269, Loss: 964.559\n",
            "Epoch  12 Batch  258/269, Loss: 952.015\n",
            "Epoch  12 Batch  259/269, Loss: 970.810\n",
            "Epoch  12 Batch  260/269, Loss: 950.884\n",
            "Epoch  12 Batch  261/269, Loss: 932.207\n",
            "Epoch  12 Batch  262/269, Loss: 953.887\n",
            "Epoch  12 Batch  263/269, Loss: 970.281\n",
            "Epoch  12 Batch  264/269, Loss: 953.139\n",
            "Epoch  12 Batch  265/269, Loss: 929.866\n",
            "Epoch  12 Batch  266/269, Loss: 957.786\n",
            "Epoch  12 Batch  267/269, Loss: 951.327\n",
            "Epoch  13 Batch    0/269, Loss: 926.348\n",
            "Epoch  13 Batch    1/269, Loss: 922.223\n",
            "Epoch  13 Batch    2/269, Loss: 939.636\n",
            "Epoch  13 Batch    3/269, Loss: 942.193\n",
            "Epoch  13 Batch    4/269, Loss: 907.420\n",
            "Epoch  13 Batch    5/269, Loss: 922.108\n",
            "Epoch  13 Batch    6/269, Loss: 944.434\n",
            "Epoch  13 Batch    7/269, Loss: 932.947\n",
            "Epoch  13 Batch    8/269, Loss: 921.279\n",
            "Epoch  13 Batch    9/269, Loss: 927.912\n",
            "Epoch  13 Batch   10/269, Loss: 906.507\n",
            "Epoch  13 Batch   11/269, Loss: 948.098\n",
            "Epoch  13 Batch   12/269, Loss: 923.158\n",
            "Epoch  13 Batch   13/269, Loss: 970.031\n",
            "Epoch  13 Batch   14/269, Loss: 929.998\n",
            "Epoch  13 Batch   15/269, Loss: 929.451\n",
            "Epoch  13 Batch   16/269, Loss: 956.069\n",
            "Epoch  13 Batch   17/269, Loss: 945.049\n",
            "Epoch  13 Batch   18/269, Loss: 913.573\n",
            "Epoch  13 Batch   19/269, Loss: 961.848\n",
            "Epoch  13 Batch   20/269, Loss: 914.552\n",
            "Epoch  13 Batch   21/269, Loss: 924.611\n",
            "Epoch  13 Batch   22/269, Loss: 981.060\n",
            "Epoch  13 Batch   23/269, Loss: 1008.372\n",
            "Epoch  13 Batch   24/269, Loss: 932.764\n",
            "Epoch  13 Batch   25/269, Loss: 947.715\n",
            "Epoch  13 Batch   26/269, Loss: 1005.911\n",
            "Epoch  13 Batch   27/269, Loss: 978.248\n",
            "Epoch  13 Batch   28/269, Loss: 917.261\n",
            "Epoch  13 Batch   29/269, Loss: 950.522\n",
            "Epoch  13 Batch   30/269, Loss: 959.867\n",
            "Epoch  13 Batch   31/269, Loss: 983.569\n",
            "Epoch  13 Batch   32/269, Loss: 963.812\n",
            "Epoch  13 Batch   33/269, Loss: 972.623\n",
            "Epoch  13 Batch   34/269, Loss: 960.642\n",
            "Epoch  13 Batch   35/269, Loss: 966.498\n",
            "Epoch  13 Batch   36/269, Loss: 970.505\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch  13 Batch   37/269, Loss: 961.942\n",
            "Epoch  13 Batch   38/269, Loss: 966.640\n",
            "Epoch  13 Batch   39/269, Loss: 959.026\n",
            "Epoch  13 Batch   40/269, Loss: 954.693\n",
            "Epoch  13 Batch   41/269, Loss: 959.689\n",
            "Epoch  13 Batch   42/269, Loss: 995.403\n",
            "Epoch  13 Batch   43/269, Loss: 959.788\n",
            "Epoch  13 Batch   44/269, Loss: 996.891\n",
            "Epoch  13 Batch   45/269, Loss: 953.206\n",
            "Epoch  13 Batch   46/269, Loss: 916.438\n",
            "Epoch  13 Batch   47/269, Loss: 992.478\n",
            "Epoch  13 Batch   48/269, Loss: 980.343\n",
            "Epoch  13 Batch   49/269, Loss: 944.132\n",
            "Epoch  13 Batch   50/269, Loss: 963.907\n",
            "Epoch  13 Batch   51/269, Loss: 963.928\n",
            "Epoch  13 Batch   52/269, Loss: 953.662\n",
            "Epoch  13 Batch   53/269, Loss: 941.164\n",
            "Epoch  13 Batch   54/269, Loss: 928.471\n",
            "Epoch  13 Batch   55/269, Loss: 954.422\n",
            "Epoch  13 Batch   56/269, Loss: 972.287\n",
            "Epoch  13 Batch   57/269, Loss: 964.398\n",
            "Epoch  13 Batch   58/269, Loss: 940.536\n",
            "Epoch  13 Batch   59/269, Loss: 936.702\n",
            "Epoch  13 Batch   60/269, Loss: 911.098\n",
            "Epoch  13 Batch   61/269, Loss: 932.342\n",
            "Epoch  13 Batch   62/269, Loss: 910.436\n",
            "Epoch  13 Batch   63/269, Loss: 879.758\n",
            "Epoch  13 Batch   64/269, Loss: 859.556\n",
            "Epoch  13 Batch   65/269, Loss: 852.129\n",
            "Epoch  13 Batch   66/269, Loss: 869.416\n",
            "Epoch  13 Batch   67/269, Loss: 878.052\n",
            "Epoch  13 Batch   68/269, Loss: 897.263\n",
            "Epoch  13 Batch   69/269, Loss: 859.725\n",
            "Epoch  13 Batch   70/269, Loss: 886.471\n",
            "Epoch  13 Batch   71/269, Loss: 849.835\n",
            "Epoch  13 Batch   72/269, Loss: 908.751\n",
            "Epoch  13 Batch   73/269, Loss: 899.484\n",
            "Epoch  13 Batch   74/269, Loss: 874.890\n",
            "Epoch  13 Batch   75/269, Loss: 880.769\n",
            "Epoch  13 Batch   76/269, Loss: 860.122\n",
            "Epoch  13 Batch   77/269, Loss: 879.264\n",
            "Epoch  13 Batch   78/269, Loss: 868.589\n",
            "Epoch  13 Batch   79/269, Loss: 852.804\n",
            "Epoch  13 Batch   80/269, Loss: 871.446\n",
            "Epoch  13 Batch   81/269, Loss: 893.429\n",
            "Epoch  13 Batch   82/269, Loss: 867.746\n",
            "Epoch  13 Batch   83/269, Loss: 884.989\n",
            "Epoch  13 Batch   84/269, Loss: 863.742\n",
            "Epoch  13 Batch   85/269, Loss: 843.607\n",
            "Epoch  13 Batch   86/269, Loss: 859.172\n",
            "Epoch  13 Batch   87/269, Loss: 843.971\n",
            "Epoch  13 Batch   88/269, Loss: 866.704\n",
            "Epoch  13 Batch   89/269, Loss: 863.602\n",
            "Epoch  13 Batch   90/269, Loss: 838.920\n",
            "Epoch  13 Batch   91/269, Loss: 875.044\n",
            "Epoch  13 Batch   92/269, Loss: 880.272\n",
            "Epoch  13 Batch   93/269, Loss: 876.395\n",
            "Epoch  13 Batch   94/269, Loss: 874.940\n",
            "Epoch  13 Batch   95/269, Loss: 882.194\n",
            "Epoch  13 Batch   96/269, Loss: 893.554\n",
            "Epoch  13 Batch   97/269, Loss: 894.390\n",
            "Epoch  13 Batch   98/269, Loss: 900.359\n",
            "Epoch  13 Batch   99/269, Loss: 900.409\n",
            "Epoch  13 Batch  100/269, Loss: 943.357\n",
            "Epoch  13 Batch  101/269, Loss: 930.857\n",
            "Epoch  13 Batch  102/269, Loss: 975.199\n",
            "Epoch  13 Batch  103/269, Loss: 1003.955\n",
            "Epoch  13 Batch  104/269, Loss: 994.974\n",
            "Epoch  13 Batch  105/269, Loss: 1036.260\n",
            "Epoch  13 Batch  106/269, Loss: 1007.530\n",
            "Epoch  13 Batch  107/269, Loss: 1000.338\n",
            "Epoch  13 Batch  108/269, Loss: 997.450\n",
            "Epoch  13 Batch  109/269, Loss: 1037.314\n",
            "Epoch  13 Batch  110/269, Loss: 1034.187\n",
            "Epoch  13 Batch  111/269, Loss: 1048.825\n",
            "Epoch  13 Batch  112/269, Loss: 1057.955\n",
            "Epoch  13 Batch  113/269, Loss: 1093.670\n",
            "Epoch  13 Batch  114/269, Loss: 1092.732\n",
            "Epoch  13 Batch  115/269, Loss: 1071.815\n",
            "Epoch  13 Batch  116/269, Loss: 1129.357\n",
            "Epoch  13 Batch  117/269, Loss: 1139.460\n",
            "Epoch  13 Batch  118/269, Loss: 1167.596\n",
            "Epoch  13 Batch  119/269, Loss: 1233.179\n",
            "Epoch  13 Batch  120/269, Loss: 1208.921\n",
            "Epoch  13 Batch  121/269, Loss: 1216.146\n",
            "Epoch  13 Batch  122/269, Loss: 1248.402\n",
            "Epoch  13 Batch  123/269, Loss: 1227.275\n",
            "Epoch  13 Batch  124/269, Loss: 1255.652\n",
            "Epoch  13 Batch  125/269, Loss: 1273.450\n",
            "Epoch  13 Batch  126/269, Loss: 1302.603\n",
            "Epoch  13 Batch  127/269, Loss: 1347.954\n",
            "Epoch  13 Batch  128/269, Loss: 1381.412\n",
            "Epoch  13 Batch  129/269, Loss: 1411.913\n",
            "Epoch  13 Batch  130/269, Loss: 1363.538\n",
            "Epoch  13 Batch  131/269, Loss: 1410.761\n",
            "Epoch  13 Batch  132/269, Loss: 1425.082\n",
            "Epoch  13 Batch  133/269, Loss: 1485.051\n",
            "Epoch  13 Batch  134/269, Loss: 1557.316\n",
            "Epoch  13 Batch  135/269, Loss: 1551.858\n",
            "Epoch  13 Batch  136/269, Loss: 1635.376\n",
            "Epoch  13 Batch  137/269, Loss: 1693.218\n",
            "Epoch  13 Batch  138/269, Loss: 1616.654\n",
            "Epoch  13 Batch  139/269, Loss: 1591.224\n",
            "Epoch  13 Batch  140/269, Loss: 1481.989\n",
            "Epoch  13 Batch  141/269, Loss: 1662.410\n",
            "Epoch  13 Batch  142/269, Loss: 1774.644\n",
            "Epoch  13 Batch  143/269, Loss: 1761.839\n",
            "Epoch  13 Batch  144/269, Loss: 1688.142\n",
            "Epoch  13 Batch  145/269, Loss: 1642.170\n",
            "Epoch  13 Batch  146/269, Loss: 1590.079\n",
            "Epoch  13 Batch  147/269, Loss: 1451.052\n",
            "Epoch  13 Batch  148/269, Loss: 1325.365\n",
            "Epoch  13 Batch  149/269, Loss: 1176.663\n",
            "Epoch  13 Batch  150/269, Loss: 1084.907\n",
            "Epoch  13 Batch  151/269, Loss: 1029.846\n",
            "Epoch  13 Batch  152/269, Loss: 960.773\n",
            "Epoch  13 Batch  153/269, Loss: 937.449\n",
            "Epoch  13 Batch  154/269, Loss: 909.208\n",
            "Epoch  13 Batch  155/269, Loss: 912.947\n",
            "Epoch  13 Batch  156/269, Loss: 921.927\n",
            "Epoch  13 Batch  157/269, Loss: 925.509\n",
            "Epoch  13 Batch  158/269, Loss: 857.562\n",
            "Epoch  13 Batch  159/269, Loss: 841.263\n",
            "Epoch  13 Batch  160/269, Loss: 793.866\n",
            "Epoch  13 Batch  161/269, Loss: 726.213\n",
            "Epoch  13 Batch  162/269, Loss: 660.905\n",
            "Epoch  13 Batch  163/269, Loss: 621.700\n",
            "Epoch  13 Batch  164/269, Loss: 603.374\n",
            "Epoch  13 Batch  165/269, Loss: 535.041\n",
            "Epoch  13 Batch  166/269, Loss: 504.888\n",
            "Epoch  13 Batch  167/269, Loss: 459.036\n",
            "Epoch  13 Batch  168/269, Loss: 454.867\n",
            "Epoch  13 Batch  169/269, Loss: 381.093\n",
            "Epoch  13 Batch  170/269, Loss: 356.784\n",
            "Epoch  13 Batch  171/269, Loss: 349.562\n",
            "Epoch  13 Batch  172/269, Loss: 360.602\n",
            "Epoch  13 Batch  173/269, Loss: 400.972\n",
            "Epoch  13 Batch  174/269, Loss: 462.161\n",
            "Epoch  13 Batch  175/269, Loss: 524.643\n",
            "Epoch  13 Batch  176/269, Loss: 589.383\n",
            "Epoch  13 Batch  177/269, Loss: 649.255\n",
            "Epoch  13 Batch  178/269, Loss: 678.285\n",
            "Epoch  13 Batch  179/269, Loss: 741.457\n",
            "Epoch  13 Batch  180/269, Loss: 774.755\n",
            "Epoch  13 Batch  181/269, Loss: 820.516\n",
            "Epoch  13 Batch  182/269, Loss: 873.125\n",
            "Epoch  13 Batch  183/269, Loss: 925.990\n",
            "Epoch  13 Batch  184/269, Loss: 908.225\n",
            "Epoch  13 Batch  185/269, Loss: 947.961\n",
            "Epoch  13 Batch  186/269, Loss: 951.576\n",
            "Epoch  13 Batch  187/269, Loss: 978.286\n",
            "Epoch  13 Batch  188/269, Loss: 954.460\n",
            "Epoch  13 Batch  189/269, Loss: 951.606\n",
            "Epoch  13 Batch  190/269, Loss: 951.785\n",
            "Epoch  13 Batch  191/269, Loss: 957.450\n",
            "Epoch  13 Batch  192/269, Loss: 962.488\n",
            "Epoch  13 Batch  193/269, Loss: 920.317\n",
            "Epoch  13 Batch  194/269, Loss: 907.177\n",
            "Epoch  13 Batch  195/269, Loss: 865.361\n",
            "Epoch  13 Batch  196/269, Loss: 868.504\n",
            "Epoch  13 Batch  197/269, Loss: 843.972\n",
            "Epoch  13 Batch  198/269, Loss: 839.789\n",
            "Epoch  13 Batch  199/269, Loss: 774.299\n",
            "Epoch  13 Batch  200/269, Loss: 691.811\n",
            "Epoch  13 Batch  201/269, Loss: 683.849\n",
            "Epoch  13 Batch  202/269, Loss: 684.838\n",
            "Epoch  13 Batch  203/269, Loss: 706.085\n",
            "Epoch  13 Batch  204/269, Loss: 733.561\n",
            "Epoch  13 Batch  205/269, Loss: 762.342\n",
            "Epoch  13 Batch  206/269, Loss: 807.065\n",
            "Epoch  13 Batch  207/269, Loss: 812.360\n",
            "Epoch  13 Batch  208/269, Loss: 823.336\n",
            "Epoch  13 Batch  209/269, Loss: 827.182\n",
            "Epoch  13 Batch  210/269, Loss: 859.788\n",
            "Epoch  13 Batch  211/269, Loss: 902.531\n",
            "Epoch  13 Batch  212/269, Loss: 964.486\n",
            "Epoch  13 Batch  213/269, Loss: 1021.620\n",
            "Epoch  13 Batch  214/269, Loss: 1122.551\n",
            "Epoch  13 Batch  215/269, Loss: 1193.222\n",
            "Epoch  13 Batch  216/269, Loss: 1123.966\n",
            "Epoch  13 Batch  217/269, Loss: 1052.900\n",
            "Epoch  13 Batch  218/269, Loss: 1027.201\n",
            "Epoch  13 Batch  219/269, Loss: 997.141\n",
            "Epoch  13 Batch  220/269, Loss: 999.763\n",
            "Epoch  13 Batch  221/269, Loss: 1065.752\n",
            "Epoch  13 Batch  222/269, Loss: 1184.644\n",
            "Epoch  13 Batch  223/269, Loss: 1286.453\n",
            "Epoch  13 Batch  224/269, Loss: 1392.561\n",
            "Epoch  13 Batch  225/269, Loss: 1411.260\n",
            "Epoch  13 Batch  226/269, Loss: 1510.088\n",
            "Epoch  13 Batch  227/269, Loss: 1538.794\n",
            "Epoch  13 Batch  228/269, Loss: 1430.587\n",
            "Epoch  13 Batch  229/269, Loss: 1433.635\n",
            "Epoch  13 Batch  230/269, Loss: 1378.903\n",
            "Epoch  13 Batch  231/269, Loss: 1349.498\n",
            "Epoch  13 Batch  232/269, Loss: 1269.851\n",
            "Epoch  13 Batch  233/269, Loss: 1304.337\n",
            "Epoch  13 Batch  234/269, Loss: 1240.332\n",
            "Epoch  13 Batch  235/269, Loss: 1232.951\n",
            "Epoch  13 Batch  236/269, Loss: 1108.071\n",
            "Epoch  13 Batch  237/269, Loss: 955.549\n",
            "Epoch  13 Batch  238/269, Loss: 885.671\n",
            "Epoch  13 Batch  239/269, Loss: 707.964\n",
            "Epoch  13 Batch  240/269, Loss: 688.952\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch  13 Batch  241/269, Loss: 703.222\n",
            "Epoch  13 Batch  242/269, Loss: 734.936\n",
            "Epoch  13 Batch  243/269, Loss: 731.226\n",
            "Epoch  13 Batch  244/269, Loss: 740.659\n",
            "Epoch  13 Batch  245/269, Loss: 762.747\n",
            "Epoch  13 Batch  246/269, Loss: 754.948\n",
            "Epoch  13 Batch  247/269, Loss: 778.914\n",
            "Epoch  13 Batch  248/269, Loss: 773.415\n",
            "Epoch  13 Batch  249/269, Loss: 807.730\n",
            "Epoch  13 Batch  250/269, Loss: 823.654\n",
            "Epoch  13 Batch  251/269, Loss: 866.850\n",
            "Epoch  13 Batch  252/269, Loss: 901.504\n",
            "Epoch  13 Batch  253/269, Loss: 926.333\n",
            "Epoch  13 Batch  254/269, Loss: 924.281\n",
            "Epoch  13 Batch  255/269, Loss: 998.585\n",
            "Epoch  13 Batch  256/269, Loss: 1021.456\n",
            "Epoch  13 Batch  257/269, Loss: 1053.760\n",
            "Epoch  13 Batch  258/269, Loss: 1097.700\n",
            "Epoch  13 Batch  259/269, Loss: 1153.740\n",
            "Epoch  13 Batch  260/269, Loss: 1141.315\n",
            "Epoch  13 Batch  261/269, Loss: 1164.326\n",
            "Epoch  13 Batch  262/269, Loss: 1178.329\n",
            "Epoch  13 Batch  263/269, Loss: 1229.288\n",
            "Epoch  13 Batch  264/269, Loss: 1237.447\n",
            "Epoch  13 Batch  265/269, Loss: 1264.942\n",
            "Epoch  13 Batch  266/269, Loss: 1316.727\n",
            "Epoch  13 Batch  267/269, Loss: 1334.400\n",
            "Epoch  14 Batch    0/269, Loss: 1395.904\n",
            "Epoch  14 Batch    1/269, Loss: 1398.180\n",
            "Epoch  14 Batch    2/269, Loss: 1424.748\n",
            "Epoch  14 Batch    3/269, Loss: 1505.257\n",
            "Epoch  14 Batch    4/269, Loss: 1487.584\n",
            "Epoch  14 Batch    5/269, Loss: 1497.250\n",
            "Epoch  14 Batch    6/269, Loss: 1464.101\n",
            "Epoch  14 Batch    7/269, Loss: 1463.867\n",
            "Epoch  14 Batch    8/269, Loss: 1487.218\n",
            "Epoch  14 Batch    9/269, Loss: 1482.109\n",
            "Epoch  14 Batch   10/269, Loss: 1509.257\n",
            "Epoch  14 Batch   11/269, Loss: 1479.787\n",
            "Epoch  14 Batch   12/269, Loss: 1487.542\n",
            "Epoch  14 Batch   13/269, Loss: 1476.111\n",
            "Epoch  14 Batch   14/269, Loss: 1551.866\n",
            "Epoch  14 Batch   15/269, Loss: 1546.590\n",
            "Epoch  14 Batch   16/269, Loss: 1589.838\n",
            "Epoch  14 Batch   17/269, Loss: 1589.947\n",
            "Epoch  14 Batch   18/269, Loss: 1635.674\n",
            "Epoch  14 Batch   19/269, Loss: 1666.120\n",
            "Epoch  14 Batch   20/269, Loss: 1699.687\n",
            "Epoch  14 Batch   21/269, Loss: 1687.657\n",
            "Epoch  14 Batch   22/269, Loss: 1702.086\n",
            "Epoch  14 Batch   23/269, Loss: 1733.986\n",
            "Epoch  14 Batch   24/269, Loss: 1712.861\n",
            "Epoch  14 Batch   25/269, Loss: 1727.558\n",
            "Epoch  14 Batch   26/269, Loss: 1719.603\n",
            "Epoch  14 Batch   27/269, Loss: 1739.421\n",
            "Epoch  14 Batch   28/269, Loss: 1723.214\n",
            "Epoch  14 Batch   29/269, Loss: 1719.546\n",
            "Epoch  14 Batch   30/269, Loss: 1754.183\n",
            "Epoch  14 Batch   31/269, Loss: 1780.861\n",
            "Epoch  14 Batch   32/269, Loss: 1754.533\n",
            "Epoch  14 Batch   33/269, Loss: 1761.802\n",
            "Epoch  14 Batch   34/269, Loss: 1759.885\n",
            "Epoch  14 Batch   35/269, Loss: 1801.546\n",
            "Epoch  14 Batch   36/269, Loss: 1738.822\n",
            "Epoch  14 Batch   37/269, Loss: 1740.890\n",
            "Epoch  14 Batch   38/269, Loss: 1775.658\n",
            "Epoch  14 Batch   39/269, Loss: 1734.805\n",
            "Epoch  14 Batch   40/269, Loss: 1757.490\n",
            "Epoch  14 Batch   41/269, Loss: 1749.243\n",
            "Epoch  14 Batch   42/269, Loss: 1751.228\n",
            "Epoch  14 Batch   43/269, Loss: 1748.993\n",
            "Epoch  14 Batch   44/269, Loss: 1720.368\n",
            "Epoch  14 Batch   45/269, Loss: 1672.412\n",
            "Epoch  14 Batch   46/269, Loss: 1680.587\n",
            "Epoch  14 Batch   47/269, Loss: 1704.712\n",
            "Epoch  14 Batch   48/269, Loss: 1710.693\n",
            "Epoch  14 Batch   49/269, Loss: 1692.420\n",
            "Epoch  14 Batch   50/269, Loss: 1728.963\n",
            "Epoch  14 Batch   51/269, Loss: 1754.985\n",
            "Epoch  14 Batch   52/269, Loss: 1766.997\n",
            "Epoch  14 Batch   53/269, Loss: 1843.119\n",
            "Epoch  14 Batch   54/269, Loss: 1922.300\n",
            "Epoch  14 Batch   55/269, Loss: 1987.990\n",
            "Epoch  14 Batch   56/269, Loss: 2151.930\n",
            "Epoch  14 Batch   57/269, Loss: 2233.667\n",
            "Epoch  14 Batch   58/269, Loss: 2330.705\n",
            "Epoch  14 Batch   59/269, Loss: 2410.868\n",
            "Epoch  14 Batch   60/269, Loss: 2474.181\n",
            "Epoch  14 Batch   61/269, Loss: 2633.521\n",
            "Epoch  14 Batch   62/269, Loss: 2734.499\n",
            "Epoch  14 Batch   63/269, Loss: 2691.707\n",
            "Epoch  14 Batch   64/269, Loss: 2483.854\n",
            "Epoch  14 Batch   65/269, Loss: 2385.884\n",
            "Epoch  14 Batch   66/269, Loss: 2292.793\n",
            "Epoch  14 Batch   67/269, Loss: 2145.118\n",
            "Epoch  14 Batch   68/269, Loss: 2053.348\n",
            "Epoch  14 Batch   69/269, Loss: 1941.362\n",
            "Epoch  14 Batch   70/269, Loss: 1965.305\n",
            "Epoch  14 Batch   71/269, Loss: 1964.043\n",
            "Epoch  14 Batch   72/269, Loss: 1982.811\n",
            "Epoch  14 Batch   73/269, Loss: 2037.360\n",
            "Epoch  14 Batch   74/269, Loss: 2019.496\n",
            "Epoch  14 Batch   75/269, Loss: 2049.007\n",
            "Epoch  14 Batch   76/269, Loss: 2037.871\n",
            "Epoch  14 Batch   77/269, Loss: 2085.706\n",
            "Epoch  14 Batch   78/269, Loss: 2164.079\n",
            "Epoch  14 Batch   79/269, Loss: 2153.804\n",
            "Epoch  14 Batch   80/269, Loss: 2174.865\n",
            "Epoch  14 Batch   81/269, Loss: 2222.604\n",
            "Epoch  14 Batch   82/269, Loss: 2303.629\n",
            "Epoch  14 Batch   83/269, Loss: 2211.809\n",
            "Epoch  14 Batch   84/269, Loss: 2223.875\n",
            "Epoch  14 Batch   85/269, Loss: 2289.334\n",
            "Epoch  14 Batch   86/269, Loss: 2294.274\n",
            "Epoch  14 Batch   87/269, Loss: 2138.095\n",
            "Epoch  14 Batch   88/269, Loss: 2294.585\n",
            "Epoch  14 Batch   89/269, Loss: 2251.326\n",
            "Epoch  14 Batch   90/269, Loss: 2192.019\n",
            "Epoch  14 Batch   91/269, Loss: 2254.004\n",
            "Epoch  14 Batch   92/269, Loss: 2206.050\n",
            "Epoch  14 Batch   93/269, Loss: 2213.370\n",
            "Epoch  14 Batch   94/269, Loss: 2162.647\n",
            "Epoch  14 Batch   95/269, Loss: 2161.582\n",
            "Epoch  14 Batch   96/269, Loss: 2142.364\n",
            "Epoch  14 Batch   97/269, Loss: 2132.366\n",
            "Epoch  14 Batch   98/269, Loss: 2084.319\n",
            "Epoch  14 Batch   99/269, Loss: 2035.522\n",
            "Epoch  14 Batch  100/269, Loss: 2059.457\n",
            "Epoch  14 Batch  101/269, Loss: 2049.688\n",
            "Epoch  14 Batch  102/269, Loss: 2045.635\n",
            "Epoch  14 Batch  103/269, Loss: 2092.834\n",
            "Epoch  14 Batch  104/269, Loss: 1998.548\n",
            "Epoch  14 Batch  105/269, Loss: 2008.995\n",
            "Epoch  14 Batch  106/269, Loss: 2031.544\n",
            "Epoch  14 Batch  107/269, Loss: 2022.459\n",
            "Epoch  14 Batch  108/269, Loss: 1963.658\n",
            "Epoch  14 Batch  109/269, Loss: 1960.342\n",
            "Epoch  14 Batch  110/269, Loss: 1977.123\n",
            "Epoch  14 Batch  111/269, Loss: 1938.885\n",
            "Epoch  14 Batch  112/269, Loss: 1925.580\n",
            "Epoch  14 Batch  113/269, Loss: 1943.371\n",
            "Epoch  14 Batch  114/269, Loss: 1946.397\n",
            "Epoch  14 Batch  115/269, Loss: 1886.783\n",
            "Epoch  14 Batch  116/269, Loss: 1945.360\n",
            "Epoch  14 Batch  117/269, Loss: 1847.895\n",
            "Epoch  14 Batch  118/269, Loss: 1910.336\n",
            "Epoch  14 Batch  119/269, Loss: 1792.362\n",
            "Epoch  14 Batch  120/269, Loss: 1761.169\n",
            "Epoch  14 Batch  121/269, Loss: 1677.215\n",
            "Epoch  14 Batch  122/269, Loss: 1724.590\n",
            "Epoch  14 Batch  123/269, Loss: 1580.355\n",
            "Epoch  14 Batch  124/269, Loss: 1562.893\n",
            "Epoch  14 Batch  125/269, Loss: 1546.518\n",
            "Epoch  14 Batch  126/269, Loss: 1549.049\n",
            "Epoch  14 Batch  127/269, Loss: 1528.527\n",
            "Epoch  14 Batch  128/269, Loss: 1527.365\n",
            "Epoch  14 Batch  129/269, Loss: 1562.337\n",
            "Epoch  14 Batch  130/269, Loss: 1484.755\n",
            "Epoch  14 Batch  131/269, Loss: 1502.960\n",
            "Epoch  14 Batch  132/269, Loss: 1450.850\n",
            "Epoch  14 Batch  133/269, Loss: 1471.949\n",
            "Epoch  14 Batch  134/269, Loss: 1460.163\n",
            "Epoch  14 Batch  135/269, Loss: 1429.470\n",
            "Epoch  14 Batch  136/269, Loss: 1420.299\n",
            "Epoch  14 Batch  137/269, Loss: 1436.500\n",
            "Epoch  14 Batch  138/269, Loss: 1444.717\n",
            "Epoch  14 Batch  139/269, Loss: 1450.438\n",
            "Epoch  14 Batch  140/269, Loss: 1462.445\n",
            "Epoch  14 Batch  141/269, Loss: 1434.152\n",
            "Epoch  14 Batch  142/269, Loss: 1417.267\n",
            "Epoch  14 Batch  143/269, Loss: 1375.721\n",
            "Epoch  14 Batch  144/269, Loss: 1364.291\n",
            "Epoch  14 Batch  145/269, Loss: 1369.192\n",
            "Epoch  14 Batch  146/269, Loss: 1374.210\n",
            "Epoch  14 Batch  147/269, Loss: 1346.553\n",
            "Epoch  14 Batch  148/269, Loss: 1323.942\n",
            "Epoch  14 Batch  149/269, Loss: 1376.205\n",
            "Epoch  14 Batch  150/269, Loss: 1374.366\n",
            "Epoch  14 Batch  151/269, Loss: 1363.390\n",
            "Epoch  14 Batch  152/269, Loss: 1368.092\n",
            "Epoch  14 Batch  153/269, Loss: 1355.469\n",
            "Epoch  14 Batch  154/269, Loss: 1336.178\n",
            "Epoch  14 Batch  155/269, Loss: 1330.822\n",
            "Epoch  14 Batch  156/269, Loss: 1402.709\n",
            "Epoch  14 Batch  157/269, Loss: 1402.703\n",
            "Epoch  14 Batch  158/269, Loss: 1448.845\n",
            "Epoch  14 Batch  159/269, Loss: 1475.703\n",
            "Epoch  14 Batch  160/269, Loss: 1527.847\n",
            "Epoch  14 Batch  161/269, Loss: 1493.271\n",
            "Epoch  14 Batch  162/269, Loss: 1528.960\n",
            "Epoch  14 Batch  163/269, Loss: 1528.349\n",
            "Epoch  14 Batch  164/269, Loss: 1589.786\n",
            "Epoch  14 Batch  165/269, Loss: 1548.947\n",
            "Epoch  14 Batch  166/269, Loss: 1603.781\n",
            "Epoch  14 Batch  167/269, Loss: 1486.959\n",
            "Epoch  14 Batch  168/269, Loss: 1414.872\n",
            "Epoch  14 Batch  169/269, Loss: 1356.596\n",
            "Epoch  14 Batch  170/269, Loss: 1323.465\n",
            "Epoch  14 Batch  171/269, Loss: 1247.237\n",
            "Epoch  14 Batch  172/269, Loss: 1143.691\n",
            "Epoch  14 Batch  173/269, Loss: 1075.695\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch  14 Batch  174/269, Loss: 1065.870\n",
            "Epoch  14 Batch  175/269, Loss: 1018.787\n",
            "Epoch  14 Batch  176/269, Loss: 998.337\n",
            "Epoch  14 Batch  177/269, Loss: 988.095\n",
            "Epoch  14 Batch  178/269, Loss: 968.087\n",
            "Epoch  14 Batch  179/269, Loss: 963.828\n",
            "Epoch  14 Batch  180/269, Loss: 932.931\n",
            "Epoch  14 Batch  181/269, Loss: 820.914\n",
            "Epoch  14 Batch  182/269, Loss: 775.392\n",
            "Epoch  14 Batch  183/269, Loss: 742.519\n",
            "Epoch  14 Batch  184/269, Loss: 690.909\n",
            "Epoch  14 Batch  185/269, Loss: 689.692\n",
            "Epoch  14 Batch  186/269, Loss: 669.676\n",
            "Epoch  14 Batch  187/269, Loss: 669.264\n",
            "Epoch  14 Batch  188/269, Loss: 666.372\n",
            "Epoch  14 Batch  189/269, Loss: 697.666\n",
            "Epoch  14 Batch  190/269, Loss: 689.596\n",
            "Epoch  14 Batch  191/269, Loss: 692.080\n",
            "Epoch  14 Batch  192/269, Loss: 727.060\n",
            "Epoch  14 Batch  193/269, Loss: 744.510\n",
            "Epoch  14 Batch  194/269, Loss: 736.799\n",
            "Epoch  14 Batch  195/269, Loss: 758.974\n",
            "Epoch  14 Batch  196/269, Loss: 766.209\n",
            "Epoch  14 Batch  197/269, Loss: 776.778\n",
            "Epoch  14 Batch  198/269, Loss: 793.113\n",
            "Epoch  14 Batch  199/269, Loss: 807.742\n",
            "Epoch  14 Batch  200/269, Loss: 797.979\n",
            "Epoch  14 Batch  201/269, Loss: 834.479\n",
            "Epoch  14 Batch  202/269, Loss: 824.832\n",
            "Epoch  14 Batch  203/269, Loss: 812.435\n",
            "Epoch  14 Batch  204/269, Loss: 816.395\n",
            "Epoch  14 Batch  205/269, Loss: 837.804\n",
            "Epoch  14 Batch  206/269, Loss: 848.931\n",
            "Epoch  14 Batch  207/269, Loss: 847.636\n",
            "Epoch  14 Batch  208/269, Loss: 847.485\n",
            "Epoch  14 Batch  209/269, Loss: 866.470\n",
            "Epoch  14 Batch  210/269, Loss: 846.615\n",
            "Epoch  14 Batch  211/269, Loss: 875.257\n",
            "Epoch  14 Batch  212/269, Loss: 854.302\n",
            "Epoch  14 Batch  213/269, Loss: 876.125\n",
            "Epoch  14 Batch  214/269, Loss: 877.951\n",
            "Epoch  14 Batch  215/269, Loss: 865.623\n",
            "Epoch  14 Batch  216/269, Loss: 875.347\n",
            "Epoch  14 Batch  217/269, Loss: 867.620\n",
            "Epoch  14 Batch  218/269, Loss: 847.365\n",
            "Epoch  14 Batch  219/269, Loss: 889.667\n",
            "Epoch  14 Batch  220/269, Loss: 903.540\n",
            "Epoch  14 Batch  221/269, Loss: 890.745\n",
            "Epoch  14 Batch  222/269, Loss: 888.622\n",
            "Epoch  14 Batch  223/269, Loss: 891.097\n",
            "Epoch  14 Batch  224/269, Loss: 900.347\n",
            "Epoch  14 Batch  225/269, Loss: 889.776\n",
            "Epoch  14 Batch  226/269, Loss: 901.652\n",
            "Epoch  14 Batch  227/269, Loss: 901.826\n",
            "Epoch  14 Batch  228/269, Loss: 893.965\n",
            "Epoch  14 Batch  229/269, Loss: 908.037\n",
            "Epoch  14 Batch  230/269, Loss: 902.971\n",
            "Epoch  14 Batch  231/269, Loss: 885.373\n",
            "Epoch  14 Batch  232/269, Loss: 886.378\n",
            "Epoch  14 Batch  233/269, Loss: 899.770\n",
            "Epoch  14 Batch  234/269, Loss: 905.520\n",
            "Epoch  14 Batch  235/269, Loss: 920.791\n",
            "Epoch  14 Batch  236/269, Loss: 913.986\n",
            "Epoch  14 Batch  237/269, Loss: 932.804\n",
            "Epoch  14 Batch  238/269, Loss: 955.740\n",
            "Epoch  14 Batch  239/269, Loss: 985.297\n",
            "Epoch  14 Batch  240/269, Loss: 1000.766\n",
            "Epoch  14 Batch  241/269, Loss: 957.295\n",
            "Epoch  14 Batch  242/269, Loss: 997.297\n",
            "Epoch  14 Batch  243/269, Loss: 977.000\n",
            "Epoch  14 Batch  244/269, Loss: 950.604\n",
            "Epoch  14 Batch  245/269, Loss: 845.652\n",
            "Epoch  14 Batch  246/269, Loss: 749.433\n",
            "Epoch  14 Batch  247/269, Loss: 720.734\n",
            "Epoch  14 Batch  248/269, Loss: 698.601\n",
            "Epoch  14 Batch  249/269, Loss: 625.911\n",
            "Epoch  14 Batch  250/269, Loss: 567.995\n",
            "Epoch  14 Batch  251/269, Loss: 501.693\n",
            "Epoch  14 Batch  252/269, Loss: 562.702\n",
            "Epoch  14 Batch  253/269, Loss: 585.364\n",
            "Epoch  14 Batch  254/269, Loss: 643.461\n",
            "Epoch  14 Batch  255/269, Loss: 694.659\n",
            "Epoch  14 Batch  256/269, Loss: 753.860\n",
            "Epoch  14 Batch  257/269, Loss: 693.423\n",
            "Epoch  14 Batch  258/269, Loss: 825.358\n",
            "Epoch  14 Batch  259/269, Loss: 763.178\n",
            "Epoch  14 Batch  260/269, Loss: 869.094\n",
            "Epoch  14 Batch  261/269, Loss: 974.231\n",
            "Epoch  14 Batch  262/269, Loss: 1058.130\n",
            "Epoch  14 Batch  263/269, Loss: 1131.738\n",
            "Epoch  14 Batch  264/269, Loss: 1225.556\n",
            "Epoch  14 Batch  265/269, Loss: 1302.741\n",
            "Epoch  14 Batch  266/269, Loss: 1381.922\n",
            "Epoch  14 Batch  267/269, Loss: 1435.051\n",
            "Epoch  15 Batch    0/269, Loss: 1554.131\n",
            "Epoch  15 Batch    1/269, Loss: 1605.962\n",
            "Epoch  15 Batch    2/269, Loss: 1656.270\n",
            "Epoch  15 Batch    3/269, Loss: 1724.010\n",
            "Epoch  15 Batch    4/269, Loss: 1768.356\n",
            "Epoch  15 Batch    5/269, Loss: 1777.289\n",
            "Epoch  15 Batch    6/269, Loss: 1759.934\n",
            "Epoch  15 Batch    7/269, Loss: 1752.904\n",
            "Epoch  15 Batch    8/269, Loss: 1736.140\n",
            "Epoch  15 Batch    9/269, Loss: 1703.121\n",
            "Epoch  15 Batch   10/269, Loss: 1698.629\n",
            "Epoch  15 Batch   11/269, Loss: 1690.369\n",
            "Epoch  15 Batch   12/269, Loss: 1687.191\n",
            "Epoch  15 Batch   13/269, Loss: 1683.751\n",
            "Epoch  15 Batch   14/269, Loss: 1669.304\n",
            "Epoch  15 Batch   15/269, Loss: 1654.326\n",
            "Epoch  15 Batch   16/269, Loss: 1667.112\n",
            "Epoch  15 Batch   17/269, Loss: 1630.536\n",
            "Epoch  15 Batch   18/269, Loss: 1640.594\n",
            "Epoch  15 Batch   19/269, Loss: 1630.938\n",
            "Epoch  15 Batch   20/269, Loss: 1636.421\n",
            "Epoch  15 Batch   21/269, Loss: 1583.630\n",
            "Epoch  15 Batch   22/269, Loss: 1583.674\n",
            "Epoch  15 Batch   23/269, Loss: 1594.612\n",
            "Epoch  15 Batch   24/269, Loss: 1585.067\n",
            "Epoch  15 Batch   25/269, Loss: 1572.043\n",
            "Epoch  15 Batch   26/269, Loss: 1576.349\n",
            "Epoch  15 Batch   27/269, Loss: 1629.642\n",
            "Epoch  15 Batch   28/269, Loss: 1615.069\n",
            "Epoch  15 Batch   29/269, Loss: 1653.199\n",
            "Epoch  15 Batch   30/269, Loss: 1649.571\n",
            "Epoch  15 Batch   31/269, Loss: 1700.928\n",
            "Epoch  15 Batch   32/269, Loss: 1716.618\n",
            "Epoch  15 Batch   33/269, Loss: 1728.934\n",
            "Epoch  15 Batch   34/269, Loss: 1848.620\n",
            "Epoch  15 Batch   35/269, Loss: 1982.963\n",
            "Epoch  15 Batch   36/269, Loss: 2037.180\n",
            "Epoch  15 Batch   37/269, Loss: 2114.287\n",
            "Epoch  15 Batch   38/269, Loss: 2185.492\n",
            "Epoch  15 Batch   39/269, Loss: 2270.175\n",
            "Epoch  15 Batch   40/269, Loss: 2349.633\n",
            "Epoch  15 Batch   41/269, Loss: 2500.114\n",
            "Epoch  15 Batch   42/269, Loss: 2454.932\n",
            "Epoch  15 Batch   43/269, Loss: 2020.362\n",
            "Epoch  15 Batch   44/269, Loss: 1721.153\n",
            "Epoch  15 Batch   45/269, Loss: 1560.161\n",
            "Epoch  15 Batch   46/269, Loss: 1161.526\n",
            "Epoch  15 Batch   47/269, Loss: 1055.273\n",
            "Epoch  15 Batch   48/269, Loss: 1052.322\n",
            "Epoch  15 Batch   49/269, Loss: 1048.863\n",
            "Epoch  15 Batch   50/269, Loss: 1163.653\n",
            "Epoch  15 Batch   51/269, Loss: 1134.894\n",
            "Epoch  15 Batch   52/269, Loss: 1055.581\n",
            "Epoch  15 Batch   53/269, Loss: 1069.624\n",
            "Epoch  15 Batch   54/269, Loss: 1023.584\n",
            "Epoch  15 Batch   55/269, Loss: 953.907\n",
            "Epoch  15 Batch   56/269, Loss: 1018.157\n",
            "Epoch  15 Batch   57/269, Loss: 1136.488\n",
            "Epoch  15 Batch   58/269, Loss: 1234.845\n",
            "Epoch  15 Batch   59/269, Loss: 1243.344\n",
            "Epoch  15 Batch   60/269, Loss: 1206.000\n",
            "Epoch  15 Batch   61/269, Loss: 1125.577\n",
            "Epoch  15 Batch   62/269, Loss: 1017.806\n",
            "Epoch  15 Batch   63/269, Loss: 878.597\n",
            "Epoch  15 Batch   64/269, Loss: 907.396\n",
            "Epoch  15 Batch   65/269, Loss: 750.686\n",
            "Epoch  15 Batch   66/269, Loss: 738.232\n",
            "Epoch  15 Batch   67/269, Loss: 936.885\n",
            "Epoch  15 Batch   68/269, Loss: 1021.901\n",
            "Epoch  15 Batch   69/269, Loss: 1233.040\n",
            "Epoch  15 Batch   70/269, Loss: 1209.197\n",
            "Epoch  15 Batch   71/269, Loss: 1136.578\n",
            "Epoch  15 Batch   72/269, Loss: 1142.099\n",
            "Epoch  15 Batch   73/269, Loss: 1113.803\n",
            "Epoch  15 Batch   74/269, Loss: 977.623\n",
            "Epoch  15 Batch   75/269, Loss: 923.844\n",
            "Epoch  15 Batch   76/269, Loss: 907.109\n",
            "Epoch  15 Batch   77/269, Loss: 882.789\n",
            "Epoch  15 Batch   78/269, Loss: 804.220\n",
            "Epoch  15 Batch   79/269, Loss: 876.789\n",
            "Epoch  15 Batch   80/269, Loss: 878.485\n",
            "Epoch  15 Batch   81/269, Loss: 855.778\n",
            "Epoch  15 Batch   82/269, Loss: 724.973\n",
            "Epoch  15 Batch   83/269, Loss: 694.626\n",
            "Epoch  15 Batch   84/269, Loss: 715.960\n",
            "Epoch  15 Batch   85/269, Loss: 697.306\n",
            "Epoch  15 Batch   86/269, Loss: 719.991\n",
            "Epoch  15 Batch   87/269, Loss: 729.568\n",
            "Epoch  15 Batch   88/269, Loss: 745.343\n",
            "Epoch  15 Batch   89/269, Loss: 633.641\n",
            "Epoch  15 Batch   90/269, Loss: 550.126\n",
            "Epoch  15 Batch   91/269, Loss: 616.579\n",
            "Epoch  15 Batch   92/269, Loss: 630.474\n",
            "Epoch  15 Batch   93/269, Loss: 643.421\n",
            "Epoch  15 Batch   94/269, Loss: 615.376\n",
            "Epoch  15 Batch   95/269, Loss: 505.211\n",
            "Epoch  15 Batch   96/269, Loss: 540.508\n",
            "Epoch  15 Batch   97/269, Loss: 552.908\n",
            "Epoch  15 Batch   98/269, Loss: 549.147\n",
            "Epoch  15 Batch   99/269, Loss: 477.109\n",
            "Epoch  15 Batch  100/269, Loss: 525.719\n",
            "Epoch  15 Batch  101/269, Loss: 474.372\n",
            "Epoch  15 Batch  102/269, Loss: 454.301\n",
            "Epoch  15 Batch  103/269, Loss: 481.874\n",
            "Epoch  15 Batch  104/269, Loss: 462.932\n",
            "Epoch  15 Batch  105/269, Loss: 467.453\n",
            "Epoch  15 Batch  106/269, Loss: 498.352\n",
            "Epoch  15 Batch  107/269, Loss: 559.915\n",
            "Epoch  15 Batch  108/269, Loss: 610.975\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch  15 Batch  109/269, Loss: 595.353\n",
            "Epoch  15 Batch  110/269, Loss: 534.349\n",
            "Epoch  15 Batch  111/269, Loss: 505.081\n",
            "Epoch  15 Batch  112/269, Loss: 494.516\n",
            "Epoch  15 Batch  113/269, Loss: 494.352\n",
            "Epoch  15 Batch  114/269, Loss: 433.332\n",
            "Epoch  15 Batch  115/269, Loss: 426.222\n",
            "Epoch  15 Batch  116/269, Loss: 433.207\n",
            "Epoch  15 Batch  117/269, Loss: 452.666\n",
            "Epoch  15 Batch  118/269, Loss: 467.298\n",
            "Epoch  15 Batch  119/269, Loss: 457.147\n",
            "Epoch  15 Batch  120/269, Loss: 485.021\n",
            "Epoch  15 Batch  121/269, Loss: 489.227\n",
            "Epoch  15 Batch  122/269, Loss: 501.370\n",
            "Epoch  15 Batch  123/269, Loss: 523.413\n",
            "Epoch  15 Batch  124/269, Loss: 546.926\n",
            "Epoch  15 Batch  125/269, Loss: 565.088\n",
            "Epoch  15 Batch  126/269, Loss: 557.668\n",
            "Epoch  15 Batch  127/269, Loss: 584.629\n",
            "Epoch  15 Batch  128/269, Loss: 606.676\n",
            "Epoch  15 Batch  129/269, Loss: 628.486\n",
            "Epoch  15 Batch  130/269, Loss: 634.201\n",
            "Epoch  15 Batch  131/269, Loss: 631.064\n",
            "Epoch  15 Batch  132/269, Loss: 634.232\n",
            "Epoch  15 Batch  133/269, Loss: 610.528\n",
            "Epoch  15 Batch  134/269, Loss: 555.894\n",
            "Epoch  15 Batch  135/269, Loss: 518.781\n",
            "Epoch  15 Batch  136/269, Loss: 511.102\n",
            "Epoch  15 Batch  137/269, Loss: 480.964\n",
            "Epoch  15 Batch  138/269, Loss: 464.349\n",
            "Epoch  15 Batch  139/269, Loss: 460.193\n",
            "Epoch  15 Batch  140/269, Loss: 461.975\n",
            "Epoch  15 Batch  141/269, Loss: 454.465\n",
            "Epoch  15 Batch  142/269, Loss: 449.010\n",
            "Epoch  15 Batch  143/269, Loss: 461.755\n",
            "Epoch  15 Batch  144/269, Loss: 457.323\n",
            "Epoch  15 Batch  145/269, Loss: 458.868\n",
            "Epoch  15 Batch  146/269, Loss: 462.887\n",
            "Epoch  15 Batch  147/269, Loss: 501.378\n",
            "Epoch  15 Batch  148/269, Loss: 547.078\n",
            "Epoch  15 Batch  149/269, Loss: 559.274\n",
            "Epoch  15 Batch  150/269, Loss: 509.512\n",
            "Epoch  15 Batch  151/269, Loss: 477.620\n",
            "Epoch  15 Batch  152/269, Loss: 480.796\n",
            "Epoch  15 Batch  153/269, Loss: 514.905\n",
            "Epoch  15 Batch  154/269, Loss: 571.802\n",
            "Epoch  15 Batch  155/269, Loss: 615.387\n",
            "Epoch  15 Batch  156/269, Loss: 671.451\n",
            "Epoch  15 Batch  157/269, Loss: 662.931\n",
            "Epoch  15 Batch  158/269, Loss: 617.208\n",
            "Epoch  15 Batch  159/269, Loss: 577.561\n",
            "Epoch  15 Batch  160/269, Loss: 540.988\n",
            "Epoch  15 Batch  161/269, Loss: 546.206\n",
            "Epoch  15 Batch  162/269, Loss: 545.642\n",
            "Epoch  15 Batch  163/269, Loss: 543.514\n",
            "Epoch  15 Batch  164/269, Loss: 536.106\n",
            "Epoch  15 Batch  165/269, Loss: 524.889\n",
            "Epoch  15 Batch  166/269, Loss: 546.892\n",
            "Epoch  15 Batch  167/269, Loss: 562.151\n",
            "Epoch  15 Batch  168/269, Loss: 560.808\n",
            "Epoch  15 Batch  169/269, Loss: 578.441\n",
            "Epoch  15 Batch  170/269, Loss: 574.019\n",
            "Epoch  15 Batch  171/269, Loss: 602.085\n",
            "Epoch  15 Batch  172/269, Loss: 635.579\n",
            "Epoch  15 Batch  173/269, Loss: 669.476\n",
            "Epoch  15 Batch  174/269, Loss: 707.055\n",
            "Epoch  15 Batch  175/269, Loss: 722.391\n",
            "Epoch  15 Batch  176/269, Loss: 709.572\n",
            "Epoch  15 Batch  177/269, Loss: 695.479\n",
            "Epoch  15 Batch  178/269, Loss: 724.204\n",
            "Epoch  15 Batch  179/269, Loss: 721.598\n",
            "Epoch  15 Batch  180/269, Loss: 746.353\n",
            "Epoch  15 Batch  181/269, Loss: 782.905\n",
            "Epoch  15 Batch  182/269, Loss: 768.491\n",
            "Epoch  15 Batch  183/269, Loss: 764.416\n",
            "Epoch  15 Batch  184/269, Loss: 747.132\n",
            "Epoch  15 Batch  185/269, Loss: 735.847\n",
            "Epoch  15 Batch  186/269, Loss: 710.295\n",
            "Epoch  15 Batch  187/269, Loss: 701.189\n",
            "Epoch  15 Batch  188/269, Loss: 668.976\n",
            "Epoch  15 Batch  189/269, Loss: 634.786\n",
            "Epoch  15 Batch  190/269, Loss: 592.133\n",
            "Epoch  15 Batch  191/269, Loss: 578.555\n",
            "Epoch  15 Batch  192/269, Loss: 524.951\n",
            "Epoch  15 Batch  193/269, Loss: 491.997\n",
            "Epoch  15 Batch  194/269, Loss: 449.087\n",
            "Epoch  15 Batch  195/269, Loss: 445.558\n",
            "Epoch  15 Batch  196/269, Loss: 483.691\n",
            "Epoch  15 Batch  197/269, Loss: 584.942\n",
            "Epoch  15 Batch  198/269, Loss: 708.921\n",
            "Epoch  15 Batch  199/269, Loss: 831.753\n",
            "Epoch  15 Batch  200/269, Loss: 922.272\n",
            "Epoch  15 Batch  201/269, Loss: 1041.334\n",
            "Epoch  15 Batch  202/269, Loss: 1140.448\n",
            "Epoch  15 Batch  203/269, Loss: 1133.107\n",
            "Epoch  15 Batch  204/269, Loss: 1086.687\n",
            "Epoch  15 Batch  205/269, Loss: 1072.728\n",
            "Epoch  15 Batch  206/269, Loss: 1067.181\n",
            "Epoch  15 Batch  207/269, Loss: 1116.024\n",
            "Epoch  15 Batch  208/269, Loss: 1129.064\n",
            "Epoch  15 Batch  209/269, Loss: 1157.126\n",
            "Epoch  15 Batch  210/269, Loss: 1170.686\n",
            "Epoch  15 Batch  211/269, Loss: 1168.566\n",
            "Epoch  15 Batch  212/269, Loss: 1175.823\n",
            "Epoch  15 Batch  213/269, Loss: 1181.441\n",
            "Epoch  15 Batch  214/269, Loss: 1130.374\n",
            "Epoch  15 Batch  215/269, Loss: 1047.655\n",
            "Epoch  15 Batch  216/269, Loss: 1034.900\n",
            "Epoch  15 Batch  217/269, Loss: 1005.318\n",
            "Epoch  15 Batch  218/269, Loss: 997.721\n",
            "Epoch  15 Batch  219/269, Loss: 969.586\n",
            "Epoch  15 Batch  220/269, Loss: 968.245\n",
            "Epoch  15 Batch  221/269, Loss: 948.199\n",
            "Epoch  15 Batch  222/269, Loss: 920.864\n",
            "Epoch  15 Batch  223/269, Loss: 935.246\n",
            "Epoch  15 Batch  224/269, Loss: 929.810\n",
            "Epoch  15 Batch  225/269, Loss: 961.588\n",
            "Epoch  15 Batch  226/269, Loss: 986.925\n",
            "Epoch  15 Batch  227/269, Loss: 987.467\n",
            "Epoch  15 Batch  228/269, Loss: 1047.778\n",
            "Epoch  15 Batch  229/269, Loss: 1043.892\n",
            "Epoch  15 Batch  230/269, Loss: 1078.913\n",
            "Epoch  15 Batch  231/269, Loss: 1071.858\n",
            "Epoch  15 Batch  232/269, Loss: 1153.032\n",
            "Epoch  15 Batch  233/269, Loss: 1148.077\n",
            "Epoch  15 Batch  234/269, Loss: 1162.400\n",
            "Epoch  15 Batch  235/269, Loss: 1173.949\n",
            "Epoch  15 Batch  236/269, Loss: 1196.324\n",
            "Epoch  15 Batch  237/269, Loss: 1237.520\n",
            "Epoch  15 Batch  238/269, Loss: 1229.723\n",
            "Epoch  15 Batch  239/269, Loss: 1249.019\n",
            "Epoch  15 Batch  240/269, Loss: 1268.687\n",
            "Epoch  15 Batch  241/269, Loss: 1261.771\n",
            "Epoch  15 Batch  242/269, Loss: 1279.170\n",
            "Epoch  15 Batch  243/269, Loss: 1243.851\n",
            "Epoch  15 Batch  244/269, Loss: 1218.953\n",
            "Epoch  15 Batch  245/269, Loss: 1219.042\n",
            "Epoch  15 Batch  246/269, Loss: 1211.786\n",
            "Epoch  15 Batch  247/269, Loss: 1224.654\n",
            "Epoch  15 Batch  248/269, Loss: 1229.709\n",
            "Epoch  15 Batch  249/269, Loss: 1219.981\n",
            "Epoch  15 Batch  250/269, Loss: 1193.443\n",
            "Epoch  15 Batch  251/269, Loss: 1211.734\n",
            "Epoch  15 Batch  252/269, Loss: 1216.422\n",
            "Epoch  15 Batch  253/269, Loss: 1251.729\n",
            "Epoch  15 Batch  254/269, Loss: 1185.952\n",
            "Epoch  15 Batch  255/269, Loss: 1194.641\n",
            "Epoch  15 Batch  256/269, Loss: 1188.225\n",
            "Epoch  15 Batch  257/269, Loss: 1150.042\n",
            "Epoch  15 Batch  258/269, Loss: 1145.509\n",
            "Epoch  15 Batch  259/269, Loss: 1148.538\n",
            "Epoch  15 Batch  260/269, Loss: 1142.987\n",
            "Epoch  15 Batch  261/269, Loss: 1170.964\n",
            "Epoch  15 Batch  262/269, Loss: 1141.183\n",
            "Epoch  15 Batch  263/269, Loss: 1182.732\n",
            "Epoch  15 Batch  264/269, Loss: 1181.209\n",
            "Epoch  15 Batch  265/269, Loss: 1189.812\n",
            "Epoch  15 Batch  266/269, Loss: 1202.517\n",
            "Epoch  15 Batch  267/269, Loss: 1215.524\n",
            "Epoch  16 Batch    0/269, Loss: 1212.212\n",
            "Epoch  16 Batch    1/269, Loss: 1250.958\n",
            "Epoch  16 Batch    2/269, Loss: 1259.065\n",
            "Epoch  16 Batch    3/269, Loss: 1281.590\n",
            "Epoch  16 Batch    4/269, Loss: 1322.271\n",
            "Epoch  16 Batch    5/269, Loss: 1339.021\n",
            "Epoch  16 Batch    6/269, Loss: 1373.273\n",
            "Epoch  16 Batch    7/269, Loss: 1387.991\n",
            "Epoch  16 Batch    8/269, Loss: 1436.682\n",
            "Epoch  16 Batch    9/269, Loss: 1493.224\n",
            "Epoch  16 Batch   10/269, Loss: 1520.384\n",
            "Epoch  16 Batch   11/269, Loss: 1559.771\n",
            "Epoch  16 Batch   12/269, Loss: 1585.603\n",
            "Epoch  16 Batch   13/269, Loss: 1599.891\n",
            "Epoch  16 Batch   14/269, Loss: 1616.281\n",
            "Epoch  16 Batch   15/269, Loss: 1640.911\n",
            "Epoch  16 Batch   16/269, Loss: 1624.897\n",
            "Epoch  16 Batch   17/269, Loss: 1643.768\n",
            "Epoch  16 Batch   18/269, Loss: 1668.984\n",
            "Epoch  16 Batch   19/269, Loss: 1702.853\n",
            "Epoch  16 Batch   20/269, Loss: 1733.646\n",
            "Epoch  16 Batch   21/269, Loss: 1804.463\n",
            "Epoch  16 Batch   22/269, Loss: 1868.134\n",
            "Epoch  16 Batch   23/269, Loss: 1882.717\n",
            "Epoch  16 Batch   24/269, Loss: 1944.038\n",
            "Epoch  16 Batch   25/269, Loss: 1951.290\n",
            "Epoch  16 Batch   26/269, Loss: 1942.102\n",
            "Epoch  16 Batch   27/269, Loss: 1951.015\n",
            "Epoch  16 Batch   28/269, Loss: 1935.358\n",
            "Epoch  16 Batch   29/269, Loss: 1936.792\n",
            "Epoch  16 Batch   30/269, Loss: 1953.356\n",
            "Epoch  16 Batch   31/269, Loss: 1900.766\n",
            "Epoch  16 Batch   32/269, Loss: 1909.158\n",
            "Epoch  16 Batch   33/269, Loss: 1874.921\n",
            "Epoch  16 Batch   34/269, Loss: 1869.999\n",
            "Epoch  16 Batch   35/269, Loss: 1815.771\n",
            "Epoch  16 Batch   36/269, Loss: 1787.025\n",
            "Epoch  16 Batch   37/269, Loss: 1725.657\n",
            "Epoch  16 Batch   38/269, Loss: 1722.191\n",
            "Epoch  16 Batch   39/269, Loss: 1684.375\n",
            "Epoch  16 Batch   40/269, Loss: 1670.579\n",
            "Epoch  16 Batch   41/269, Loss: 1704.624\n",
            "Epoch  16 Batch   42/269, Loss: 1755.763\n",
            "Epoch  16 Batch   43/269, Loss: 1748.588\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch  16 Batch   44/269, Loss: 1844.266\n",
            "Epoch  16 Batch   45/269, Loss: 1879.710\n",
            "Epoch  16 Batch   46/269, Loss: 1919.995\n",
            "Epoch  16 Batch   47/269, Loss: 1985.347\n",
            "Epoch  16 Batch   48/269, Loss: 1980.985\n",
            "Epoch  16 Batch   49/269, Loss: 1990.029\n",
            "Epoch  16 Batch   50/269, Loss: 2036.617\n",
            "Epoch  16 Batch   51/269, Loss: 2051.805\n",
            "Epoch  16 Batch   52/269, Loss: 2052.482\n",
            "Epoch  16 Batch   53/269, Loss: 2061.703\n",
            "Epoch  16 Batch   54/269, Loss: 2047.547\n",
            "Epoch  16 Batch   55/269, Loss: 2075.425\n",
            "Epoch  16 Batch   56/269, Loss: 2039.376\n",
            "Epoch  16 Batch   57/269, Loss: 2018.772\n",
            "Epoch  16 Batch   58/269, Loss: 2026.846\n",
            "Epoch  16 Batch   59/269, Loss: 1995.029\n",
            "Epoch  16 Batch   60/269, Loss: 1980.320\n",
            "Epoch  16 Batch   61/269, Loss: 1926.336\n",
            "Epoch  16 Batch   62/269, Loss: 1900.712\n",
            "Epoch  16 Batch   63/269, Loss: 1878.806\n",
            "Epoch  16 Batch   64/269, Loss: 1863.468\n",
            "Epoch  16 Batch   65/269, Loss: 1762.828\n",
            "Epoch  16 Batch   66/269, Loss: 1778.517\n",
            "Epoch  16 Batch   67/269, Loss: 1714.355\n",
            "Epoch  16 Batch   68/269, Loss: 1674.221\n",
            "Epoch  16 Batch   69/269, Loss: 1659.450\n",
            "Epoch  16 Batch   70/269, Loss: 1616.867\n",
            "Epoch  16 Batch   71/269, Loss: 1589.559\n",
            "Epoch  16 Batch   72/269, Loss: 1559.810\n",
            "Epoch  16 Batch   73/269, Loss: 1473.427\n",
            "Epoch  16 Batch   74/269, Loss: 1416.318\n",
            "Epoch  16 Batch   75/269, Loss: 1361.531\n",
            "Epoch  16 Batch   76/269, Loss: 1346.390\n",
            "Epoch  16 Batch   77/269, Loss: 1274.394\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UWRq_JE9lUiE",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Save parameters for checkpoint\n",
        "save_params(save_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fl6sqfrMlUiG",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "_, (source_vocab_to_int, target_vocab_to_int), (source_int_to_vocab, target_int_to_vocab) = load_preprocess()\n",
        "load_path = load_params()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cH1_ZIA3lUiJ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "translate_sentence = \"new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\"\n",
        "#fr to en\n",
        "#input: \"new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\"\n",
        "#target:\"new jersey is sometimes quiet during autumn , and it is snowy in april .\"\n",
        "\n",
        "print(translate_sentence)\n",
        "\n",
        "\"\"\"\n",
        "DON'T MODIFY ANYTHING IN THIS CELL\n",
        "\"\"\"\n",
        "translate_sentence = sentence_to_seq(translate_sentence, source_vocab_to_int)\n",
        "print(np.shape(translate_sentence))\n",
        "\n",
        "loaded_graph = tf.Graph()\n",
        "with tf.Session(graph=loaded_graph) as sess:\n",
        "    # Load saved model\n",
        "    loader = tf.train.import_meta_graph(load_path + '.meta')\n",
        "    loader.restore(sess, load_path)\n",
        "\n",
        "    encoder_inputs = loaded_graph.get_tensor_by_name('encoder_inputs:0')\n",
        "    encoder_inputs_length = loaded_graph.get_tensor_by_name('encoder_inputs_length:0')\n",
        "    decoder_pred_decode = loaded_graph.get_tensor_by_name('decoder_pred_decode:0')\n",
        "    \n",
        "    predicted_ids = sess.run(decoder_pred_decode, {encoder_inputs: [translate_sentence],\n",
        "                                                       encoder_inputs_length: [np.shape(translate_sentence)[0]]})[0]\n",
        "\n",
        "print('Input')\n",
        "print('  Word Ids:      {}'.format([i for i in translate_sentence]))\n",
        "print('  Source Words: {}'.format([source_int_to_vocab[i] for i in translate_sentence]))\n",
        "\n",
        "print('\\nPrediction')\n",
        "print('  Word Ids:      {}'.format([i[0] for i in predicted_ids]))\n",
        "print('  Predicted Words: {}'.format([target_int_to_vocab[i[0]] for i in predicted_ids]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SC5kTfQ5lUiL",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "outputId": "f670fb50-0de3-4ccf-d8d5-5e7efda99d71"
      },
      "cell_type": "code",
      "source": [
        "print('\\nTranslation:\\n')\n",
        "translation = ''\n",
        "for word_i in predicted_ids:\n",
        "    translation += target_int_to_vocab[word_i[0]] + ' '\n",
        "    \n",
        "print(translation)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Translation:\n",
            "\n",
            "going go freezing going going pears animals may store tower little saw little little little little little little little little little little little little little little little little little little little little \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DeUaen4SlUiP",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aGmBN99QlUiS",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}