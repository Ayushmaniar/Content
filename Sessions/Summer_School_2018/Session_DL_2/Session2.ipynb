{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Session2.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "7zSVW-HayxII",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Deep Feedforward Networks - a general description"
      ]
    },
    {
      "metadata": {
        "id": "SPkFcch2zKiP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Deep feedforward networks**, also often called feedforward neural networks, or **multilayer perceptrons (MLPs)**, are the quintessential deep learning models. The goal of a feedforward network is to approximate some function $f^*$. For example, for a classifier, $y = f^∗(x)$ maps an input $x$ to a category $y$. A feedforward network defines a mapping $y = f (x; θ)$ and learns the value of the parameters θ that result in the best function approximation.\n",
        " \n",
        "These models are called feedforward because information flows through the function being evaluated from $x$, through the intermediate computations used to define $f$, and finally to the output $y$. \n",
        "\n",
        "Feedforward neural networks are called networks because they are typically represented by composing together many different functions. The model is asso- ciated with a directed acyclic graph describing how the functions are composed together. For example, we might have three functions $f^{(1)}$,$ f^{(2)}$, and  $f^{(3)}$ connected in a chain, to form $f(x) = f^{(3)}(f^{(2)}(f^{(1)}(x)))$. These chain structures are the most commonly used structures of neural networks. In this case, $f^{(1)}$ is called the first layer of the network, $f^{(2)}$ is called the second layer, and so on. \n",
        "\n",
        "The overall **length of the chain** gives the **depth** of the model. It is from this terminology that the name **“deep learning”** arises. The final layer of a feedforward network is called the **output layer**. During neural network training, we drive $f(x)$ to match $f^∗(x)$. The training data provides us with noisy, approximate examples of $f^∗(x)$ evaluated at different training points. Each example $x$ is accompanied by a label $y ≈ f^∗(x)$. The training examples specify directly what the output layer must do at each point x; it must produce a value that is close to $y$. The behavior of the other layers is not directly specified by the training data. The learning algorithm must decide how to use those layers to produce the desired output, but the training data does not say what each individual layer should do. Instead, the learning algorithm must decide how to use these layers to best implement an approximation of $f^∗$. Because the training data does not show the desired output for each of these layers, these layers are called **hidden layers**.\n",
        "\n",
        "Finally, these networks are called **neural** because they are loosely inspired by **neuroscience**. Each hidden layer of the network is typically **vector-valued**. The dimensionality of these hidden layers determines the width of the model. Each element of the vector may be interpreted as playing a role analogous to a neuron.\n",
        "\n",
        "![alt text](https://i.imgur.com/38lpenv.png\n",
        ")"
      ]
    },
    {
      "metadata": {
        "id": "rUOfJNmw5FFX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Learning the XOR function  "
      ]
    },
    {
      "metadata": {
        "id": "3uXdVYV08xVS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To make the idea of a feedforward network more concrete, we begin with an example of a fully functioning feedforward network on a very simple task: learning the **XOR function**.\n",
        "\n",
        "The **XOR function (“exclusive or”)** is an operation on **two binary values, x1 and x2**. When **exactly one** of these binary values is **equal to 1**, the XOR function **returns** **1**. Otherwise, it returns 0. The XOR function provides the target function $y = f^∗(x)$ that we want to learn. Our model provides a function $y = f(x;θ)$ and our learning algorithm will adapt the parameters θ to make f as similar as possible to $f^∗$.\n",
        "\n",
        "In this simple example, we will not be concerned with statistical generalization. We want our network to perform correctly on the four points  \n",
        "$\\mathbb{X}= \\{{[0,0]^T, [0,1^T, [1,0]^T,[1,1]^T}\\}$. We will train the network on all four of these points. The only challenge is to fit the training set.\n",
        "\n",
        "Clearly, this is a regression problem where we can use the mean squared error loss function.\n",
        "\n",
        "Evaluated on our whole training set, the MSE loss function is\n",
        "\n",
        "$J(θ)= \\dfrac{1}{4} \\Sigma_{x\\epsilon\\mathbb{X}}(f (x)−f(x;θ))^2$ .\n",
        "\n",
        "Now we must choose the form of our model, f (x; θ). Suppose that we choose a linear model, with θ consisting of w and b. Our model is defined to be \n",
        "\n",
        "$f(x; w, b) = x^Tw + b$.\n",
        "\n",
        "We can minimize $J(θ)$ in closed form with respect to w and b using the normal\n",
        "equations.\n",
        "\n",
        "After solving the normal equations, we obtain a closed form solution, $w = 0$ and $b = 0.5$. This means that the linear model simply outputs 0.5 everywhere.\n",
        "\n",
        "**But why couldn't a linear model  represent this function?**"
      ]
    },
    {
      "metadata": {
        "id": "47LeVbfZDXyJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's now model this with a **DNN**. \n",
        "Specifically, we will introduce a very simple feedforward network with one\n",
        "hidden layer containing two hidden units.  This feedforward network has a vector of hidden units $h$ that are\n",
        "computed by a function $f^{(1)} (x; W , c )$. The values of these hidden units are then\n",
        "used as the input for a second layer. The second layer is the output layer of the\n",
        "network. The output layer is still just a linear regression model, but now it is\n",
        "applied to $h$ rather than to $x$ . The network now contains two functions chained\n",
        "together: $h = f (1) (x; W , c )$ and $y = f^{(2)} (h; w, b)$, with the complete model being\n",
        "$f ( x ; W , c , w , b ) = f^{(2)} (f^{(1)} ( x ))$ .\n",
        "\n",
        "For a better illustration of the network, let's look at this figure.\n",
        "\n",
        "![alt text](https://imgur.com/BQH5IG1.png)\n",
        "\n",
        "Here $[x1,x2]$ is the input vector, and $y$ is the output. Now we need a non-linear transformation from the input feature space to the hidden feature space of $h$. This is achieved through the first layer. The first layer can be represented as $h=f^{(1)}(x,W,c)$ where $f^{(1)}$ is a non-linear transformation in itself. Thus $h = g(W^Tx+c) $ where $W^Tx+c$ is an affine transform and $g$ is a non-linear function. This function $g$ is called as the activation function. In this case, (and most cases we will) let's use a simple function known as **ReLU**. ReLU (Rectified Linear Unit), not as complex as it sounds, is the simple function $max\\{0,x\\}$. The output layer is just a linear function $w^Th+b$. Overall, the neural network represents the function\n",
        "\n",
        "$f(x;W,c,w,b)=w^Tmax\\{0,W^Tx+c\\} +b$\n",
        "\n",
        "\n",
        "**With this setup let's guess a solution**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "3Un46qtVn3Tp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Gradient-Based Learning\n",
        "**Can we keep guessing solutions like this ?**\n",
        "\n",
        "State-of-the-art neural networks have millions of parameters to be tuned. For optimizing these million parameters, we need an objective towards which we would like to drive our model. This objective is minimizing a value defined by a **cost function**.  There are many cost functions which we use depending on the purpose"
      ]
    },
    {
      "metadata": {
        "id": "l0Ed4U8xpizs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Cost functions\n",
        "The cost functions used in neural networks are the same as those used in simpler paramteric models such as the linear model. These cost functions represent a parametrized distribution whose parameters are to be optimized.\n",
        "\n",
        "You would have seen some cost functions yesterday. \n",
        "\n",
        "To name a few,\n",
        "\n",
        "*   Mean Squared Error Loss\n",
        "*   Cross Entropy Loss\n",
        "*   L1 Loss\n",
        "\n",
        "Some times we use a neural network to model our loss function as well. You'll be learning that later.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "7HhHKnE2rJvt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Gradient Descent\n",
        "Our task is now to minimize the cost function.\n",
        "\n",
        "**How do we do that?**\n",
        " In the case of the linear model, the loss function was convex w.r.t the parameters as seen in the image. This is a plot of a loss function $J$ w.r.t a parameter $w$.\n",
        " ![alt text](https://i.imgur.com/LAJ8Uag.png)\n",
        " \n",
        " From, the figure it is clear that for convex functions we can achieve the minima by descending using gradients. \n",
        " Esssentially, we can represent this using the update statement. We see that the gradient is positive if we move far too much from the optimal point and negative if we are far behind the optimal point. Thus the negative of the gradient gives the direction in which we have to drive our parameter $w$.\n",
        "    \n",
        " ![alt text](https://i.imgur.com/oBIKHky.png)\n",
        " \n",
        " Here, $\\alpha$ is known as the learning rate because it decides how much to alter the parameter in each step.\n",
        " \n",
        "For convex cost functions there are global convergance guarantees.\n",
        " \n",
        "This step represents one iteration of gradient descent. We do this iteratively until we reach the optimal point. \n",
        " \n",
        " While this is true for convex loss functions, what about neural networks?\n",
        "\n",
        " The cost functions for Neural Networks need not be convex. Thus, there is no guarantees as such.\n",
        " \n",
        "So what do we do?\n",
        "\n",
        "We continue using gradient descent with some precautions. **We initialise the weights to be some random numbers close to 0**.\n"
      ]
    },
    {
      "metadata": {
        "id": "O-dIQqj00umt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Back Propagation\n",
        "While in linear models gradients are easy to compute, what about neural networks?\n",
        "\n",
        "This can be done using chain rule. \n",
        "\n",
        "In \"**Deep Learning**\", we call it backpropagation."
      ]
    },
    {
      "metadata": {
        "id": "0i2GSwye4Q_L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Circuit Intuition for Back Prop\n",
        "\n",
        "![alt text](https://i.imgur.com/MjZjeSP.png)"
      ]
    },
    {
      "metadata": {
        "id": "_5Le7tSs1ZBD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Optimization\n",
        "\n",
        "We will now discuss 3 optimizers.\n",
        "\n",
        "*   **Batch Gradient Descent**\n",
        "*   **Stochastic Gradient Descent**\n",
        "*   **Mini Batch Gradient Descent**\n",
        "\n",
        "**Batch Gradient Descent Optimization ** involves computing the loss for the entire dataset at once and then backpropagating and updating at a time.\n",
        "\n",
        "In **Stochastic Gradient Descent Optimization**, we compute the loss function for a single training example , one at a time, and then backpropagating.\n",
        "\n",
        "In **Mini Batch Gradient Descent**, the entire training data is divided into batches and we compute loss for each batch at a time and backpropagate.\n",
        "\n",
        "Mini Batch Gradient Descent technique is commonly used for some advantages that it provides.\n",
        "\n",
        "There are various complex optimization algorithms such as momentum, RMSprop, Adam etc.  Of course, we won't go into details on them.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "yPS1lbGE8ueK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Hyperparameters\n",
        "\n",
        "Hyperparameters are some predefined constants used to train a network. \n",
        "For example, the learning rate,  number of epochs to train for, batch size, number of layers in the network etc. are all hyperparameters\n",
        "\n",
        "There is no simple way to decide these. Basically, try them all and choose the best."
      ]
    },
    {
      "metadata": {
        "id": "0ec4pRkVu46U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# A Fast Pythonic Implementation of a Feed Forward Neural Network (Vectorised)"
      ]
    },
    {
      "metadata": {
        "id": "79vGJ99fE17s",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Setup\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "# for auto-reloading extenrnal modules\n",
        "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TStYjt7Fu-B2",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "outputId": "381faa3c-156e-42a1-b94b-ed16e63d8238",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1531822868765,
          "user_tz": -330,
          "elapsed": 1228,
          "user": {
            "displayName": "Rishhanth Maanav",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "116055284600069515854"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#generate random data -- not linearly separable \n",
        "np.random.seed(0)\n",
        "N = 100 # number of points per class\n",
        "D = 2 # dimensionality\n",
        "K = 3 # number of classes\n",
        "X = np.zeros((N*K,D))\n",
        "num_train_examples = X.shape[0]\n",
        "y = np.zeros(N*K, dtype='uint8')\n",
        "for j in range(K):\n",
        "  ix = range(N*j,N*(j+1))\n",
        "  r = np.linspace(0.0,1,N) # radius\n",
        "  t = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*0.2 # theta\n",
        "  X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
        "  y[ix] = j\n",
        "fig = plt.figure()\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
        "plt.xlim([-1,1])\n",
        "plt.ylim([-1,1])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-1, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmYAAAHWCAYAAADdDkViAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fadbbbdccf8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "73yCicW1vW3v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The sigmoid function \"squashes\" inputs to lie between 0 and 1. Unfortunately, this means that for inputs with sigmoid output close to 0 or 1, the gradient with respect to those inputs are close to zero. This leads to the phenomenon of vanishing gradients, where gradients drop close to zero, and the net does not learn well.\n",
        "\n",
        "On the other hand, the relu function (max(0, x)) does not saturate with input size. Plot these functions to gain intution."
      ]
    },
    {
      "metadata": {
        "id": "ZqIIKzcLvEeX",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    x = 1/(1+np.exp(-x))\n",
        "    return x\n",
        "\n",
        "def sigmoid_grad(x):\n",
        "    return (x)*(1-x)\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0,x)\n",
        "\n",
        "def tanh(x):\n",
        "    return (2*sigmoid(2*x) - 1)\n",
        "\n",
        "def tanh_grad(x):\n",
        "    return (1 - (tanh(x))**2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pUrzH3KqvHQx",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#function to train a three layer neural net with either RELU or sigmoid nonlinearity via vanilla grad descent\n",
        "\n",
        "def three_layer_net(NONLINEARITY,X,y, model, step_size, reg):\n",
        "    #parameter initialization\n",
        "    \n",
        "    h= model['h']\n",
        "    h2= model['h2']\n",
        "    W1= model['W1']\n",
        "    W2= model['W2']\n",
        "    W3= model['W3']\n",
        "    b1= model['b1']\n",
        "    b2= model['b2']\n",
        "    b3= model['b3']\n",
        "    \n",
        "    \n",
        "    # some hyperparameters\n",
        "\n",
        "\n",
        "    # gradient descent loop\n",
        "    num_examples = X.shape[0]\n",
        "    plot_array_1=[]\n",
        "    plot_array_2=[]\n",
        "    for i in range(50000):\n",
        "\n",
        "        #FOWARD PROP\n",
        "\n",
        "        if NONLINEARITY== 'RELU':\n",
        "            hidden_layer = relu(np.dot(X, W1) + b1)\n",
        "            hidden_layer2 = relu(np.dot(hidden_layer, W2) + b2)\n",
        "            scores = np.dot(hidden_layer2, W3) + b3\n",
        "\n",
        "        elif NONLINEARITY == 'SIGM':\n",
        "            hidden_layer = sigmoid(np.dot(X, W1) + b1)\n",
        "            hidden_layer2 = sigmoid(np.dot(hidden_layer, W2) + b2)\n",
        "            scores = np.dot(hidden_layer2, W3) + b3\n",
        "        \n",
        "        elif NONLINEARITY == 'TANH':\n",
        "            hidden_layer = tanh(np.dot(X, W1) + b1)\n",
        "            hidden_layer2 = tanh(np.dot(hidden_layer, W2) + b2)\n",
        "            scores = np.dot(hidden_layer2, W3) + b3\n",
        "\n",
        "        exp_scores = np.exp(scores)\n",
        "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]\n",
        "\n",
        "        # compute the loss: average cross-entropy loss and regularization\n",
        "        corect_logprobs = -np.log(probs[range(num_examples),y])\n",
        "        data_loss = np.sum(corect_logprobs)/num_examples\n",
        "        reg_loss = 0.5*reg*np.sum(W1*W1) + 0.5*reg*np.sum(W2*W2)+ 0.5*reg*np.sum(W3*W3)\n",
        "        loss = data_loss + reg_loss\n",
        "        if i % 1000 == 0:\n",
        "            print(\"iteration : \"+ str(i) + \" loss : \" + str(loss)  )\n",
        "\n",
        "\n",
        "        # compute the gradient on scores\n",
        "        dscores = probs\n",
        "        dscores[range(num_examples),y] -= 1\n",
        "        dscores /= num_examples\n",
        "\n",
        " \n",
        "        # BACKPROP HERE\n",
        "        dW3 = (hidden_layer2.T).dot(dscores)\n",
        "        db3 = np.sum(dscores, axis=0, keepdims=True)\n",
        "\n",
        "\n",
        "        if NONLINEARITY == 'RELU':\n",
        "\n",
        "            #backprop ReLU nonlinearity here\n",
        "            dhidden2 = np.dot(dscores, W3.T)\n",
        "            dhidden2[hidden_layer2 <= 0] = 0\n",
        "            dW2 =  np.dot( hidden_layer.T, dhidden2)\n",
        "            plot_array_2.append(np.sum(np.abs(dW2))/np.sum(np.abs(dW2.shape)))\n",
        "            db2 = np.sum(dhidden2, axis=0)\n",
        "            dhidden = np.dot(dhidden2, W2.T)\n",
        "            dhidden[hidden_layer <= 0] = 0\n",
        "            \n",
        "        elif NONLINEARITY == 'SIGM':\n",
        "\n",
        "            #backprop sigmoid nonlinearity here\n",
        "            dhidden2 = dscores.dot(W3.T)*sigmoid_grad(hidden_layer2)\n",
        "            dW2 = (hidden_layer.T).dot(dhidden2)\n",
        "            plot_array_2.append(np.sum(np.abs(dW2))/np.sum(np.abs(dW2.shape)))\n",
        "            db2 = np.sum(dhidden2, axis=0)\n",
        "            dhidden = dhidden2.dot(W2.T)*sigmoid_grad(hidden_layer)\n",
        "        \n",
        "        elif NONLINEARITY == 'TANH':\n",
        "            \n",
        "            #backprop tanh nonlinearity here\n",
        "            dhidden2 = dscores.dot(W3.T)*tanh_grad(hidden_layer2)\n",
        "            dW2 = (hidden_layer.T).dot(dhidden2)\n",
        "            plot_array_2.append(np.sum(np.abs(dW2))/np.sum(np.abs(dW2.shape)))\n",
        "            db2 = np.sum(dhidden2, axis=0)\n",
        "            dhidden = dhidden2.dot(W2.T)*tanh_grad(hidden_layer)\n",
        "            \n",
        "\n",
        "        \n",
        "        dW1 =  np.dot(X.T, dhidden)\n",
        "        plot_array_1.append(np.sum(np.abs(dW1))/np.sum(np.abs(dW1.shape)))\n",
        "        db1 = np.sum(dhidden, axis=0)\n",
        "\n",
        "        # add regularization\n",
        "        dW3+= reg * W3\n",
        "        dW2 += reg * W2\n",
        "        dW1 += reg * W1\n",
        "        \n",
        "        #option to return loss, grads -- uncomment next comment\n",
        "        grads={}\n",
        "        grads['W1']=dW1\n",
        "        grads['W2']=dW2\n",
        "        grads['W3']=dW3\n",
        "        grads['b1']=db1\n",
        "        grads['b2']=db2\n",
        "        grads['b3']=db3\n",
        "        #return loss, grads\n",
        "        \n",
        "        \n",
        "        # update\n",
        "        W1 += -step_size * dW1\n",
        "        b1 += -step_size * db1\n",
        "        W2 += -step_size * dW2\n",
        "        b2 += -step_size * db2\n",
        "        W3 += -step_size * dW3\n",
        "        b3 += -step_size * db3\n",
        "    # evaluate training set accuracy\n",
        "    if NONLINEARITY == 'RELU':\n",
        "        hidden_layer = relu(np.dot(X, W1) + b1)\n",
        "        hidden_layer2 = relu(np.dot(hidden_layer, W2) + b2)\n",
        "    elif NONLINEARITY == 'SIGM':\n",
        "        hidden_layer = sigmoid(np.dot(X, W1) + b1)\n",
        "        hidden_layer2 = sigmoid(np.dot(hidden_layer, W2) + b2)\n",
        "    scores = np.dot(hidden_layer2, W3) + b3\n",
        "    predicted_class = np.argmax(scores, axis=1)\n",
        "    print(\"training accuracy:\" + str(np.mean(predicted_class == y)))  \n",
        "    #return cost, grads\n",
        "    return plot_array_1, plot_array_2, W1, W2, W3, b1, b2, b3\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xBECHHL3veTs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train net with sigmoid nonlinearity first"
      ]
    },
    {
      "metadata": {
        "id": "22BtZy8-vJ1i",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "outputId": "5748be4e-89e5-40ae-b24c-7552b64b2b48",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1531822994727,
          "user_tz": -330,
          "elapsed": 123616,
          "user": {
            "displayName": "Rishhanth Maanav",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "116055284600069515854"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#Initialize toy model, train sigmoid net\n",
        "\n",
        "N = 100 # number of points per class\n",
        "D = 2 # dimensionality\n",
        "K = 3 # number of classes\n",
        "h=50\n",
        "h2=50\n",
        "num_train_examples = X.shape[0]\n",
        "\n",
        "model={}\n",
        "model['h'] = h # size of hidden layer 1\n",
        "model['h2']= h2# size of hidden layer 2\n",
        "model['W1']= 0.1 * np.random.randn(D,h)\n",
        "model['b1'] = np.zeros((1,h))\n",
        "model['W2'] = 0.1 * np.random.randn(h,h2)\n",
        "model['b2']= np.zeros((1,h2))\n",
        "model['W3'] = 0.1 * np.random.randn(h2,K)\n",
        "model['b3'] = np.zeros((1,K))\n",
        "\n",
        "(sigm_array_1, sigm_array_2, s_W1, s_W2,s_W3, s_b1, s_b2,s_b3) = three_layer_net('SIGM', X,y,model, step_size=1e-1, reg=1e-3)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration : 0 loss : 1.1564048888640268\n",
            "iteration : 1000 loss : 1.1007369422747169\n",
            "iteration : 2000 loss : 0.9996976474584222\n",
            "iteration : 3000 loss : 0.8554946768068612\n",
            "iteration : 4000 loss : 0.819427292698583\n",
            "iteration : 5000 loss : 0.8148248201455693\n",
            "iteration : 6000 loss : 0.8105259085596594\n",
            "iteration : 7000 loss : 0.805942788846737\n",
            "iteration : 8000 loss : 0.8006883214893732\n",
            "iteration : 9000 loss : 0.7939760223457257\n",
            "iteration : 10000 loss : 0.7832005415212004\n",
            "iteration : 11000 loss : 0.7599087859714144\n",
            "iteration : 12000 loss : 0.7197916117626582\n",
            "iteration : 13000 loss : 0.6831938875797997\n",
            "iteration : 14000 loss : 0.6558471217342814\n",
            "iteration : 15000 loss : 0.6349964251436384\n",
            "iteration : 16000 loss : 0.6185271218753058\n",
            "iteration : 17000 loss : 0.6022458750439876\n",
            "iteration : 18000 loss : 0.5797104701761295\n",
            "iteration : 19000 loss : 0.5462637334154146\n",
            "iteration : 20000 loss : 0.512830977998761\n",
            "iteration : 21000 loss : 0.49240291901240074\n",
            "iteration : 22000 loss : 0.481853679426717\n",
            "iteration : 23000 loss : 0.4759230744953975\n",
            "iteration : 24000 loss : 0.4720309172464674\n",
            "iteration : 25000 loss : 0.46908590088064406\n",
            "iteration : 26000 loss : 0.46661087956268\n",
            "iteration : 27000 loss : 0.46438591591476175\n",
            "iteration : 28000 loss : 0.4623058909741165\n",
            "iteration : 29000 loss : 0.460318964920952\n",
            "iteration : 30000 loss : 0.45839806593179555\n",
            "iteration : 31000 loss : 0.4565275149552443\n",
            "iteration : 32000 loss : 0.45469715130085886\n",
            "iteration : 33000 loss : 0.45290030054511704\n",
            "iteration : 34000 loss : 0.4511336807434284\n",
            "iteration : 35000 loss : 0.44939811411154595\n",
            "iteration : 36000 loss : 0.447699222080574\n",
            "iteration : 37000 loss : 0.44604745512027805\n",
            "iteration : 38000 loss : 0.4444570476958292\n",
            "iteration : 39000 loss : 0.4429438722197052\n",
            "iteration : 40000 loss : 0.44152263485289395\n",
            "iteration : 41000 loss : 0.44020421535901644\n",
            "iteration : 42000 loss : 0.43899396979334127\n",
            "iteration : 43000 loss : 0.4378914243500146\n",
            "iteration : 44000 loss : 0.4368912133753138\n",
            "iteration : 45000 loss : 0.4359847027821242\n",
            "iteration : 46000 loss : 0.43516167788146876\n",
            "iteration : 47000 loss : 0.4344116831215931\n",
            "iteration : 48000 loss : 0.4337248769098645\n",
            "iteration : 49000 loss : 0.4330924580113884\n",
            "training accuracy:0.97\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OkOedIPBvNXO",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "outputId": "62e8b87c-b250-4ffa-e21e-3c1b004618ff",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1531823074622,
          "user_tz": -330,
          "elapsed": 79867,
          "user": {
            "displayName": "Rishhanth Maanav",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "116055284600069515854"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#Re-initialize model, train relu net\n",
        "\n",
        "model={}\n",
        "model['h'] = h # size of hidden layer 1\n",
        "model['h2']= h2# size of hidden layer 2\n",
        "model['W1']= 0.1 * np.random.randn(D,h)\n",
        "model['b1'] = np.zeros((1,h))\n",
        "model['W2'] = 0.1 * np.random.randn(h,h2)\n",
        "model['b2']= np.zeros((1,h2))\n",
        "model['W3'] = 0.1 * np.random.randn(h2,K)\n",
        "model['b3'] = np.zeros((1,K))\n",
        "\n",
        "(relu_array_1, relu_array_2, r_W1, r_W2,r_W3, r_b1, r_b2,r_b3) = three_layer_net('RELU', X,y,model, step_size=1e-1, reg=1e-3)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration : 0 loss : 1.1161878505332794\n",
            "iteration : 1000 loss : 0.2750473719183875\n",
            "iteration : 2000 loss : 0.152297391085688\n",
            "iteration : 3000 loss : 0.13637004985082735\n",
            "iteration : 4000 loss : 0.13085339604761725\n",
            "iteration : 5000 loss : 0.1278783087185082\n",
            "iteration : 6000 loss : 0.12595052350220226\n",
            "iteration : 7000 loss : 0.12459912899799813\n",
            "iteration : 8000 loss : 0.12350220538037718\n",
            "iteration : 9000 loss : 0.1225942191667746\n",
            "iteration : 10000 loss : 0.12183316159784802\n",
            "iteration : 11000 loss : 0.12120238213822204\n",
            "iteration : 12000 loss : 0.12064999237838803\n",
            "iteration : 13000 loss : 0.12016460077173034\n",
            "iteration : 14000 loss : 0.11973390288156796\n",
            "iteration : 15000 loss : 0.11934477378825432\n",
            "iteration : 16000 loss : 0.1190004088273231\n",
            "iteration : 17000 loss : 0.11869574387761535\n",
            "iteration : 18000 loss : 0.11842259785330743\n",
            "iteration : 19000 loss : 0.11816648588754518\n",
            "iteration : 20000 loss : 0.11793219180877032\n",
            "iteration : 21000 loss : 0.11771802461981928\n",
            "iteration : 22000 loss : 0.11752094533738822\n",
            "iteration : 23000 loss : 0.11733713100171589\n",
            "iteration : 24000 loss : 0.11716774683599085\n",
            "iteration : 25000 loss : 0.11701149261769349\n",
            "iteration : 26000 loss : 0.116863059134613\n",
            "iteration : 27000 loss : 0.1167208802261758\n",
            "iteration : 28000 loss : 0.11657374759428482\n",
            "iteration : 29000 loss : 0.11642691110007014\n",
            "iteration : 30000 loss : 0.11629276946275251\n",
            "iteration : 31000 loss : 0.1161640711615599\n",
            "iteration : 32000 loss : 0.11603227969555498\n",
            "iteration : 33000 loss : 0.1159054915790852\n",
            "iteration : 34000 loss : 0.11578298884723859\n",
            "iteration : 35000 loss : 0.11566921776371357\n",
            "iteration : 36000 loss : 0.11555980390120008\n",
            "iteration : 37000 loss : 0.11545403322867084\n",
            "iteration : 38000 loss : 0.11535612294801567\n",
            "iteration : 39000 loss : 0.1152637560525614\n",
            "iteration : 40000 loss : 0.11517710451443962\n",
            "iteration : 41000 loss : 0.11509379999158581\n",
            "iteration : 42000 loss : 0.11501422326618355\n",
            "iteration : 43000 loss : 0.11493689583958577\n",
            "iteration : 44000 loss : 0.11486124742108808\n",
            "iteration : 45000 loss : 0.11478712171992038\n",
            "iteration : 46000 loss : 0.1147163841299043\n",
            "iteration : 47000 loss : 0.11464813357123932\n",
            "iteration : 48000 loss : 0.11458281113135127\n",
            "iteration : 49000 loss : 0.11452164702668921\n",
            "training accuracy:0.9933333333333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bB9GjQWnvRCD",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 816
        },
        "outputId": "7b098450-c924-4978-f966-df5c4b8d7bde"
      },
      "cell_type": "code",
      "source": [
        "#Re-initialize model, train tanh net\n",
        "\n",
        "model={}\n",
        "model['h'] = h # size of hidden layer 1\n",
        "model['h2']= h2# size of hidden layer 2\n",
        "model['W1']= 0.1 * np.random.randn(D,h)\n",
        "model['b1'] = np.zeros((1,h))\n",
        "model['W2'] = 0.1 * np.random.randn(h,h2)\n",
        "model['b2']= np.zeros((1,h2))\n",
        "model['W3'] = 0.1 * np.random.randn(h2,K)\n",
        "model['b3'] = np.zeros((1,K))\n",
        "\n",
        "(tanh_array_1, tanh_array_2, t_W1, t_W2,t_W3, t_b1, t_b2,t_b3) = three_layer_net('TANH', X,y,model, step_size=1e-1, reg=1e-3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration : 0 loss : 1.1068763405022943\n",
            "iteration : 1000 loss : 0.765900305099139\n",
            "iteration : 2000 loss : 0.4343334057505796\n",
            "iteration : 3000 loss : 0.22102602425514706\n",
            "iteration : 4000 loss : 0.17630149048886173\n",
            "iteration : 5000 loss : 0.16120110789075096\n",
            "iteration : 6000 loss : 0.1540333097894043\n",
            "iteration : 7000 loss : 0.14995389923671046\n",
            "iteration : 8000 loss : 0.1475698759286856\n",
            "iteration : 9000 loss : 0.14600335540677895\n",
            "iteration : 10000 loss : 0.14484074848949718\n",
            "iteration : 11000 loss : 0.14391715055634402\n",
            "iteration : 12000 loss : 0.14315248607034425\n",
            "iteration : 13000 loss : 0.14250242535048072\n",
            "iteration : 14000 loss : 0.14193969836543047\n",
            "iteration : 15000 loss : 0.14144531574346847\n",
            "iteration : 16000 loss : 0.14100513992998645\n",
            "iteration : 17000 loss : 0.14060861470524466\n",
            "iteration : 18000 loss : 0.14024783674661545\n",
            "iteration : 19000 loss : 0.13991673977990238\n",
            "iteration : 20000 loss : 0.1396105838903235\n",
            "iteration : 21000 loss : 0.1393257816714214\n",
            "iteration : 22000 loss : 0.13905997472316955\n",
            "iteration : 23000 loss : 0.13881219388086213\n",
            "iteration : 24000 loss : 0.13858276838754063\n",
            "iteration : 25000 loss : 0.1383725565194345\n",
            "iteration : 26000 loss : 0.1381815508658317\n",
            "iteration : 27000 loss : 0.13800777673779102\n",
            "iteration : 28000 loss : 0.13784743936906896\n",
            "iteration : 29000 loss : 0.13769626192764078\n",
            "iteration : 30000 loss : 0.13755085423353522\n",
            "iteration : 31000 loss : 0.13740910541048212\n",
            "iteration : 32000 loss : 0.13726981066869703\n",
            "iteration : 33000 loss : 0.13713220837167583\n",
            "iteration : 34000 loss : 0.13699572940488333\n",
            "iteration : 35000 loss : 0.13685993178648884\n",
            "iteration : 36000 loss : 0.13672451675256087\n",
            "iteration : 37000 loss : 0.1365893505563261\n",
            "iteration : 38000 loss : 0.13645445725368094\n",
            "iteration : 39000 loss : 0.13631997913899038\n",
            "iteration : 40000 loss : 0.1361861202595961\n",
            "iteration : 41000 loss : 0.136053095438686\n",
            "iteration : 42000 loss : 0.13592110291294718\n",
            "iteration : 43000 loss : 0.13579032434723323\n",
            "iteration : 44000 loss : 0.13566093965311882\n",
            "iteration : 45000 loss : 0.13553313752362017\n",
            "iteration : 46000 loss : 0.1354071093879815\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pWJ8yas8vj7W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## The Vanishing Gradient Issue"
      ]
    },
    {
      "metadata": {
        "id": "DH7Wcv-Hvlqk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can use the sum of the magnitude of gradients for the weights between hidden layers as a cheap heuristic to measure speed of learning (you can also use the magnitude of gradients for each neuron in the hidden layer here). Intuitevely, when the magnitude of the gradients of the weight vectors or of each neuron are large, the net is learning faster. (NOTE: For our net, each hidden layer has the same number of neurons. If you want to play around with this, make sure to adjust the heuristic to account for the number of neurons in the layer)."
      ]
    },
    {
      "metadata": {
        "id": "jnXq-vKPvkWy",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "outputId": "62c28bb4-001a-48fe-ebd6-3d387d7810c9",
        "executionInfo": {
          "status": "error",
          "timestamp": 1531822681048,
          "user_tz": -330,
          "elapsed": 2938,
          "user": {
            "displayName": "Rishhanth Maanav",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "116055284600069515854"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "plt.plot(np.array(sigm_array_1))\n",
        "plt.plot(np.array(sigm_array_2))\n",
        "plt.title('Sum of magnitudes of gradients -- SIGM weights')\n",
        "plt.legend((\"sigm first layer\", \"sigm second layer\"))\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d06530e68271>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigm_array_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigm_array_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Sum of magnitudes of gradients -- SIGM weights'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sigm first layer\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sigm second layer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "ptKnXtCdvr3q",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "plt.plot(np.array(relu_array_1))\n",
        "plt.plot(np.array(relu_array_2))\n",
        "plt.title('Sum of magnitudes of gradients -- ReLU weights')\n",
        "plt.legend((\"relu first layer\", \"relu second layer\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WLHDF9lOvwif",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "plt.plot(np.array(tanh_array_1))\n",
        "plt.plot(np.array(tanh_array_2))\n",
        "plt.title('Sum of magnitudes of gradients -- TANH weights')\n",
        "plt.legend((\"tanh first layer\", \"tanh second layer\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QQQmG_DBvwj_",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Overlaying the two plots to compare\n",
        "plt.plot(np.array(relu_array_1))\n",
        "plt.plot(np.array(relu_array_2))\n",
        "plt.plot(np.array(sigm_array_1))\n",
        "plt.plot(np.array(sigm_array_2))\n",
        "plt.plot(np.array(tanh_array_1))\n",
        "plt.plot(np.array(tanh_array_2))\n",
        "plt.title('Sum of magnitudes of gradients -- hidden layer neurons')\n",
        "plt.legend((\"relu first layer\", \"relu second layer\",\"sigm first layer\", \"sigm second layer\",\"tanh first layer\",\"tanh second layer\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S0fw2FG5v55n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can see how well each classifier does in terms of distinguishing the toy data classes. As expected, since the ReLU net trains faster, for a set number of epochs it performs better compared to the sigmoid net"
      ]
    },
    {
      "metadata": {
        "id": "1HVsYKhNvwlk",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# plot the classifiers- SIGMOID\n",
        "h = 0.02\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                     np.arange(y_min, y_max, h))\n",
        "Z = np.dot(sigmoid(np.dot(sigmoid(np.dot(np.c_[xx.ravel(), yy.ravel()], s_W1) + s_b1), s_W2) + s_b2), s_W3) + s_b3\n",
        "Z = np.argmax(Z, axis=1)\n",
        "Z = Z.reshape(xx.shape)\n",
        "fig = plt.figure()\n",
        "plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.8)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
        "plt.xlim(xx.min(), xx.max())\n",
        "plt.ylim(yy.min(), yy.max())\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5bAFKBFMvwp6",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# plot the classifiers-- RELU\n",
        "h = 0.02\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                     np.arange(y_min, y_max, h))\n",
        "Z = np.dot(relu(np.dot(relu(np.dot(np.c_[xx.ravel(), yy.ravel()], r_W1) + r_b1), r_W2) + r_b2), r_W3) + r_b3\n",
        "Z = np.argmax(Z, axis=1)\n",
        "Z = Z.reshape(xx.shape)\n",
        "fig = plt.figure()\n",
        "plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.8)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
        "plt.xlim(xx.min(), xx.max())\n",
        "plt.ylim(yy.min(), yy.max())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XmnhXwPjwCQE",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "h = 0.02\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                     np.arange(y_min, y_max, h))\n",
        "Z = np.dot(tanh(np.dot(tanh(np.dot(np.c_[xx.ravel(), yy.ravel()], t_W1) + t_b1), t_W2) + t_b2), t_W3) + t_b3\n",
        "Z = np.argmax(Z, axis=1)\n",
        "Z = Z.reshape(xx.shape)\n",
        "fig = plt.figure()\n",
        "plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.8)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
        "plt.xlim(xx.min(), xx.max())\n",
        "plt.ylim(yy.min(), yy.max())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qvWJhpih1rZr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Overfitting and Underfitting"
      ]
    },
    {
      "metadata": {
        "id": "HfAwGQ3x11hJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![overfitting](https://cdn-images-1.medium.com/max/1125/1*_7OPgojau8hkiPUiHoGK_w.png)\n",
        "\n",
        "![overfittign](https://www.apixio.com/wp-content/uploads/2017/10/classification-with-overfitting-2.png)\n",
        "\n",
        "![over](https://raw.githubusercontent.com/alexeygrigorev/wiki-figures/master/ufrt/kddm/overfitting-logreg-ex.png)\n",
        "\n",
        "![fitting_curves](http://bioinfo.iric.ca/wpbioinfo/wp-content/uploads/2017/10/error_curves.png)\n",
        "\n",
        "![overfitting](http://srdas.github.io/DLBook/DL_images/UnderfittingOverfitting.png)"
      ]
    },
    {
      "metadata": {
        "id": "MIUR7PBY3cAo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Overfitting\n",
        "\n",
        "Overfitting refers to a model that models the training data too well.\n",
        "\n",
        "- Overfitting happens when a model learns the **detail and noise** in the training data to the extent that it **negatively** impacts the performance of the model on new data.\n",
        "- **Noise or random fluctuations** in the training data is picked up and **learned** as concepts by the model\n",
        "\n",
        "- Occurs when the **Representation Power** of the model is way too much when compared to the **actual complexity** needed to solve the problem."
      ]
    },
    {
      "metadata": {
        "id": "OLpeC9a74l3O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Underfitting\n",
        "\n",
        "Underfitting refers to a model that can neither model the training data nor generalize to new data.\n",
        "\n",
        "- An underfit machine learning model is not a suitable model and will be obvious as it will have poor performance on the training data.\n",
        "\n",
        "- Underfitting can easily be detected as the training performance will be low given a proper metric. So its obviously not suitable for deployment.\n",
        "\n",
        "- Increase the model's representation power by increasing the number of parameters to optimize incase of parametric models\n",
        "  - In neural Nets, increase the number of hidden layers and no of neurons per hidden layer. This increases the models representation capability.\n",
        "  "
      ]
    },
    {
      "metadata": {
        "id": "CUpvJ-6i9PBk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# How to avoid overfitting? \n",
        "\n",
        "> Regularization"
      ]
    },
    {
      "metadata": {
        "id": "sdxXCCiQ9dt0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Parameter Penalties\n",
        "\n",
        "- Adding Parameter norm penalty $\\Omega(\\theta)$ to the loss function\n",
        "- $\\Omega(\\theta)$ can be any function of $\\theta$, we will see about it in detail.\n",
        "\n",
        "\\begin{equation}\n",
        "\\vec J(\\theta: X,y) = J(\\theta : X, y) + \\alpha\\ \\Omega(\\theta) \\\\\n",
        "\\alpha\\ \\epsilon\\ [0, \\infty]\n",
        "\\end{equation}\n",
        "\n",
        "The term $\\alpha$ decides the amount of regularization term to add.\n",
        "\n",
        "| $\\alpha$ | Regularization |\n",
        "|---|---|\n",
        "|0| No Regularization whatsoever| \n",
        "| $\\downarrow$ | $\\downarrow$ |\n",
        "| $\\uparrow$ | $\\uparrow$ |\n",
        "| $\\infty$ | Infinite Penalty, $\\theta$ collapses to 0\n",
        "\n",
        "- In Neural nets, only $W$ parameters are subject to regularization, bias vectors ($b$) are not. This is because, \n",
        "  - Each weight $W_{ij}$ specifies how 2 variables interact. Fitting / finding the correct Weight value requires observing different values of the 2 variables at different conditions.\n",
        "  - Bias Vectors control only single variable.\n",
        "  - We might induce underfitting by including redularization in bias values."
      ]
    },
    {
      "metadata": {
        "id": "Hgn4SAppDyy1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### $L^2$ Parameter Regularization\n",
        "\n",
        "> Commonly known as **weight decay**.\n",
        "\n",
        "- This regularization strategy drives weights close to **origin**.\n",
        "\n",
        "- $\\Omega(\\theta) = \\frac{1}{2}||w||_2^2$\n",
        "\n",
        "- Also known as **ridge regression** or **Tikhonov Regression**\n",
        "\n",
        "Lets assume A, B are highly correlated features.\n",
        "\n",
        "> Corellation means A and B are couppled in a sense. +ve correlation and -ve correlation.\n",
        "\n",
        "They are so correlated so that we can assume A $\\approx$ B. \n",
        "\n",
        "These 2 being the features of the model, the weights will multiply them and we get\n",
        "\n",
        "$\n",
        " Y = W_aA + W_bB\n",
        "$\n",
        "\n",
        "Lets assume $W_a = 4, W_b = -2$, but since A and B are almost equal so\n",
        "\n",
        "\\begin{equation}\n",
        "Y = 4A - 2B \\approx 2A\n",
        "\\end{equation}\n",
        "\n",
        "But,\n",
        "\n",
        "\\begin{equation}\n",
        "Y = 10A - 8B \\approx 2A\n",
        "\\end{equation}\n",
        "\n",
        "and again,\n",
        "\n",
        "\\begin{equation}\n",
        "Y = 1000002A - 1000000B \\approx 2A\n",
        "\\end{equation}\n",
        "\n",
        "So you can see the difficulty in $optimization$. So this regularization basically says, if such a condition arises, choose the smallest (closest to origin) $W_a, W_b$ which satisfies the condition.\n"
      ]
    },
    {
      "metadata": {
        "id": "CXvP8w7-H9f6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### $L^1$ Regularization\n",
        "\n",
        "Here similar to $L^2$ Regularization, but the regularization function is different.\n",
        "\n",
        "\\begin{equation}\n",
        "\\Omega(\\theta) = ||w||_1 = \\sum_i|w_i|\n",
        "\\end{equation}\n",
        "\n",
        "Here the optimal solution for some paramters will be 0. This means, $L^1$ regularization will favour **sparse** solutions. \n",
        "\n",
        "Can be used in **feature selection** mechanism. If weights of some features reduces to 0, this means we can safely disregard those features from our model.\n",
        "  - Remember $W$ values for features implies the importance of the features in the prediction output. If the weight for a particular feature is 0, this means its nor important.\n"
      ]
    },
    {
      "metadata": {
        "id": "TBIzPubsLCoe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Norm Regularizations as Constraint Optimizations\n",
        "\n",
        "Recall,\n",
        "\n",
        "\\begin{equation}\n",
        "\\vec J(\\theta; X, y) = J(\\theta; X, y) + \\alpha\\ \\Omega(\\theta)\n",
        "\\end{equation}\n",
        "\n",
        "Also recalling Lagrange Multipliers, \n",
        "\n",
        "![lagrange_multipliers](https://i.stack.imgur.com/9NIoJ.png)\n",
        "\n",
        "![lagrange_multipliers](http://math.etsu.edu/multicalc/prealpha/chap2/chap2-9/10-8-20.gif)\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "B5-kcQSU-waE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# We'll now see an example of overfitting and another where we try to combat that using regularization"
      ]
    },
    {
      "metadata": {
        "id": "Gcv-ZlCp--DZ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dthGvXbN_IG3",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical\n",
        " \n",
        "print('Training data shape : ', train_images.shape, train_labels.shape)\n",
        " \n",
        "print('Testing data shape : ', test_images.shape, test_labels.shape)\n",
        " \n",
        "# Find the unique numbers from the train labels\n",
        "classes = np.unique(train_labels)\n",
        "nClasses = len(classes)\n",
        "print('Total number of outputs : ', nClasses)\n",
        "print('Output classes : ', classes)\n",
        " \n",
        "plt.figure(figsize=[10,5])\n",
        " \n",
        "# Display the first image in training data\n",
        "plt.subplot(121)\n",
        "plt.imshow(train_images[0,:,:], cmap='gray')\n",
        "plt.title(\"Ground Truth : {}\".format(train_labels[0]))\n",
        " \n",
        "# Display the first image in testing data\n",
        "plt.subplot(122)\n",
        "plt.imshow(test_images[0,:,:], cmap='gray')\n",
        "plt.title(\"Ground Truth : {}\".format(test_labels[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KRENCQ3-_Kt0",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Change from matrix to array of dimension 28x28 to array of dimention 784\n",
        "dimData = np.prod(train_images.shape[1:])\n",
        "train_data = train_images.reshape(train_images.shape[0], dimData)\n",
        "test_data = test_images.reshape(test_images.shape[0], dimData)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0cXV8R_q_M-K",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Change to float datatype\n",
        "train_data = train_data.astype('float32')\n",
        "test_data = test_data.astype('float32')\n",
        " \n",
        "# Scale the data to lie between 0 to 1\n",
        "train_data /= 255\n",
        "test_data /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "46HIlRDn_PSo",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Change to float datatype\n",
        "train_data = train_data.astype('float32')\n",
        "test_data = test_data.astype('float32')\n",
        " \n",
        "# Scale the data to lie between 0 to 1\n",
        "train_data /= 255\n",
        "test_data /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9qsxxDFN_RIf",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        " \n",
        "model = Sequential()\n",
        "model.add(Dense(512, activation='relu', input_shape=(dimData,)))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(nClasses, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mRKtJUtO_TPC",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PJYgU0MS_Uke",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "history = model.fit(train_data, train_labels_one_hot, batch_size=256, epochs=20, verbose=1, \n",
        "                   validation_data=(test_data, test_labels_one_hot))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lgs9evVP_Vno",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "[test_loss, test_acc] = model.evaluate(test_data, test_labels_one_hot)\n",
        "print(\"Evaluation result on Test Data : Loss = {}, accuracy = {}\".format(test_loss, test_acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZLMKzPOd_Z5C",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#Plot the Loss Curves\n",
        "plt.figure(figsize=[8,6])\n",
        "plt.plot(history.history['loss'],'r',linewidth=3.0)\n",
        "plt.plot(history.history['val_loss'],'b',linewidth=3.0)\n",
        "plt.legend(['Training loss', 'Validation Loss'],fontsize=18)\n",
        "plt.xlabel('Epochs ',fontsize=16)\n",
        "plt.ylabel('Loss',fontsize=16)\n",
        "plt.title('Loss Curves',fontsize=16)\n",
        " \n",
        "#Plot the Accuracy Curves\n",
        "plt.figure(figsize=[8,6])\n",
        "plt.plot(history.history['acc'],'r',linewidth=3.0)\n",
        "plt.plot(history.history['val_acc'],'b',linewidth=3.0)\n",
        "plt.legend(['Training Accuracy', 'Validation Accuracy'],fontsize=18)\n",
        "plt.xlabel('Epochs ',fontsize=16)\n",
        "plt.ylabel('Accuracy',fontsize=16)\n",
        "plt.title('Accuracy Curves',fontsize=16)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rsYJYkcX_eHA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## There is a clear sign of OverFitting. Why do you think so?\n",
        "\n",
        "Carefully see the Validation loss and Training loss curve. Validation loss decreases and then it gradually increases. This means that model is memorising the dataset, though in this case accuracy is much higher. \n",
        "\n",
        "** How to combat that?? **\n",
        "# Use Regularization !"
      ]
    },
    {
      "metadata": {
        "id": "evRF4jI6_fww",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Dropout\n",
        " \n",
        "model_reg = Sequential()\n",
        "model_reg.add(Dense(512, activation='relu', input_shape=(dimData,)))\n",
        "model_reg.add(Dropout(0.5))\n",
        "model_reg.add(Dense(512, activation='relu'))\n",
        "model_reg.add(Dropout(0.5))\n",
        "model_reg.add(Dense(nClasses, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YbMVvqfb_jaX",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "model_reg.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "history_reg = model_reg.fit(train_data, train_labels_one_hot, batch_size=256, epochs=20, verbose=1, \n",
        "                            validation_data=(test_data, test_labels_one_hot))\n",
        " \n",
        "#Plot the Loss Curves\n",
        "plt.figure(figsize=[8,6])\n",
        "plt.plot(history_reg.history['loss'],'r',linewidth=3.0)\n",
        "plt.plot(history_reg.history['val_loss'],'b',linewidth=3.0)\n",
        "plt.legend(['Training loss', 'Validation Loss'],fontsize=18)\n",
        "plt.xlabel('Epochs ',fontsize=16)\n",
        "plt.ylabel('Loss',fontsize=16)\n",
        "plt.title('Loss Curves',fontsize=16)\n",
        " \n",
        "#Plot the Accuracy Curves\n",
        "plt.figure(figsize=[8,6])\n",
        "plt.plot(history_reg.history['acc'],'r',linewidth=3.0)\n",
        "plt.plot(history_reg.history['val_acc'],'b',linewidth=3.0)\n",
        "plt.legend(['Training Accuracy', 'Validation Accuracy'],fontsize=18)\n",
        "plt.xlabel('Epochs ',fontsize=16)\n",
        "plt.ylabel('Accuracy',fontsize=16)\n",
        "plt.title('Accuracy Curves',fontsize=16)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OCP-IqMD_n9D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## What we note??\n",
        "\n",
        "* Validation loss is not increasing as it did before.\n",
        "* Difference between the validation and training accuracy is not that much\n",
        "\n",
        "This implies better generalisation and can work will on unseen data samples.\n"
      ]
    },
    {
      "metadata": {
        "id": "3tkYXx4et74o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Comparision of Various Optimizers: Stochastic Gradient Descent, RMSprop, Adam, Adagrad"
      ]
    },
    {
      "metadata": {
        "id": "J1ISUIQZm6TB",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2a2c3e8b-ce85-49b4-ac10-234bc0208c17",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1531761101645,
          "user_tz": -330,
          "elapsed": 8530,
          "user": {
            "displayName": "Rishhanth Maanav",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "116055284600069515854"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from keras.utils import np_utils\n",
        "import keras.callbacks as cb\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from keras.optimizers import *\n",
        "from keras.datasets import mnist"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "RmdLi4CGm_eK",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class LossHistory(cb.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.losses = []\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        batch_loss = logs.get('loss')\n",
        "        self.losses.append(batch_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KTDro5wJnClX",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def load_data():\n",
        "    print ('Loading data...')\n",
        "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "    X_train = X_train.astype('float32')\n",
        "    X_test = X_test.astype('float32')\n",
        "\n",
        "    X_train /= 255\n",
        "    X_test /= 255\n",
        "\n",
        "    y_train = np_utils.to_categorical(y_train, 10)\n",
        "    y_test = np_utils.to_categorical(y_test, 10)\n",
        "\n",
        "    X_train = np.reshape(X_train, (60000, 784))\n",
        "    X_test = np.reshape(X_test, (10000, 784))\n",
        "\n",
        "    print ('Data loaded.')\n",
        "    return [X_train, X_test, y_train, y_test]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XGSzKUf6nbLX",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def init_model(Optimizer,lrnr):\n",
        "    start_time = time.time()\n",
        "    print ('Compiling Model ... ')\n",
        "    model = Sequential()\n",
        "    model.add(Dense(500, input_dim=784))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(300))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(10))\n",
        "    model.add(Activation('softmax'))\n",
        "    \n",
        "    optim_dict = {}\n",
        "    optim_dict['RMSprop'] = RMSprop(lr = lrnr)\n",
        "    optim_dict['SGD'] = SGD(lr = lrnr)\n",
        "    optim_dict['Adam'] = Adam(lr = lrnr)\n",
        "    optim_dict['Adagrad'] = Adagrad(lr = lrnr)\n",
        "    optim = optim_dict[Optimizer]\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optim, metrics=['accuracy'])\n",
        "    print ('Model compield in {0} seconds'.format(time.time() - start_time))\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dZpfXbzYn-8z",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def run_network(data=None, model=None, epochs=20, batch=256, Optimizer = 'RMSprop',lrnr = 1e-2):\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "        if data is None:\n",
        "            X_train, X_test, y_train, y_test = load_data()\n",
        "        else:\n",
        "            X_train, X_test, y_train, y_test = data\n",
        "\n",
        "        if model is None:\n",
        "            model = init_model(Optimizer,lrnr)\n",
        "\n",
        "        history = LossHistory()\n",
        "        print(\"####### Optimizer being Used: \" + Optimizer)\n",
        "        print ('Training model...')\n",
        "        model.fit(X_train, y_train, epochs=epochs, batch_size=batch,\n",
        "                  callbacks=[history],\n",
        "                  validation_data=(X_test, y_test), verbose=2)\n",
        "\n",
        "        print (\"Training duration : {0}\".format(time.time() - start_time))\n",
        "        score = model.evaluate(X_test, y_test, batch_size=16)\n",
        "\n",
        "        print (\"Network's test score [loss, accuracy]: {0}\".format(score))\n",
        "        return model, history.losses\n",
        "    except KeyboardInterrupt:\n",
        "        print (' KeyboardInterrupt')\n",
        "        return model, history.losses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "77npFi_boEQG",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def plot_losses(losses):\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    ax.plot(losses)\n",
        "    ax.set_title('Loss per batch')\n",
        "    fig.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KVfSCHDNoQS6",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 3471
        },
        "outputId": "153a975f-ddf7-41b0-fdaa-5bf847494971",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1531761247397,
          "user_tz": -330,
          "elapsed": 140286,
          "user": {
            "displayName": "Rishhanth Maanav",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "116055284600069515854"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "model1,losses1 = run_network(Optimizer='SGD')\n",
        "model2,losses2 = run_network(Optimizer='RMSprop')\n",
        "model3,losses3 = run_network(Optimizer='Adam')\n",
        "model4,losses4 = run_network(Optimizer='Adagrad')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "Data loaded.\n",
            "Compiling Model ... \n",
            "Model compield in 0.1579577922821045 seconds\n",
            "####### Optimizer being Used: SGD\n",
            "Training model...\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            " - 2s - loss: 1.7671 - acc: 0.4466 - val_loss: 1.0192 - val_acc: 0.8004\n",
            "Epoch 2/20\n",
            " - 1s - loss: 0.9803 - acc: 0.7141 - val_loss: 0.5928 - val_acc: 0.8565\n",
            "Epoch 3/20\n",
            " - 1s - loss: 0.7243 - acc: 0.7842 - val_loss: 0.4638 - val_acc: 0.8801\n",
            "Epoch 4/20\n",
            " - 1s - loss: 0.6136 - acc: 0.8156 - val_loss: 0.4024 - val_acc: 0.8920\n",
            "Epoch 5/20\n",
            " - 1s - loss: 0.5440 - acc: 0.8377 - val_loss: 0.3664 - val_acc: 0.8982\n",
            "Epoch 6/20\n",
            " - 1s - loss: 0.5036 - acc: 0.8503 - val_loss: 0.3427 - val_acc: 0.9035\n",
            "Epoch 7/20\n",
            " - 1s - loss: 0.4707 - acc: 0.8590 - val_loss: 0.3233 - val_acc: 0.9079\n",
            "Epoch 8/20\n",
            " - 1s - loss: 0.4443 - acc: 0.8685 - val_loss: 0.3076 - val_acc: 0.9121\n",
            "Epoch 9/20\n",
            " - 1s - loss: 0.4222 - acc: 0.8750 - val_loss: 0.2944 - val_acc: 0.9154\n",
            "Epoch 10/20\n",
            " - 1s - loss: 0.4041 - acc: 0.8796 - val_loss: 0.2837 - val_acc: 0.9188\n",
            "Epoch 11/20\n",
            " - 1s - loss: 0.3869 - acc: 0.8848 - val_loss: 0.2740 - val_acc: 0.9211\n",
            "Epoch 12/20\n",
            " - 1s - loss: 0.3749 - acc: 0.8908 - val_loss: 0.2655 - val_acc: 0.9234\n",
            "Epoch 13/20\n",
            " - 1s - loss: 0.3625 - acc: 0.8941 - val_loss: 0.2566 - val_acc: 0.9250\n",
            "Epoch 14/20\n",
            " - 1s - loss: 0.3498 - acc: 0.8980 - val_loss: 0.2493 - val_acc: 0.9279\n",
            "Epoch 15/20\n",
            " - 1s - loss: 0.3394 - acc: 0.9010 - val_loss: 0.2432 - val_acc: 0.9299\n",
            "Epoch 16/20\n",
            " - 1s - loss: 0.3329 - acc: 0.9027 - val_loss: 0.2370 - val_acc: 0.9308\n",
            "Epoch 17/20\n",
            " - 1s - loss: 0.3202 - acc: 0.9056 - val_loss: 0.2305 - val_acc: 0.9324\n",
            "Epoch 18/20\n",
            " - 1s - loss: 0.3144 - acc: 0.9080 - val_loss: 0.2248 - val_acc: 0.9346\n",
            "Epoch 19/20\n",
            " - 1s - loss: 0.3068 - acc: 0.9104 - val_loss: 0.2204 - val_acc: 0.9362\n",
            "Epoch 20/20\n",
            " - 1s - loss: 0.2972 - acc: 0.9124 - val_loss: 0.2151 - val_acc: 0.9370\n",
            "Training duration : 30.938060998916626\n",
            " 8816/10000 [=========================>....] - ETA: 0s"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 1s 133us/step\n",
            "Network's test score [loss, accuracy]: [0.21510502181351185, 0.937]\n",
            "Loading data...\n",
            "Data loaded.\n",
            "Compiling Model ... \n",
            "Model compield in 0.22509074211120605 seconds\n",
            "####### Optimizer being Used: RMSprop\n",
            "Training model...\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            " - 2s - loss: 2.3645 - acc: 0.7604 - val_loss: 0.2360 - val_acc: 0.9344\n",
            "Epoch 2/20\n",
            " - 2s - loss: 0.3112 - acc: 0.9241 - val_loss: 0.2191 - val_acc: 0.9448\n",
            "Epoch 3/20\n",
            " - 2s - loss: 0.2915 - acc: 0.9346 - val_loss: 0.2198 - val_acc: 0.9503\n",
            "Epoch 4/20\n",
            " - 2s - loss: 0.2892 - acc: 0.9377 - val_loss: 0.1691 - val_acc: 0.9615\n",
            "Epoch 5/20\n",
            " - 2s - loss: 0.2993 - acc: 0.9401 - val_loss: 0.1728 - val_acc: 0.9633\n",
            "Epoch 6/20\n",
            " - 2s - loss: 0.3019 - acc: 0.9435 - val_loss: 0.1949 - val_acc: 0.9580\n",
            "Epoch 7/20\n",
            " - 2s - loss: 0.3089 - acc: 0.9446 - val_loss: 0.2113 - val_acc: 0.9592\n",
            "Epoch 8/20\n",
            " - 2s - loss: 0.3112 - acc: 0.9446 - val_loss: 0.1913 - val_acc: 0.9640\n",
            "Epoch 9/20\n",
            " - 2s - loss: 0.3238 - acc: 0.9472 - val_loss: 0.2244 - val_acc: 0.9597\n",
            "Epoch 10/20\n",
            " - 2s - loss: 0.3093 - acc: 0.9486 - val_loss: 0.2045 - val_acc: 0.9632\n",
            "Epoch 11/20\n",
            " - 2s - loss: 0.3238 - acc: 0.9478 - val_loss: 0.2120 - val_acc: 0.9670\n",
            "Epoch 12/20\n",
            " - 2s - loss: 0.3249 - acc: 0.9494 - val_loss: 0.2061 - val_acc: 0.9673\n",
            "Epoch 13/20\n",
            " - 2s - loss: 0.3414 - acc: 0.9493 - val_loss: 0.2301 - val_acc: 0.9666\n",
            "Epoch 14/20\n",
            " - 2s - loss: 0.3863 - acc: 0.9499 - val_loss: 0.2377 - val_acc: 0.9658\n",
            "Epoch 15/20\n",
            " - 2s - loss: 0.3689 - acc: 0.9496 - val_loss: 0.2419 - val_acc: 0.9694\n",
            "Epoch 16/20\n",
            " - 2s - loss: 0.3765 - acc: 0.9497 - val_loss: 0.2354 - val_acc: 0.9711\n",
            "Epoch 17/20\n",
            " - 2s - loss: 0.3701 - acc: 0.9521 - val_loss: 0.2294 - val_acc: 0.9703\n",
            "Epoch 18/20\n",
            " - 2s - loss: 0.3832 - acc: 0.9506 - val_loss: 0.2253 - val_acc: 0.9668\n",
            "Epoch 19/20\n",
            " - 2s - loss: 0.3934 - acc: 0.9523 - val_loss: 0.2585 - val_acc: 0.9700\n",
            "Epoch 20/20\n",
            " - 2s - loss: 0.3848 - acc: 0.9526 - val_loss: 0.2594 - val_acc: 0.9703\n",
            "Training duration : 34.21602439880371\n",
            " 9536/10000 [===========================>..] - ETA: 0s"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 1s 130us/step\n",
            "Network's test score [loss, accuracy]: [0.259412504883776, 0.9703]\n",
            "Loading data...\n",
            "Data loaded.\n",
            "Compiling Model ... \n",
            "Model compield in 0.14969754219055176 seconds\n",
            "####### Optimizer being Used: Adam\n",
            "Training model...\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            " - 2s - loss: 0.3653 - acc: 0.8904 - val_loss: 0.1559 - val_acc: 0.9538\n",
            "Epoch 2/20\n",
            " - 2s - loss: 0.2541 - acc: 0.9281 - val_loss: 0.1254 - val_acc: 0.9649\n",
            "Epoch 3/20\n",
            " - 2s - loss: 0.2396 - acc: 0.9347 - val_loss: 0.1349 - val_acc: 0.9620\n",
            "Epoch 4/20\n",
            " - 2s - loss: 0.2359 - acc: 0.9379 - val_loss: 0.1396 - val_acc: 0.9607\n",
            "Epoch 5/20\n",
            " - 2s - loss: 0.2196 - acc: 0.9424 - val_loss: 0.1400 - val_acc: 0.9644\n",
            "Epoch 6/20\n",
            " - 2s - loss: 0.2348 - acc: 0.9407 - val_loss: 0.1371 - val_acc: 0.9634\n",
            "Epoch 7/20\n",
            " - 2s - loss: 0.2251 - acc: 0.9443 - val_loss: 0.1524 - val_acc: 0.9619\n",
            "Epoch 8/20\n",
            " - 2s - loss: 0.2204 - acc: 0.9452 - val_loss: 0.1371 - val_acc: 0.9667\n",
            "Epoch 9/20\n",
            " - 2s - loss: 0.2064 - acc: 0.9490 - val_loss: 0.1353 - val_acc: 0.9684\n",
            "Epoch 10/20\n",
            " - 2s - loss: 0.2132 - acc: 0.9487 - val_loss: 0.1271 - val_acc: 0.9703\n",
            "Epoch 11/20\n",
            " - 2s - loss: 0.2039 - acc: 0.9502 - val_loss: 0.1359 - val_acc: 0.9683\n",
            "Epoch 12/20\n",
            " - 2s - loss: 0.2073 - acc: 0.9510 - val_loss: 0.1252 - val_acc: 0.9705\n",
            "Epoch 13/20\n",
            " - 2s - loss: 0.2103 - acc: 0.9494 - val_loss: 0.1421 - val_acc: 0.9680\n",
            "Epoch 14/20\n",
            " - 2s - loss: 0.1871 - acc: 0.9541 - val_loss: 0.1409 - val_acc: 0.9666\n",
            "Epoch 15/20\n",
            " - 2s - loss: 0.1960 - acc: 0.9548 - val_loss: 0.1379 - val_acc: 0.9690\n",
            "Epoch 16/20\n",
            " - 2s - loss: 0.1895 - acc: 0.9543 - val_loss: 0.1411 - val_acc: 0.9692\n",
            "Epoch 17/20\n",
            " - 2s - loss: 0.1866 - acc: 0.9550 - val_loss: 0.1400 - val_acc: 0.9671\n",
            "Epoch 18/20\n",
            " - 2s - loss: 0.1977 - acc: 0.9560 - val_loss: 0.1426 - val_acc: 0.9678\n",
            "Epoch 19/20\n",
            " - 2s - loss: 0.1921 - acc: 0.9550 - val_loss: 0.1437 - val_acc: 0.9700\n",
            "Epoch 20/20\n",
            " - 2s - loss: 0.1894 - acc: 0.9565 - val_loss: 0.1380 - val_acc: 0.9724\n",
            "Training duration : 36.44550323486328\n",
            " 9216/10000 [==========================>...] - ETA: 0s"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 1s 133us/step\n",
            "Network's test score [loss, accuracy]: [0.1379884784717869, 0.9724]\n",
            "Loading data...\n",
            "Data loaded.\n",
            "Compiling Model ... \n",
            "Model compield in 0.24301552772521973 seconds\n",
            "####### Optimizer being Used: Adagrad\n",
            "Training model...\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            " - 2s - loss: 0.3534 - acc: 0.8954 - val_loss: 0.1362 - val_acc: 0.9580\n",
            "Epoch 2/20\n",
            " - 2s - loss: 0.1589 - acc: 0.9531 - val_loss: 0.1043 - val_acc: 0.9669\n",
            "Epoch 3/20\n",
            " - 2s - loss: 0.1250 - acc: 0.9624 - val_loss: 0.0880 - val_acc: 0.9716\n",
            "Epoch 4/20\n",
            " - 2s - loss: 0.1071 - acc: 0.9680 - val_loss: 0.0830 - val_acc: 0.9741\n",
            "Epoch 5/20\n",
            " - 2s - loss: 0.0933 - acc: 0.9718 - val_loss: 0.0772 - val_acc: 0.9766\n",
            "Epoch 6/20\n",
            " - 2s - loss: 0.0834 - acc: 0.9752 - val_loss: 0.0716 - val_acc: 0.9774\n",
            "Epoch 7/20\n",
            " - 2s - loss: 0.0767 - acc: 0.9763 - val_loss: 0.0678 - val_acc: 0.9791\n",
            "Epoch 8/20\n",
            " - 2s - loss: 0.0696 - acc: 0.9789 - val_loss: 0.0658 - val_acc: 0.9791\n",
            "Epoch 9/20\n",
            " - 2s - loss: 0.0645 - acc: 0.9806 - val_loss: 0.0642 - val_acc: 0.9798\n",
            "Epoch 10/20\n",
            " - 2s - loss: 0.0587 - acc: 0.9817 - val_loss: 0.0628 - val_acc: 0.9800\n",
            "Epoch 11/20\n",
            " - 2s - loss: 0.0549 - acc: 0.9832 - val_loss: 0.0636 - val_acc: 0.9798\n",
            "Epoch 12/20\n",
            " - 2s - loss: 0.0524 - acc: 0.9839 - val_loss: 0.0608 - val_acc: 0.9811\n",
            "Epoch 13/20\n",
            " - 2s - loss: 0.0492 - acc: 0.9843 - val_loss: 0.0607 - val_acc: 0.9808\n",
            "Epoch 14/20\n",
            " - 2s - loss: 0.0480 - acc: 0.9849 - val_loss: 0.0588 - val_acc: 0.9817\n",
            "Epoch 15/20\n",
            " - 2s - loss: 0.0447 - acc: 0.9862 - val_loss: 0.0577 - val_acc: 0.9825\n",
            "Epoch 16/20\n",
            " - 2s - loss: 0.0413 - acc: 0.9870 - val_loss: 0.0584 - val_acc: 0.9828\n",
            "Epoch 17/20\n",
            " - 2s - loss: 0.0403 - acc: 0.9872 - val_loss: 0.0596 - val_acc: 0.9824\n",
            "Epoch 18/20\n",
            " - 2s - loss: 0.0385 - acc: 0.9875 - val_loss: 0.0584 - val_acc: 0.9824\n",
            "Epoch 19/20\n",
            " - 2s - loss: 0.0365 - acc: 0.9885 - val_loss: 0.0575 - val_acc: 0.9831\n",
            "Epoch 20/20\n",
            " - 2s - loss: 0.0346 - acc: 0.9889 - val_loss: 0.0550 - val_acc: 0.9848\n",
            "Training duration : 32.48259496688843\n",
            " 9344/10000 [===========================>..] - ETA: 0s"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 1s 131us/step\n",
            "Network's test score [loss, accuracy]: [0.05498526737435022, 0.9848]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9uv-RZBHooTA",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "outputId": "2509eb4a-17aa-4d91-95ba-b64c49066642",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1531761248543,
          "user_tz": -330,
          "elapsed": 1112,
          "user": {
            "displayName": "Rishhanth Maanav",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "116055284600069515854"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "plt.title(\"Comparison of Various Optimizers' Performance\")\n",
        "plt.plot(losses1)\n",
        "plt.plot(losses2)\n",
        "plt.plot(losses3)\n",
        "plt.plot(losses4)\n",
        "plt.legend(['SGD','RMSProp','Adam','Adagrad'])\n",
        "plt.show()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAd8AAAFZCAYAAADZ6SWdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fc6c4ff6ef0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "B3vv0ICWEe5N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Time to train your own neural network !!!"
      ]
    },
    {
      "metadata": {
        "id": "clk5SDoU_sw4",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install matplotlib\n",
        "\n",
        "from keras.datasets import cifar10\n",
        "import numpy as np\n",
        "(train_images, train_labels), (test_images, test_labels)  = cifar10.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Gmer73id_vpo",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical\n",
        " \n",
        "print('Training data shape : ', train_images.shape, train_labels.shape)\n",
        " \n",
        "print('Testing data shape : ', test_images.shape, test_labels.shape)\n",
        " \n",
        "# Find the unique numbers from the train labels\n",
        "classes = np.unique(train_labels)\n",
        "nClasses = len(classes)\n",
        "print('Total number of outputs : ', nClasses)\n",
        "print('Output classes : ', classes)\n",
        " \n",
        "plt.figure(figsize=[5,2])\n",
        " \n",
        "# Display the first image in training data\n",
        "plt.subplot(121)\n",
        "plt.imshow(train_images[0,:,:], cmap='gray')\n",
        "plt.title(\"Ground Truth : {}\".format(train_labels[0]))\n",
        " \n",
        "# Display the first image in testing data\n",
        "plt.subplot(122)\n",
        "plt.imshow(test_images[0,:,:], cmap='gray')\n",
        "plt.title(\"Ground Truth : {}\".format(test_labels[0]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1w-ldsNx_4b4",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Change from matrix to array of dimension 28x28 to array of dimention 784\n",
        "dimData = np.prod(train_images.shape[1:])\n",
        "train_data = train_images.reshape(train_images.shape[0], dimData)\n",
        "test_data = test_images.reshape(test_images.shape[0], dimData)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iYqqvCQ8_5Qt",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Change to float datatype\n",
        "train_data = train_data.astype('float32')\n",
        "test_data = test_data.astype('float32')\n",
        " \n",
        "# Scale the data to lie between 0 to 1\n",
        "train_data /= 255\n",
        "test_data /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nNPTvC8S_7Gx",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Change the labels from integer to categorical data\n",
        "train_labels_one_hot = to_categorical(train_labels)\n",
        "test_labels_one_hot = to_categorical(test_labels)\n",
        " \n",
        "# Display the change for category label using one-hot encoding\n",
        "print('Original label 0 : ', train_labels[0])\n",
        "print('After conversion to categorical ( one-hot ) : ', train_labels_one_hot[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fpVcf7kSADBU",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        " \n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(720, activation='sigmoid', input_shape=(dimData,)))\n",
        "model.add(Dense(720, activation='relu', input_shape=(dimData,)))\n",
        "model.add(Dense(720, activation='sigmoid', input_shape=(dimData,)))\n",
        "model.add(Dense(720, activation='relu', input_shape=(dimData,)))\n",
        "model.add(Dense(720, activation='sigmoid', input_shape=(dimData,)))\n",
        "model.add(Dense(720, activation='relu', input_shape=(dimData,)))\n",
        "model.add(Dense(720, activation='sigmoid', input_shape=(dimData,)))\n",
        "model.add(Dense(720, activation='relu', input_shape=(dimData,)))\n",
        "model.add(Dense(720, activation='sigmoid', input_shape=(dimData,)))\n",
        "model.add(Dense(720, activation='relu', input_shape=(dimData,)))\n",
        "model.add(Dense(720, activation='relu'))\n",
        "model.add(Dense(nClasses, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1SglB_EnAFSQ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jEqD8W6KAGHa",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "history = model.fit(train_data, train_labels_one_hot, batch_size=256, epochs=20, verbose=1, \n",
        "                   validation_data=(test_data, test_labels_one_hot))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GqRpjKusAKUA",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "[test_loss, test_acc] = model.evaluate(test_data, test_labels_one_hot)\n",
        "print(\"Evaluation result on Test Data : Loss = {}, accuracy = {}\".format(test_loss, test_acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mKJ78Hi4AOjq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Try to maximize the test accuracy !!! Take it as a challenge. (Tuning the parameters is basically an ART :p)"
      ]
    },
    {
      "metadata": {
        "id": "OMtvA5tzwCxo",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}