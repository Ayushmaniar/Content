{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CVIG Summer School 2018 Notes - Session 3",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "OKSmiZ703pCo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](http://i63.tinypic.com/352kyeb.jpg)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "nFzcZohQnNrx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# What are Features?\n",
        "\n",
        "In computer vision and image processing, a feature is a piece of information which is relevant for solving the computational task related to a certain application.\n",
        " \n",
        "There is no universal or exact definition of what constitutes a feature, and the exact definition often depends on the problem or the type of application. Given that, a feature is defined as an \"interesting\" part of an image, and features are used as a starting point for many computer vision algorithms.\n",
        "\n",
        "Features maybe specific structures in the image such as points, edges or objects. Features may also be the result of a general neighbourhood operation or feature detection applied to the image.\n",
        "\n",
        "Feature detection includes methods for computing abstractions of image information and making local decisions at every image point whether there is an image feature of a given type at that point or not. The resulting features will be subsets of the image domain, often in the form of isolated points, continuous curves or connected regions.\n",
        "\n",
        "Feature detection is a low-level image processing operation. That is, it is usually performed as the first operation on an image, and examines every pixel to see if there is a feature present at that pixel. If this is part of a larger algorithm, then the algorithm will typically only examine the image in the region of the features. As a built-in pre-requisite to feature detection, the input image is usually smoothed by a Gaussian kernel in a scale-space representation and one or several feature images are computed, often expressed in terms of local image derivatives operations.\n",
        "\n",
        "The feature concept is very general and the choice of features in a particular computer vision system may be highly dependent on the specific problem at hand.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "LIY9s_Lys7kV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Basic Image Features\n",
        "\n",
        "## Edges\n",
        "\n",
        "Edges are points where there is a boundary (or an edge) between two image regions. In general, an edge can be of almost arbitrary shape, and may include junctions. In practice, edges are usually defined as sets of points in the image which have a strong gradient magnitude. Furthermore, some common algorithms will then chain high gradient points together to form a more complete description of an edge. These algorithms usually place some constraints on the properties of an edge, such as shape, smoothness, and gradient value.\n",
        "\n",
        "Locally, edges have a one-dimensional structure.\n",
        "\n",
        "## Corners/Interest Points\n",
        "\n",
        "The terms corners and interest points are used somewhat interchangeably and refer to point-like features in an image, which have a local two dimensional structure. The name \"Corner\" arose since early algorithms first performed edge detection, and then analysed the edges to find rapid changes in direction (corners). These algorithms were then developed so that explicit edge detection was no longer required, for instance by looking for high levels of curvature in the image gradient. It was then noticed that the so-called corners were also being detected on parts of the image which were not corners in the traditional sense (for instance a small bright spot on a dark background may be detected). These points are frequently known as interest points, but the term \"corner\" is used by tradition.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "qUtsk1zNoi9t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Image Gradients\n",
        "\n",
        "One of the most basic and important convolutions is the computation of derivatives (or\n",
        "approximations to them).  There are many ways to do this, but only a few are well suited\n",
        "to a given situation. \n",
        "\n",
        "Let’s imagine we want to detect the edges present in the image. You can easily notice that in an edge, the pixel intensity changes in a notorious way. A good way to express changes is by using derivatives. A high change in gradient indicates a major change in the image. \n",
        "\n",
        "Let’s assume we have a 1D-image. An edge is shown by the “jump” in intensity as shown in the plot below:\n",
        "\n",
        "![alt text](https://docs.opencv.org/2.4.13.4/_images/Sobel_Derivatives_Tutorial_Theory_Intensity_Function.jpg)\n",
        "\n",
        "The edge “jump” can be seen more easily if we take the first derivative (actually, here it appears as a maximum)\n",
        "\n",
        "![alt text](https://docs.opencv.org/2.4.13.4/_images/Sobel_Derivatives_Tutorial_Theory_dIntensity_Function.jpg)\n",
        "\n",
        "So, from the explanation above, we can deduce that a method to detect edges in an image can be performed by locating pixel locations where the gradient is higher than its neighbors (or to generalize, higher than a threshold).\n",
        "\n",
        "OpenCV provides three types of gradient filters or High-pass filters.\n",
        "\n",
        "*   Sobel\n",
        "*   Scharr\n",
        "*   Laplacian\n",
        "\n",
        "We will see each one of them.\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "hwnzZTjjEeRr",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "69895725-95bc-4e2e-f80d-5b6dd84e18e3",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530794623203,
          "user_tz": -330,
          "elapsed": 11173,
          "user": {
            "displayName": "Siva Subramaniyan",
            "photoUrl": "//lh3.googleusercontent.com/-CeVc5JSOZSg/AAAAAAAAAAI/AAAAAAAAAA0/h9GqyBAovnQ/s50-c-k-no/photo.jpg",
            "userId": "102411634545146112960"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-95ff00e4-498c-4393-8f4a-a9b9b88c8486\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-95ff00e4-498c-4393-8f4a-a9b9b88c8486\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving sudoku.jpg to sudoku.jpg\n",
            "User uploaded file \"sudoku.jpg\" with length 53178 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ir7Hfb7q0fF5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Sobel \n",
        "\n",
        "The Sobel Operator is a discrete differentiation operator. It computes an approximation of the gradient of an image intensity function.\n",
        "The Sobel Operator combines Gaussian smoothing and differentiation.\n",
        "\n",
        "Assuming that the image to be operated is I:\n",
        "\n",
        "We calculate two derivatives:\n",
        "\n",
        "**Horizontal changes**: This is computed by convolving I with a kernel of odd size as given below. For a kernel size of 3, $ G{x} $ would be computed as the convolution of the following matrix with I.\n",
        "\n",
        "\\begin{bmatrix}\n",
        "-1 & 0 & +1  \\\\\n",
        "-2 & 0 & +2  \\\\\n",
        "-1 & 0 & +1\n",
        "\\end{bmatrix}\n",
        "\n",
        "**Vertical changes**: This is computed by convolving I with a kernel of odd size as given below. For a kernel size of 3, $G{y}$ would be computed as the convolution of the following matrix with I.\n",
        "\n",
        "\\begin{bmatrix}\n",
        "-1 & -2 & -1  \\\\\n",
        "0 & 0 & 0  \\\\\n",
        "+1 & +2 & +1\n",
        "\\end{bmatrix}\n",
        "\n",
        "At each point of the image we calculate an approximation of the gradient at that point by combining both the results above:\n",
        "\n",
        "$$ \\begin{equation*} G = \\sqrt{ G_{x}^{2} + G_{y}^{2} }\\end{equation*} $$\n",
        "\n",
        "Although sometimes the following simpler equation can also be used:\n",
        "\n",
        "$$ \\begin{equation*} G = |G{x}| + |G{y}| \\end{equation*} $$\n",
        "\n",
        "\n",
        "\n",
        "**Python**: \n",
        "\n",
        "```\n",
        "cv2.Sobel(src, ddepth, xorder, yorder, ksize)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "**Parameters:**\n",
        "\n",
        "*   src – input image.\n",
        "*   ddepth –\n",
        "output image depth; the following combinations of ddepth are supported:\n",
        "    *  CV_8U\n",
        "    *  CV_16U/CV_16S\n",
        "    *  CV_32F\n",
        "    *  CV_64F\n",
        "*  xorder – order of the derivative x.\n",
        "*  yorder – order of the derivative y.\n",
        "*  ksize – size of the extended Sobel kernel; it must be 1, 3, 5, or 7.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "8P5ySsdR9LmY",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "a0bf6c81-b120-4491-e943-30076dc4408d"
      },
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "img = cv2.imread('sudoku.jpg')\n",
        "sobelx = cv2.Sobel(img,cv2.CV_64F,1,0,ksize=3)\n",
        "sobely = cv2.Sobel(img,cv2.CV_64F,0,1,ksize=3)\n",
        "\n",
        "cv2.imshow('Sobel Horizontal',sobelx)\n",
        "cv2.imshow('Sobel Vertical',sobely)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3-LC5CkyApQF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Scharr\n",
        "\n",
        "When the size of the kernel is 3, the Sobel kernel shown above may produce noticeable inaccuracies (after all, Sobel is only an approximation of the derivative). OpenCV addresses this inaccuracy for kernels of size 3 by using the Scharr function. This is as fast but more accurate than the standard Sobel function. It implements the following kernels:\n",
        "\n",
        "**Horizontal changes**:  $G{x}$ would be computed as the convolution of the following matrix with I.\n",
        "\n",
        "\\begin{bmatrix}\n",
        "-3 & 0 & +3  \\\\\n",
        "-10 & 0 & +10  \\\\\n",
        "-3 & 0 & +3\n",
        "\\end{bmatrix}\n",
        "\n",
        "**Vertical changes**: $G{y}$ would be computed as the convolution of the following matrix with I.\n",
        "\n",
        "\\begin{bmatrix}\n",
        "-3 & -10 & -3  \\\\\n",
        "0 & 0 & 0  \\\\\n",
        "+3 & +10 & +3\n",
        "\\end{bmatrix}\n",
        "\n",
        "**Python**: \n",
        "\n",
        "```\n",
        "cv2.Scharr(src, ddepth, xorder, yorder)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "**Parameters:**\n",
        "\n",
        "*   src – input image.\n",
        "*   ddepth –\n",
        "output image depth; the following combinations of ddepth are supported:\n",
        "    *  CV_8U\n",
        "    *  CV_16U/CV_16S\n",
        "    *  CV_32F\n",
        "    *  CV_64F\n",
        "*  xorder – order of the derivative x.\n",
        "*  yorder – order of the derivative y.\n",
        "---"
      ]
    },
    {
      "metadata": {
        "id": "-c2xhPepDW8e",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "e96b38f9-47e3-4ab2-b507-d721e54ee9d7"
      },
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "img = cv2.imread('sudoku.jpg')\n",
        "scharrx = cv2.Scharr(img,cv2.CV_64F,1,0)\n",
        "scharry = cv2.Scharr(img,cv2.CV_64F,0,1)\n",
        "\n",
        "cv2.imshow('Scharr Horizontal',scharrx)\n",
        "cv2.imshow('Scharr Vertical',scharry)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j3xnpeQtAp4-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Laplacian\n",
        "\n",
        "In the previous section we learned how to use the Sobel Operator. It was based on the fact that in the edge area, the pixel intensity shows a sudden jump. By getting the first derivative of the intensity, we observed that an edge is characterized by a maximum, as it can be seen in the figure:\n",
        "\n",
        "![alt text](https://docs.opencv.org/2.4.13.4/_images/Laplace_Operator_Tutorial_Theory_Previous.jpg)\n",
        "\n",
        "What happens if we take the second derivative?\n",
        "\n",
        "![alt text](https://docs.opencv.org/3.4/Laplace_Operator_Tutorial_Theory_ddIntensity.jpg)\n",
        "\n",
        "You can observe that the second derivative is zero! So, we can also use this criterion to attempt to detect edges in an image. \n",
        "\n",
        "However, note that zeros will not only appear in edges, they can appear in other meaningless locations too due to noise. This can be easily solved by applying filters.\n",
        "\n",
        "From the explanation above, we deduce that the second derivative can be used to detect edges. Since images are “2D”, we would need to take the derivative in both dimensions.\n",
        "\n",
        "The Laplacian operator is defined by:\n",
        "$$ \\begin{equation*} Laplace(f) = \\dfrac{\\partial^{2} f}{\\partial x^{2}} + \\dfrac{\\partial^{2} f}{\\partial y^{2}} \\end{equation*} $$\n",
        "\n",
        "Since the Laplacian uses the gradient of images, it calls internally the Sobel operator to perform its computation.\n",
        "\n",
        "But when ksize==1, the Laplacian is computed by convolving the image with the following 3X3 kernel, as double gradient for a ksize of 1 returns a zero everywhere.\n",
        "\n",
        "\\begin{bmatrix}\n",
        "0 & 1 & 0  \\\\\n",
        "1 & -4 & 1  \\\\\n",
        "0 & 1 & 0\n",
        "\\end{bmatrix}\n",
        "\n",
        "**Python**: \n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "cv2.Laplacian(src, ddepth)\n",
        "```\n",
        "\n",
        "\n",
        "**Parameters:**\n",
        "\n",
        "*   src – input image.\n",
        "*   ddepth –\n",
        "output image depth; the following combinations of ddepth are supported:\n",
        "    *  CV_8U\n",
        "    *  CV_16U/CV_16S\n",
        "    *  CV_32F\n",
        "    *  CV_64F\n",
        "    \n",
        "    \n",
        "---"
      ]
    },
    {
      "metadata": {
        "id": "96044LkLY-9U",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "d116138d-eaab-4a6a-a236-a4027b9a771b"
      },
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "img = cv2.imread('sudoku.jpg')\n",
        "laplacian = cv2.Laplacian(img,cv2.CV_64F)\n",
        "\n",
        "cv2.imshow('Laplacian',laplacian)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9sdV8uu9aCRz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Edge Detection\n",
        "\n",
        "The general criteria for edge detection include:\n",
        "\n",
        "*   **Low error rate**: This means that the detection should accurately catch as many edges in the image as possible.\n",
        "*   **Good localization**: The edge point detected from the operator should accurately localize on the center of the edge.\n",
        "*   **Minimal response**: A given edge in the image should only be marked once, and where possible, image noise should not create false edges.\n",
        "\n",
        "There are many methods for edge detection, but most of them can be grouped into two categories:\n",
        "\n",
        "*  **Search based methods**: These detect edges by first computing a measure of edge strength, usually the gradient magnitude, and then searching for local directional maximas of the gradient magnitude. \n",
        "*  **Zero-crossing based methods**: These search for zero crossings in a second-order derivative expression computed from the image, usually the zero-crossings of the Laplacian operator. \n",
        "\n",
        "As a pre-processing step to edge detection, a smoothing stage, typically **Gaussian smoothing**, is almost always applied for noise reduction.\n",
        "\n",
        "The various edge detection methods that have been published mainly differ in the types of smoothing filters applied and the way the measures of edge strength are computed.\n",
        "\n",
        "The most popular and widely implemented algorithm for edge detection in Computer Vision is the Canny Edge Detection.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "3hpfQm7z1ste",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Canny Edge Detector\n",
        "\n",
        "The Canny edge detector is an edge detection operator that uses a multi-stage algorithm to detect a wide range of edges in images. It was developed by John F. Canny in 1986.\n",
        "\n",
        "The Process of Canny edge detection algorithm can be broken down to 5 different steps:\n",
        "\n",
        "1.   Apply Gaussian filter to smooth the image in order to remove the noise\n",
        "2.   Find the intensity gradients of the image.\n",
        "3.   Apply non-maximum suppression to get rid of spurious response to edge detection.\n",
        "4.   Apply double threshold to determine potential edges.\n",
        "5.   Track edge by hysteresis: Finalize the detection of edges by suppressing all the other edges that are weak and not connected to strong edges.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "KmnNw4OxZQgd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##1) Gaussian filter\n",
        "\n",
        "Since edge detection is susceptible to noise in the image, first step is to remove it with a 5x5 Gaussian filter. The Gaussian filter is applied to convolve with the image. This step will slightly smoothen the image to reduce the effects of obvious noise on the edge detector.\n",
        "\n",
        "Given below is the 5×5 Gaussian filter generally used before implementing the Canny Edge Detection with a standard deviation of 1.4.\n",
        "\n",
        "The asterisk below denotes a convolution operation.\n",
        "\n",
        "![alt text](https://wikimedia.org/api/rest_v1/media/math/render/svg/efce20969e243d1ba3f34c2f7126041095bd4656)\n",
        "\n",
        "**Note**: The gaussian filter won't be implicitly implemented on a call to the cv2.Canny() function. Hence, make sure that the image is filtered before passing it on to the Canny function.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "jxUCYggGaCwQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##2) Finding intensity gradient of the image\n",
        "\n",
        "Smoothened image is then filtered with a Sobel kernel in both horizontal and vertical directions to get first derivative in horizontal direction ($G_x$) and vertical direction ($G_y$). From these two images, we can find edge gradient and direction for each pixel as follows:\n",
        "\n",
        "![alt text](https://docs.opencv.org/3.0-beta/_images/math/a1d81c63db2a560ef2fc82222b7851257d1ef4a2.png)\n",
        "\n",
        "The edge direction angle is then rounded to one of four angles representing vertical, horizontal and the two diagonals (0°, 45°, 90° and 135°). For instance θ in [0°, 22.5°] or [157.5°, 180°] is mapped to 0°.\n",
        "\n",
        "After getting gradient magnitude and direction, a full scan of image is done to remove any unwanted pixels which may not constitute the edge. A threshold for the image gradients is set and only those pixels whose gradient magnitudes fall above this threshold are taken as edges.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "K7g1GNGeZ3CX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##3) Non-maximum suppression\n",
        "\n",
        "Non-maximum suppression is an **edge thinning technique**. \n",
        "After applying gradient calculation, the edge extracted from the gradient value is still quite blurred. As minimal response is one of the important criterions for edge detection, there should only be one accurate response to the edge. Thus non-maximum suppression can help to suppress all the gradient values (by setting them to 0) except the local maxima, which indicate locations with the sharpest change of intensity value.\n",
        "\n",
        "For this, every pixel is checked if it is a local maximum in the gradient direction.\n",
        "\n",
        "The following algorithm is followed for each pixel in the gradient image.\n",
        "\n",
        "*  Compare the edge strength of the current pixel with the edge strength of the pixel in the positive and negative gradient directions.\n",
        "*  If the edge strength of the current pixel is the largest compared to both, the value will be preserved. Otherwise, the value will be suppressed.\n",
        "\n",
        "It is by logic, we observe that the **gradient direction is always normal to the edge**.\n",
        "\n",
        "![alt text](https://docs.opencv.org/3.0-beta/_images/nms.jpg)\n",
        "\n",
        "Point A is on the edge (in vertical direction). Point B and C are in gradient directions. So point A is checked with point B and C to see if it forms a local maximum. If so, it is considered for next stage, otherwise, it is suppressed (put to zero).\n",
        "\n",
        "*  If the rounded gradient angle is 0° (i.e. the edge is in the north-south direction), the point will be considered to be on the edge if its gradient magnitude is greater than the magnitudes at pixels in the east and west directions.\n",
        "*  If the rounded gradient angle is 90° (i.e. the edge is in the east-west direction) the point will be considered to be on the edge if its gradient magnitude is greater than the magnitudes at pixels in the north and south directions.\n",
        "*  If the rounded gradient angle is 135° (i.e. the edge is in the northeast-southwest direction) the point will be considered to be on the edge if its gradient magnitude is greater than the magnitudes at pixels in the north west and south-east directions.\n",
        "*  If the rounded gradient angle is 45° (i.e. the edge is in the north west–south east direction) the point will be considered to be on the edge if its gradient magnitude is greater than the magnitudes at pixels in the north east and south west directions.\n",
        "\n",
        "Note that the sign of the direction is irrelevant, i.e. north–south is the same as south–north and so on.\n",
        "\n",
        "In short, the result you get is a binary image with “thin edges” as shown below.\n",
        "\n",
        "![alt text](http://ai.stanford.edu/~syyeung/cvweb/Pictures1/canny2.png)\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "6-N91NmkYUWy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##4) Double thresholding\n",
        "\n",
        "After application of non-maximum suppression, remaining edge pixels provide a more accurate representation of real edges in an image. However, some edge pixels remain that are caused by noise and color variation. In order to account for these spurious responses, it is essential to filter out edge pixels with a weak gradient value and preserve edge pixels with a high gradient value. \n",
        "\n",
        "This is accomplished by selecting upper and lower threshold values. \n",
        "\n",
        "*  If the pixel gradient is higher than the upper threshold, it is marked as a strong edge pixel. \n",
        "*  If the pixel gradient is between the two thresholds, it is marked as a weak edge pixel. \n",
        "*  If the pixel gradient value is below the lower threshold, it will be suppressed. \n",
        "\n",
        "The two threshold values are passed on as arguments while calling the Canny Edge Detector function.\n",
        "\n",
        "It is recommended that the upper:lower ratio is maintained between 2:1 and 3:1 for good results.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "ur30iOE8Z1Nh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##5) Hysteresis\n",
        "\n",
        "The strong edge pixels obtained from the above thresholding are accepted as \"true-edges\".\n",
        "\n",
        "The weak edge pixels obtained from the above thresholding are classified as edges or non-edges based on their connectivity. If they are somehow connected to a strong edge pixel moving along the edge, they are considered to be \"true-edges\". Otherwise, they are also discarded.\n",
        "\n",
        "So what we finally get are only the strong edges in the image.\n",
        "\n",
        "![alt text](http://ai.stanford.edu/~syyeung/cvweb/Pictures1/canny3.png)\n",
        "\n",
        "**Note**: The input image passed on to the Canny function must be a grayscale image. The output image of the Canny function will be a Boolean (binary) grayscale image.\n",
        "\n",
        "**Python**: \n",
        "\n",
        "```\n",
        "cv2.Canny(src, minVal, maxVal, ksize, L2gradient)\n",
        "```\n",
        "\n",
        "**Parameters:**\n",
        "\n",
        "*   src – input image in **grayscale**.\n",
        "*   minVal – lower threshold value for double thresholding.\n",
        "*   maxVal – upper threshold value for double thresholding.\n",
        "*   ksize – size of the Sobel kernel for finding gradients; default ksize==3.\n",
        "*   L2gradient – specifies the equation for finding gradient magnitude; default L2gradient==False.\n",
        "\n",
        "If L2gradient is $True$, it uses $ \\begin{equation*} G = \\sqrt{ G_{x}^{2} + G_{y}^{2} }\\end{equation*} $   for finding gradient magnitude.\n",
        "\n",
        "If L2gradient is $False$, it uses $ \\begin{equation*} G = |G{x}| + |G{y}| \\end{equation*} $    for finding gradient magnitude.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "xeDychfaZ5nD",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "outputId": "4361dda9-0483-473d-99cb-c9a4821a1aad",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530794712744,
          "user_tz": -330,
          "elapsed": 1099,
          "user": {
            "displayName": "Siva Subramaniyan",
            "photoUrl": "//lh3.googleusercontent.com/-CeVc5JSOZSg/AAAAAAAAAAI/AAAAAAAAAA0/h9GqyBAovnQ/s50-c-k-no/photo.jpg",
            "userId": "102411634545146112960"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "img = cv2.imread('sudoku.jpg')\n",
        "imggray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "blurred = cv2.GaussianBlur(imggray, (5, 5), 0)\n",
        "canny = cv2.Canny(blurred,20,50)\n",
        "\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(canny)\n",
        "plt.show()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAU4AAAFOCAYAAADpU/RpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNs5qvmldCQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fe754eaad10>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "QsT7zU0nvzzP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Corner Detection**\n",
        "\n",
        "We already saw that corners are point-like features in an image, which have a local two dimensional structure. In other words, corners are regions in the image with large intensity variations in two perpendicular directions. Corners are generally invariant to translation, rotation and illumination.\n",
        "\n",
        "The most widely implemented methods for corner detection in Computer Vision are the Harris and Shi-Tomasi algorithms.\n",
        "\n",
        "Let's discuss the basic algorithm used in both the methods.\n",
        "\n",
        "##Algorithm\n",
        "\n",
        "Each pixel in the image is checked to see if a corner is present by considering how similar a patch centered on the pixel is to nearby, largely overlapping patches.\n",
        "\n",
        "The 3 basic spaces in an image can be distinguished as follows:\n",
        "\n",
        "*  If the pixel is in a flat region of uniform intensity, then the nearby patches will look similar. \n",
        "*  If the pixel is on an edge, then nearby patches in a direction perpendicular to the edge will look quite different, but nearby patches in a direction parallel to the edge will result only in a small change. \n",
        "*  If the pixel is on a corner/feature with variation in all directions, then none of the nearby patches will look similar.\n",
        "\n",
        "![alt text](https://i.stack.imgur.com/k1CCY.png)\n",
        "\n",
        "The similarity between adjacent patches is measured by taking the sum of squared differences (SSD) between the corresponding pixels of two patches. A lower number indicates more similarity.\n",
        "\n",
        "Without loss of generality, we will assume that a grayscale 2-dimensional image is used. Let the intensity of the pixel at $(x,y)$ be given by $I(x,y)$. Consider taking two image patches; one centred around the pixel at $(x,y)$ and the other shifted from the first patch by $(u,v)$. The weighted sum of squared differences (SSD) between these two patches is given by:\n",
        "\n",
        "$${\\displaystyle E(u,v)=\\sum _{x}\\sum _{y}w(x,y)\\,\\left(I(x+u,y+v)-I(x,y)\\right)^{2}}$$\n",
        "\n",
        "$w(a,b)$ is a window function and is either gaussian or rectangular(equal weight for all pixels in the window). It gives weights to pixels underneath the window. The size of the window function must be given while calling the corner detector and must be odd.\n",
        "\n",
        "In the above expression, $I(x+u,y+v)$ can be approximated by Taylor expansion. Let $I_x$ and $I_y$ be the image intensity gradients in the $x$ and $y$ directions respectively found using the Sobel operator. This produces the following approximation.\n",
        "\n",
        "$$I(x+u,y+v)\\approx I(x,y)+I_{x}(x,y)u+I_{y}(x,y)v$$\n",
        "\n",
        "$$E(u,v) \\approx \\sum_x \\sum_y w(x,y) \\, \\left( I_x(x,y)u + I_y(x,y)v \\right)^2$$\n",
        "\n",
        "This can be written in matrix form as follows:\n",
        "\n",
        "$$E(u,v) \\approx \\begin{bmatrix} u & v \\end{bmatrix} M \\begin{bmatrix} u \\\\ v \\end{bmatrix}$$\n",
        "\n",
        "where, \n",
        "\n",
        "$$M = \\sum_{x,y} w(x,y) \\begin{bmatrix}I_x I_x & I_x I_y \\\\\n",
        "                                     I_x I_y & I_y I_y \\end{bmatrix}$$\n",
        "\n",
        "From the Spectral theorem in linear algebra, we know that any symmetric matrix can be represented as follows\n",
        "\n",
        "$$M=Q.D.Q^T$$\n",
        "\n",
        "where Q is a rotation and D is a diagonal matrix of the eigen values $\\lambda_1$ and $\\lambda_2$ of the matrix M.\n",
        "\n",
        "As $Q^T.\\begin{bmatrix} u & v \\end{bmatrix}^T=\\begin{bmatrix} u' & v' \\end{bmatrix}^T$ and $Q.\\begin{bmatrix} u & v \\end{bmatrix}=\\begin{bmatrix} u' & v' \\end{bmatrix}$, \n",
        "\n",
        "$$E(u,v) = \\begin{bmatrix} u' & v' \\end{bmatrix} D \\begin{bmatrix} u' \\\\ v' \\end{bmatrix}=\\lambda_1(u')^2+\\lambda_2(v')^2$$\n",
        "\n",
        "Hence, $E(u,v)$ is large in both the directions u' and v' if both the eigen values $\\lambda_1~ and ~\\lambda_2$ are large.\n",
        "\n",
        "The following inferences can be made based on this argument:\n",
        "\n",
        "*  If $ \\lambda_1 \\approx 0$ and $\\lambda_2 \\approx 0$ then this pixel $(x,y)$ has no features of interest.\n",
        "*  If $\\lambda_1 \\approx 0$ and $\\lambda _{2}$ has some large positive value or vice-versa, then an edge is found.\n",
        "*  If $\\lambda_1$ and $\\lambda_2$ both have large positive values, then a corner is found.\n",
        "\n",
        "Hence, for detecting a Corner, both the eigen values must be large enough. But the exact computation of the eigenvalues is computationally expensive, since it requires the computation of a square root while solving a quadratic equation. \n",
        "\n",
        "Harris and Shi-Tomasi Corner Detectors maximize these eigen values in 2 different ways.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "_kGA0o4SUsFZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Harris Corner Detector\n",
        "\n",
        "The Harris Corner Detector uses a score to maximize these eigen values without calculating them individually. \n",
        "\n",
        "As $ \\lambda_1 $ and $ \\lambda_2 $ are the eigen-values of $M$, we have the following properties.\n",
        "\n",
        "$$ det(M) = \\lambda_1\\lambda_2 $$\n",
        "$$ trace(M) = \\lambda_1 + \\lambda_2 $$\n",
        "\n",
        "Let's define a score $R$ as follows:\n",
        "\n",
        "$$ R = det(M)-k \\ (trace(M))^2 $$\n",
        "\n",
        "where k is an empirically determined constant between $0.04-0.06$.\n",
        "\n",
        "*   When $|R|$ is small, which happens when $\\lambda_1$ and $\\lambda_2$ are small, the region is flat.\n",
        "*  When $R\\lt0$, which happens when $\\lambda_1 \\gt\\gt \\lambda_2$ or vice-versa, the region is an edge\n",
        "*  When $R$ is large, which happens when $\\lambda_1$ and $\\lambda_2$ are large and $\\lambda_1 \\sim \\lambda_2$, the region is a corner.\n",
        "\n",
        "The image below illustrates this with $\\lambda_1$ and $\\lambda_2$ as axes.\n",
        "\n",
        "![alt text](https://docs.opencv.org/3.0-beta/_images/harris_region.jpg)\n",
        "\n",
        "The result of Harris Corner Detection is a grayscale image with these scores as intensities. Thresholding for a suitable gives the corners in the image.\n"
      ]
    },
    {
      "metadata": {
        "id": "rhXO7PgIv0at",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "outputId": "48387646-0ad5-4ecc-fcae-fef6079c86a5",
        "executionInfo": {
          "status": "error",
          "timestamp": 1527241640763,
          "user_tz": -330,
          "elapsed": 1450,
          "user": {
            "displayName": "Pranav Ramakrishnan cs17b023",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "115535056791519165192"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "img1=cv2.imread('chesss1.jpg',1)\n",
        "img2=img1.copy()\n",
        "img=cv2.imread('chesss1.jpg',0) #Image in grayscale\n",
        "cv2.imwrite('Chess_Gray.jpg',img)\n",
        "ch = cv2.cornerHarris(img,2,3,0.1)\n",
        "ch=cv2.dilate(ch,None) #Dilating the corners to make them visible \n",
        "img1[ch>0.01*ch.max()]=[0,0,0] #Thresholding \n",
        "img2[ch>0.001*ch.max()]=[0,0,0]\n",
        "cv2.imwrite('Corners.jpg',img1)\n",
        "cv2.imwrite('Corner1.jpg',img2)\n",
        "cv2.imshow('Image',img1)\n",
        "cv2.imshow('Img_less_threshold',img2)\n",
        "cv2.waitKey(0)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b8ee6e9da748>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mimg1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'chesss1.jpg'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mimg2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'chesss1.jpg'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Image in grayscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Chess_Gray.jpg'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'copy'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "8drcFVK7v02A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**cv2.cornerHarris(src, blockSize, ksize, k[, dst[, borderType]]) → dst**\n",
        "\n",
        "The cv2.cornerHarris() function takes as arguments the source image, the neighbourhood size to find the covariation matrix blockSize, the aperture parameter for the cv2.Sobel() operator ksize, and the k value in the score. It returns a grayscale image containing the intensities. The methods to extrapolate the return image may be given as an argument if needed (borderType).\n",
        "\n",
        "Original Image Image with corners Image with more corners due to lower threshold\n",
        "\n",
        "From the above images it is obvious that an optimum value of threshold is necessary to find only the desired corners. At some places the corners are cramped together, points close to each other behave as corners. Here the centroids of all these points are taken and cv2.cornerSubPix() is used to give sub-pixel accuracy to the corners. The function iterates to find the sub-pixel accurate location of corners or radial saddle points. For every corner, in the neighbourhood of the corner the image gradient at a point is approximately perpendicular to the position vector of the point with respect to the corner. This observation is made use of to accurately locate the corners. Consider the dot product\n",
        "$$\\epsilon _i = {DI_{p_i}}^T \\cdot (q - p_i)$$\n",
        "\n",
        "where ${DI_{p_i}}$ is an image gradient at one of the points $p_i$ in a neighborhood of $q$. The value of $q$ is to be found so that $\\epsilon_i$ is minimized. A system of equations may be set up with $\\epsilon_i$ set to zero:\n",
        "$$\\sum _i(DI_{p_i} \\cdot {DI_{p_i}}^T) - \\sum _i(DI_{p_i} \\cdot {DI_{p_i}}^T \\cdot p_i)$$\n",
        "\n",
        "where the gradients are summed within a neighborhood (“search window”) of $q$. Calling the first gradient term $G$ and the second gradient term $b$ gives:\n",
        "$$q = G^{-1} \\cdot b$$\n",
        "\n",
        "The new center is set to the above point. The function iterates till a specific number of iterarions are done or the center moves from the original centroid by a maximum value, whichever happens first.\n",
        "\n",
        "**cv2.cornerSubPix(image, corners, winSize, zeroZone, criteria) → None**\n",
        "\n",
        "This function takes as argument the input image, the centroids of the corners in matrix form containing the coordinates, half the side length of the search window (if (3,3) is passed operations are performed over a window of size (7,7) to refine the corners), half of the size of the dead region in the middle of the search zone over which the summation in the formula below is not done (it is used sometimes to avoid possible singularities of the autocorrelation matrix; the value of (-1,-1) indicates that there is no such a size).\n",
        "\n",
        "Criteria for termination of the iterative process of corner refinement is the last argument. That is, the process of corner position refinement stops either after criteria.maxCount iterations or when the corner position moves by less than criteria.epsilon on some iteration.\n"
      ]
    },
    {
      "metadata": {
        "id": "Te0qs0D9v1H7",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "a1abd3d5-36b5-4e32-e2cf-46d5a5bdce26"
      },
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "img = cv2.imread('polygon1.png')\n",
        "grayimg = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "grayimg = np.float32(grayimg)\n",
        "ch = cv2.cornerHarris(grayimg,2,3,0.04) #Finding corners first\n",
        "ch = cv2.dilate(ch,None)\n",
        "ret, ch = cv2.threshold(ch,0.01*ch.max(),255,0)\n",
        "ch = np.uint8(ch)\n",
        "\n",
        "#Finding centroids of corners which are bunched together\n",
        "ret, labels, stats, centroids = cv2.connectedComponentsWithStats(ch) #Works only with opencv3.0\n",
        "#Defining the criteria to end the iterations\n",
        "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.001)\n",
        "corners = cv2.cornerSubPix(grayimg,np.float32(centroids),(5,5),(-1,-1),criteria)\n",
        "\n",
        "#Marking the points found\n",
        "res = np.hstack((centroids,corners))\n",
        "res = np.int0(res)\n",
        "img[res[:,1],res[:,0]]=[0,0,255]\n",
        "img[res[:,3],res[:,2]] = [0,255,0]\n",
        "\n",
        "cv2.imwrite('subpixel.jpg',img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nFS-_r1MU2Ne",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Shi-Tomasi Corner Detector"
      ]
    },
    {
      "metadata": {
        "id": "1QR-O_CWv1TD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Shi Tomasi detector**\n",
        "\n",
        "There is only one major difference between the Harris detector and the Shi Tomasi detector. The score is the minimum of the two eigen-values. $$\\begin{equation*} R = min(\\lambda_1,\\lambda_2) \\end{equation*}$$ The illustration with $\\lambda_1$ and $\\lambda_2$ as axes looks like The cv2.goodFeaturesToTrack() function uses this algorithm to find a particular number of corners (unless Harris method is specifically asked for). cv2.goodFeaturesToTrack(image, maxCorners, qualityLevel, minDistance[, corners[, mask[, blockSize[, useHarrisDetector[, k]]]]]) → corners\n",
        "\n",
        "    image - source image\n",
        "    maxCorners - the maximum number of corners which are needed\n",
        "    qualityLevel - all the corners which fall below this threshold are rejected irrespective of whether the maximum number is obtained or not, this is relative to the best corner's quality measure; every corner returned will atleast be of quality this parameter value $\\times$ the best quality\n",
        "    minDistance - minimum possible distance between the corners; for two corners satisfying the quality check closer together than this value, the one with the lower quality is chucked out\n",
        "    mask - optional argument giving region to look for corners\n",
        "    blockSize - size of an average block for computing a derivative covariation matrix over each pixel neighborhood\n",
        "    useHarrisDetector,k - Optional parameters; k is the free parameter argument of Harris detector\n",
        "\n",
        "The function returns the corners in descending order of quality.\n",
        "Quality measure :\n",
        "\n",
        "For Harris detector the quality is directly the intensities returned by the function. For Shi-Tomasi detector another function is used to find the best quality measure; this returns an image with the minimun eigen values for all the points.\n",
        "\n",
        "cv2.cornerMinEigenVal(src, blockSize[, dst[, ksize[, borderType]]]) → dst\n",
        "\n",
        "    blockSize - neighbourhood size\n",
        "    kSize - the aperture parameter of the cv2.Sobel() operator\n",
        "    borderType - pixel extrapolation method in the image returned\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "mXcT_aXXubDq",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "img = cv2.imread('cube2.jpg')\n",
        "gimg = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "corners = cv2.goodFeaturesToTrack(gimg,25,0.01,10)\n",
        "corners = np.int0(corners)\n",
        "\n",
        "for i in corners:\n",
        "    x,y = i.ravel()\n",
        "    cv2.circle(img,(x,y),5,(0,255,0),-1) #green circles are drawn on the detected corners\n",
        "cv2.imwrite(\"Shi_Tomasi.jpg\",img)\n",
        "cv2.imwrite(\"Gray_Cube.jpg\",gimg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HZ6MQmkdrU5y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# SIFT \n",
        " They are rotation-invariant, which means, even if the image is rotated, we can find the same corners. It is obvious because corners remain corners in rotated image also. But what about scaling? A corner may not be a corner if the image is scaled. For example, check a simple image below. A corner in a small image within a small window is flat when it is zoomed in the same window. So Harris corner is not scale invariant.\n",
        " \n",
        " ![](sift_scale_invariant.jpg)\n",
        " \n",
        " "
      ]
    },
    {
      "metadata": {
        "id": "SEHwDgxqJ1r8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##1. Scale-space Extrema Detection\n",
        "\n",
        "Laplacian of Gaussian is found for the image with various σ values. LoG acts as a blob detector which detects blobs in various sizes due to change in σ. For eg, in the above image, gaussian kernel with low σ gives high value for small corner while guassian kernel with high σ fits well for larger corner. So, we can find the local maxima across the scale and space which gives us a list of (x,y,σ) values which means there is a potential keypoint at (x,y) at σ scale.\n",
        "\n",
        "But this LoG is a little costly, so SIFT algorithm uses Difference of Gaussians which is an approximation of LoG. Difference of Gaussian is obtained as the difference of Gaussian blurring of an image with two different σ, let it be σ and kσ. This process is done for different octaves of the image in Gaussian Pyramid.\n",
        "\n",
        "\n",
        "\n",
        "Once this DoG are found, images are searched for local extrema over scale and space. For eg, one pixel in an image is compared with its 8 neighbours as well as 9 pixels in next scale and 9 pixels in previous scales. If it is a local extrema, it is a potential keypoint. It basically means that keypoint is best represented in that scale.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "1lvaYqrgKWxp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##2. Keypoint Localization\n",
        "\n",
        "Once potential keypoints locations are found, they have to be refined to get more accurate results. They used Taylor series expansion of scale space to get more accurate location of extrema, and if the intensity at this extrema is less than a threshold value (0.03 as per the paper), it is rejected. This threshold is called contrastThreshold in OpenCV\n",
        "\n",
        "DoG has higher response for edges, so edges also need to be removed. For this, a concept similar to Harris corner detector is used. They used a 2x2 Hessian matrix (H) to compute the pricipal curvature. We know from Harris corner detector that for edges, one eigen value is larger than the other. So here they used a simple function,\n",
        "\n",
        "If this ratio is greater than a threshold, called edgeThreshold in OpenCV, that keypoint is discarded. It is given as 10 in paper.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "znulwpK-KvYU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##3. Orientation Assignment\n",
        "\n",
        "Now an orientation is assigned to each keypoint to achieve invariance to image rotation. A neigbourhood is taken around the keypoint location depending on the scale, and the gradient magnitude and direction is calculated in that region. An orientation histogram with 36 bins covering 360 degrees is created. (It is weighted by gradient magnitude and gaussian-weighted circular window with σ equal to 1.5 times the scale of keypoint. The highest peak in the histogram is taken and any peak above 80% of it is also considered to calculate the orientation. It creates keypoints with same location and scale, but different directions. It contribute to stability of matching.\n"
      ]
    },
    {
      "metadata": {
        "id": "pA3E2cgtK-LB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##4. Keypoint Descriptor\n",
        "\n",
        "Now keypoint descriptor is created. A 16x16 neighbourhood around the keypoint is taken. It is devided into 16 sub-blocks of 4x4 size. For each sub-block, 8 bin orientation histogram is created. So a total of 128 bin values are available. It is represented as a vector to form keypoint descriptor. In addition to this, several measures are taken to achieve robustness against illumination changes, rotation etc."
      ]
    },
    {
      "metadata": {
        "id": "I2titNLCLJCK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##5. Keypoint Matching\n",
        "Keypoints between two images are matched by identifying their nearest neighbours. But in some cases, the second closest-match may be very near to the first. It may happen due to noise or some other reasons. In that case, ratio of closest-distance to second-closest distance is taken. If it is greater than 0.8, they are rejected. It eliminaters around 90% of false matches while discards only 5% correct matches.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "AxfHab-1LTQY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Implementation\n"
      ]
    },
    {
      "metadata": {
        "id": "QS_gADXfLcfV",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "outputId": "f6d61ca8-a320-45b0-cdf6-28a038e6721e",
        "executionInfo": {
          "status": "error",
          "timestamp": 1530672427818,
          "user_tz": -330,
          "elapsed": 1385,
          "user": {
            "displayName": "Pranav Ramakrishnan cs17b023",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "115535056791519165192"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "  import cv2\n",
        "  import numpy as np\n",
        "  img = cv2.imread('home.jpg')\n",
        "  gray= cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "  sift = cv2.xfeatures2d.SIFT_create()\n",
        "  kp = sift.detect(gray,None)\n",
        "  img=cv2.drawKeypoints(gray,kp)\n",
        "  cv2.imwrite('sift_keypoints.jpg',img)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31merror\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-6a6174861dbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'home.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mgray\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0msift\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxfeatures2d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSIFT_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mkp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msift\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: OpenCV(3.4.1) /io/opencv/modules/imgproc/src/color.cpp:11147: error: (-215) scn == 3 || scn == 4 in function cvtColor\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "rj66TOVeL8Xe",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "img=cv2.drawKeypoints(gray,kp,flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
        "cv2.imwrite('sift_keypoints.jpg',img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DpAegP00L_9o",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "sift = cv2.xfeatures2d.SIFT_create()\n",
        "kp, des = sift.detectAndCompute(gray,None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ttYNpmXX4hly",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#$$THE~END!$$"
      ]
    }
  ]
}