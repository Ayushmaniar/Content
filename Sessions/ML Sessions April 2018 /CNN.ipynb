{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "mDhnAhRzWndo",
        "colab_type": "text",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": [
        "# Convolutional Neural Networks\n",
        "\n",
        "`Computer Vision and Intelligence Group, IIT Madras`"
      ]
    },
    {
      "metadata": {
        "id": "tGCP4zxT1TCv",
        "colab_type": "text",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": [
        "# Training a ConvNet on the notMNIST dataset\n",
        "For this notebook, we will be using the **notMNIST** dataset, which consists of images of **alphabets A - J** written in different styles. "
      ]
    },
    {
      "metadata": {
        "id": "CRzBTUql-G-u",
        "colab_type": "text",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": [
        "Download required files here:\n",
        "\n",
        "https://drive.google.com/file/d/1krtsuAxfKwqcOFyyN0ZfYrrNS8iczFp1/view?usp=sharing\n",
        "\n",
        "https://drive.google.com/file/d/1EMwvZ46VbcRWS9rAo-qs467sf8X5GWOs/view?usp=sharing\n",
        "\n",
        "https://drive.google.com/file/d/1doaanFK584-ZesCVWbmGA5B88M6Lqbr7/view?usp=sharing\n",
        "\n",
        "https://drive.google.com/file/d/1fKdrcG22JMG9ym4sMdRpn2kELBJbGS37/view?usp=sharing\n",
        "\n",
        "https://drive.google.com/file/d/1feQd8xZRZUzJ7kR7omjXMW0fOOkEEHmN/view?usp=sharing."
      ]
    },
    {
      "metadata": {
        "id": "sq4QKL1W1WdO",
        "colab_type": "code",
        "slideshow": {
          "slide_type": "slide"
        },
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "e84e2cdc-5aa7-46ae-a01b-3f2d95b8f114",
        "executionInfo": {
          "elapsed": 8526,
          "status": "ok",
          "timestamp": 1523536331754,
          "user": {
            "displayName": "Varun S",
            "photoUrl": "//lh6.googleusercontent.com/-ouTcePNtIug/AAAAAAAAAAI/AAAAAAAAAAA/htglc2tIvR8/s50-c-k-no/photo.jpg",
            "userId": "107242715894134261606"
          },
          "user_tz": -330
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n",
        "!pip install keras\n",
        "!pip install opencv-python"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /Users/Ankivarun/anaconda3/envs/tf_python3/lib/python3.6/site-packages\n",
            "Requirement already satisfied: astor>=0.6.0 in /Users/Ankivarun/anaconda3/envs/tf_python3/lib/python3.6/site-packages (from tensorflow)\n",
            "Requirement already satisfied: tensorboard<1.8.0,>=1.7.0 in /Users/Ankivarun/anaconda3/envs/tf_python3/lib/python3.6/site-packages (from tensorflow)\n",
            "Requirement already satisfied: gast>=0.2.0 in /Users/Ankivarun/anaconda3/envs/tf_python3/lib/python3.6/site-packages (from tensorflow)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /Users/Ankivarun/anaconda3/envs/tf_python3/lib/python3.6/site-packages (from tensorflow)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /Users/Ankivarun/anaconda3/envs/tf_python3/lib/python3.6/site-packages (from tensorflow)\n",
            "Requirement already satisfied: wheel>=0.26 in /Users/Ankivarun/anaconda3/envs/tf_python3/lib/python3.6/site-packages (from tensorflow)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /Users/Ankivarun/anaconda3/envs/tf_python3/lib/python3.6/site-packages (from tensorflow)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /Users/Ankivarun/anaconda3/envs/tf_python3/lib/python3.6/site-packages (from tensorflow)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /Users/Ankivarun/anaconda3/envs/tf_python3/lib/python3.6/site-packages (from tensorflow)\n",
            "Requirement already satisfied: six>=1.10.0 in /Users/Ankivarun/anaconda3/envs/tf_python3/lib/python3.6/site-packages (from tensorflow)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /Users/Ankivarun/anaconda3/envs/tf_python3/lib/python3.6/site-packages (from tensorboard<1.8.0,>=1.7.0->tensorflow)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /Users/Ankivarun/anaconda3/envs/tf_python3/lib/python3.6/site-packages (from tensorboard<1.8.0,>=1.7.0->tensorflow)\n",
            "Collecting bleach==1.5.0 (from tensorboard<1.8.0,>=1.7.0->tensorflow)\n",
            "  Downloading https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl\n",
            "Collecting html5lib==0.9999999 (from tensorboard<1.8.0,>=1.7.0->tensorflow)\n",
            "  Downloading https://files.pythonhosted.org/packages/ae/ae/bcb60402c60932b32dfaf19bb53870b29eda2cd17551ba5639219fb5ebf9/html5lib-0.9999999.tar.gz (889kB)\n",
            "\u001b[K    100% |████████████████████████████████| 890kB 695kB/s ta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /Users/Ankivarun/anaconda3/envs/tf_python3/lib/python3.6/site-packages (from protobuf>=3.4.0->tensorflow)\n",
            "Building wheels for collected packages: html5lib\n",
            "  Running setup.py bdist_wheel for html5lib ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /Users/Ankivarun/Library/Caches/pip/wheels/50/ae/f9/d2b189788efcf61d1ee0e36045476735c838898eef1cad6e29\n",
            "Successfully built html5lib\n",
            "Installing collected packages: html5lib, bleach\n",
            "  Found existing installation: html5lib 1.0.1\n",
            "    Uninstalling html5lib-1.0.1:\n",
            "      Successfully uninstalled html5lib-1.0.1\n",
            "  Found existing installation: bleach 2.1.3\n",
            "    Uninstalling bleach-2.1.3:\n",
            "      Successfully uninstalled bleach-2.1.3\n",
            "Successfully installed bleach-1.5.0 html5lib-0.9999999\n",
            "\u001b[33mYou are using pip version 9.0.2, however version 9.0.3 is available.\n",
            "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
            "Requirement already satisfied: keras in /Users/Ankivarun/anaconda3/envs/tf_python3/lib/python3.6/site-packages\n",
            "Requirement already satisfied: six>=1.9.0 in /Users/Ankivarun/anaconda3/envs/tf_python3/lib/python3.6/site-packages (from keras)\n",
            "Requirement already satisfied: pyyaml in /Users/Ankivarun/anaconda3/envs/tf_python3/lib/python3.6/site-packages (from keras)\n",
            "Requirement already satisfied: scipy>=0.14 in /Users/Ankivarun/anaconda3/envs/tf_python3/lib/python3.6/site-packages (from keras)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /Users/Ankivarun/anaconda3/envs/tf_python3/lib/python3.6/site-packages (from keras)\n",
            "\u001b[33mYou are using pip version 9.0.2, however version 9.0.3 is available.\n",
            "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
            "Collecting opencv-python\n",
            "  Downloading https://files.pythonhosted.org/packages/95/8b/89d27a57653ac6e8e0d8bdd261b14b792c9affe16b5b453db9f6aa1ddb68/opencv_python-3.4.0.12-cp36-cp36m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (41.2MB)\n",
            "\u001b[K    100% |████████████████████████████████| 41.2MB 30kB/s eta 0:00:011   28% |█████████                       | 11.7MB 1.1MB/s eta 0:00:26    35% |███████████▍                    | 14.6MB 1.7MB/s eta 0:00:16    49% |███████████████▉                | 20.4MB 937kB/s eta 0:00:23    70% |██████████████████████▌         | 28.9MB 2.7MB/s eta 0:00:05\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.1 in /Users/Ankivarun/anaconda3/envs/tf_python3/lib/python3.6/site-packages (from opencv-python)\n",
            "Installing collected packages: opencv-python\n",
            "Successfully installed opencv-python-3.4.0.12\n",
            "\u001b[33mYou are using pip version 9.0.2, however version 9.0.3 is available.\n",
            "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZJSX4mUz1cGw",
        "colab_type": "code",
        "slideshow": {
          "slide_type": "slide"
        },
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c8c60784-6094-49d9-f456-9bd974a05a93",
        "executionInfo": {
          "elapsed": 4511,
          "status": "ok",
          "timestamp": 1523536338541,
          "user": {
            "displayName": "Varun S",
            "photoUrl": "//lh6.googleusercontent.com/-ouTcePNtIug/AAAAAAAAAAI/AAAAAAAAAAA/htglc2tIvR8/s50-c-k-no/photo.jpg",
            "userId": "107242715894134261606"
          },
          "user_tz": -330
        }
      },
      "cell_type": "code",
      "source": [
        "# These are all the modules we'll be using later. Make sure you can import them before proceeding further.\n",
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "from six.moves import cPickle as pickle\n",
        "import os\n",
        "from six.moves import range\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.optimizers import Adam\n",
        "import cv2\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/Users/Ankivarun/anaconda3/envs/tf_python3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
            "  from ._conv import register_converters as _register_converters\n",
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "-t-0GqQM1f9C",
        "colab_type": "text",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": [
        "The dataset has been converted into a **pickle file**, which is a common and efficient way for using datasets in deep learning."
      ]
    },
    {
      "metadata": {
        "id": "jzG_bqrd1gRS",
        "colab_type": "code",
        "slideshow": {
          "slide_type": "slide"
        },
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "76c2b2a3-7b1e-450f-ed1e-dd316544983a",
        "executionInfo": {
          "elapsed": 1774,
          "status": "error",
          "timestamp": 1523536342064,
          "user": {
            "displayName": "Varun S",
            "photoUrl": "//lh6.googleusercontent.com/-ouTcePNtIug/AAAAAAAAAAI/AAAAAAAAAAA/htglc2tIvR8/s50-c-k-no/photo.jpg",
            "userId": "107242715894134261606"
          },
          "user_tz": -330
        }
      },
      "cell_type": "code",
      "source": [
        "pickle_file = './notMNIST.pickle'\n",
        "\n",
        "with open(pickle_file, 'rb') as f:\n",
        "    save = pickle.load(f)\n",
        "    train_dataset = save['train_dataset']\n",
        "    train_labels = save['train_labels']\n",
        "    valid_dataset = save['valid_dataset']\n",
        "    valid_labels = save['valid_labels']\n",
        "    test_dataset = save['test_dataset']\n",
        "    test_labels = save['test_labels']\n",
        "    del save  # hint to help free up memory\n",
        "    print('\\n')\n",
        "    print('Training set: ', train_dataset.shape, train_labels.shape)\n",
        "    print('Validation set: ', valid_dataset.shape, valid_labels.shape)\n",
        "    print('Test set: ', test_dataset.shape, test_labels.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Training set:  (200000, 28, 28) (200000,)\n",
            "Validation set:  (10000, 28, 28) (10000,)\n",
            "Test set:  (10000, 28, 28) (10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UqCPNpP31lPt",
        "colab_type": "text",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": [
        "We have **200000** train samples, **10000** validation samples and **10000** test samples.\n",
        "**Image size** is 28x28."
      ]
    },
    {
      "metadata": {
        "id": "hm0jUvMp1meS",
        "colab_type": "code",
        "slideshow": {
          "slide_type": "slide"
        },
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "outputId": "8793b6c0-49aa-4ed4-8c9c-d1fdb92a3aac"
      },
      "cell_type": "code",
      "source": [
        "# Reformat into a Keras/TensorFlow-friendly shape:\n",
        "# - Convolutions need the image data formatted as a cube (width by height by #channels)\n",
        "# - Labels as float one-hot encodings.\n",
        "\n",
        "image_size = 28\n",
        "num_labels = 10               # letters A to J\n",
        "num_channels = 1              # grayscale\n",
        "\n",
        "def reformat(dataset, labels):\n",
        "    # Reformat the dataset into 28x28x1 size\n",
        "    dataset = dataset.reshape((-1, image_size, image_size, num_channels)).astype(np.float32)\n",
        "    labels = np_utils.to_categorical(labels, num_labels)\n",
        "    return dataset, labels\n",
        "\n",
        "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
        "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
        "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
        "print('\\n')\n",
        "print('Training set: ', train_dataset.shape, train_labels.shape)\n",
        "print('Validation set: ', valid_dataset.shape, valid_labels.shape)\n",
        "print('Test set: ', test_dataset.shape, test_labels.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Training set:  (200000, 28, 28, 1) (200000, 10)\n",
            "Validation set:  (10000, 28, 28, 1) (10000, 10)\n",
            "Test set:  (10000, 28, 28, 1) (10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JiVDFhSF1y9J",
        "colab_type": "text",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": [
        "# Defining the model"
      ]
    },
    {
      "metadata": {
        "id": "RTqEGHpc1zUz",
        "colab_type": "code",
        "slideshow": {
          "slide_type": "slide"
        },
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# number of convolutional filters to use\n",
        "nb_filters = 32\n",
        "\n",
        "# size of pooling area for max pooling\n",
        "pool_size = (2, 2)\n",
        "\n",
        "# convolution kernel size\n",
        "kernel_size = (3, 3)\n",
        "\n",
        "input_shape = (img_rows, img_cols, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UUYrmbIf10u6",
        "colab_type": "code",
        "slideshow": {
          "slide_type": "slide"
        },
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1],border_mode='valid',input_shape=input_shape))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=pool_size))\n",
        "model.add(Dropout(0.25))                             # Indicates the probability of keeping a particular node\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_labels))\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Lu77aYxq1_3d",
        "colab_type": "text",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": [
        "# Visualising the model"
      ]
    },
    {
      "metadata": {
        "id": "OkEhQytr19-W",
        "colab_type": "code",
        "slideshow": {
          "slide_type": "slide"
        },
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "outputId": "92979972-eb13-4be0-b7b6-2ec0c2c8b039"
      },
      "cell_type": "code",
      "source": [
        "model.summary()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',optimizer='adadelta',metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 26, 26, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 24, 24, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 12, 12, 32)        0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               589952    \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                1290      \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 600,810\n",
            "Trainable params: 600,810\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /Users/Ankivarun/anaconda3/envs/tf_python3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2885: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n",
            "WARNING:tensorflow:From /Users/Ankivarun/anaconda3/envs/tf_python3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1349: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FcphWiTK2Gkw",
        "colab_type": "text",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": [
        "The Network can be visualised as below:\n",
        "\n",
        "![cnn_layers](cnn_layers.png \"Title\")"
      ]
    },
    {
      "metadata": {
        "id": "RJD6ojeM2G28",
        "colab_type": "code",
        "slideshow": {
          "slide_type": "slide"
        },
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "outputId": "bc9fa109-74d3-4adc-f6ca-19de54440153"
      },
      "cell_type": "code",
      "source": [
        "b_size = 32                         # batch size\n",
        "epochs = 5                          \n",
        "\n",
        "\n",
        "## Training from scratch\n",
        "# Create checkpoint\n",
        "# filepath = \"notMNIST.hdf5\"\n",
        "# checkpoint = ModelCheckpoint(filepath,monitor='val_acc',verbose=0,save_best_only=True,mode='max')\n",
        "# callbacks_list = [checkpoint]\n",
        "\n",
        "# Train the model\n",
        "# history = model.fit(train_dataset,train_labels,batch_size=b_size, nb_epoch=epochs,\n",
        "#                     verbose=1, validation_data=(valid_dataset, valid_labels),callbacks=callbacks_list)\n",
        "\n",
        "# print(\"Model saved\")\n",
        "##\n",
        "\n",
        "\n",
        "## Load pre-trained weights\n",
        "model.load_weights('./notMNIST.hdf5')\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.0001, decay=1e-6),metrics=['accuracy'])\n",
        "##\n",
        "\n",
        "score = model.evaluate(test_dataset, test_labels, verbose=0)\n",
        "print('Test score:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test score: 0.14883870215415954\n",
            "Test accuracy: 0.9543\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6Q7MgNSe2OAK",
        "colab_type": "text",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": [
        "Now, let us make a prediction on an image here, just to check if it works. \n",
        "\n",
        "We load an image 'B.png' which contains the letter **B**, and change it to shape [1,28,28,1] so that it can be passed into the model."
      ]
    },
    {
      "metadata": {
        "id": "qLCk-ots2J0a",
        "colab_type": "code",
        "slideshow": {
          "slide_type": "slide"
        },
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "outputId": "2cfe6a1d-97d8-4c48-a13a-a7a9679c030c"
      },
      "cell_type": "code",
      "source": [
        "im = cv2.imread('B.png',0)\n",
        "plt.imshow(im,cmap='gray')\n",
        "plt.show()\n",
        "im = np.expand_dims(im,2) \n",
        "im = np.expand_dims(im,0)\n",
        "im.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x12c6d5438>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "sfQzqcTk2T_9",
        "colab_type": "text",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": [
        "Here, we use the **model.predict** function of Keras to generate prediction for this image. "
      ]
    },
    {
      "metadata": {
        "id": "UJN3YoJ02Qk_",
        "colab_type": "code",
        "slideshow": {
          "slide_type": "slide"
        },
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "223233f8-7ffd-4c65-c3ca-74e8ffa1dbeb",
        "executionInfo": {
          "elapsed": 1367,
          "status": "error",
          "timestamp": 1523536471296,
          "user": {
            "displayName": "Varun S",
            "photoUrl": "//lh6.googleusercontent.com/-ouTcePNtIug/AAAAAAAAAAI/AAAAAAAAAAA/htglc2tIvR8/s50-c-k-no/photo.jpg",
            "userId": "107242715894134261606"
          },
          "user_tz": -330
        }
      },
      "cell_type": "code",
      "source": [
        "predictions = model.predict(im)  \n",
        "print(predictions)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eWB-nYiO2Z7O",
        "colab_type": "text",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": [
        "We have obtained the prediction as a **one-hot vector** with second entry 1, which means that the label refers to B.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "dTO5xPwmNpqd",
        "colab_type": "text",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": [
        "# Covnet Architectures"
      ]
    },
    {
      "metadata": {
        "id": "ysjhkHXc2kwr",
        "colab_type": "text",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://cdn-images-1.medium.com/max/800/1*kBpEOy4fzLiFxRLjpxAX6A.png)\n",
        "\n",
        "[Reference](https://arxiv.org/abs/1605.07678)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "tMKTR97Q3eaQ",
        "colab_type": "text",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": [
        "# LENET5\n",
        "\n",
        "It is the year 1994, and this is one of the very first convolutional neural networks, and what propelled the field of Deep Learning. \n",
        "\n",
        "This pioneering work by Yann LeCun was named LeNet5 after many previous successful iterations since they year 1988.\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/800/0*V1vb9SDnsU1eZQUy.jpg)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Bcqp2c873w_I",
        "colab_type": "text",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": [
        "The LeNet5 architecture was fundamental, in particular the insight that image features are distributed across the entire image, and convolutions with learnable parameters are an effective way to extract similar features at multiple location with few parameters. \n",
        "\n",
        "At that time there was no GPU to help training, and even CPUs were slow.\n",
        "\n",
        "Being able to save parameters and computation was a key advantage. Still not enough for deployment back then\n"
      ]
    },
    {
      "metadata": {
        "id": "UU1Py-Gz3yeP",
        "colab_type": "text",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": [
        "LeNet5 features can be summarized as:\n",
        "\n",
        "* convolutional neural network use sequence of 3 layers: convolution, pooling, non-linearity\n",
        "\n",
        "* use convolution to extract spatial features\n",
        "\n",
        "* non-linearity in the form of tanh or sigmoids (no ReLus back then)\n",
        "\n",
        "* multi-layer neural network (MLP) as final classifier\n",
        "\n",
        "* sparse connection matrix between layers to avoid large computational cost"
      ]
    },
    {
      "metadata": {
        "id": "Jw7fFvO-4Skf",
        "colab_type": "text",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": [
        "## THE GAP\n",
        "\n",
        "In the years from 1998 to 2010 neural network were in incubation.\n",
        "\n",
        "Most people did not notice their increasing power, while many other researchers slowly progressed. \n",
        "\n",
        "Wasnot till 2010, when GPUs plus Deep training strategies allowed for their usage, first in the DAN CIRESAN NET."
      ]
    },
    {
      "metadata": {
        "id": "AdEAZYyO4syU",
        "colab_type": "text",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": [
        "# IMAGENET\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "BIqDF0b-4gLD",
        "colab_type": "text",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": [
        "# ALEXNET\n",
        "\n",
        "In 2012, Alex Krizhevsky released AlexNet which was a deeper and much wider version of the LeNet and won by a large margin the difficult ImageNet competition.\n",
        "\n",
        "Wide as in 20+% !\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/800/0*vsi8JJFV_O6Z34ks.png)"
      ]
    },
    {
      "metadata": {
        "id": "unIqnPSj4wGg",
        "colab_type": "text",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": [
        "AlexNet scaled the insights of LeNet into a much larger neural network that could be used to learn much more complex objects and object hierarchies. The contribution of this work were:\n",
        "\n",
        "* use of rectified linear units (ReLU) as non-linearities\n",
        "* use of dropout technique (Hinton et all) to selectively ignore single neurons during training, a way to avoid overfitting of the model\n",
        "* overlapping max pooling, avoiding the averaging effects of average pooling\n",
        "* use of GPUs NVIDIA GTX 580 to reduce training time"
      ]
    },
    {
      "metadata": {
        "id": "hhoG2ihO5BZb",
        "colab_type": "text",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": [
        "At the time GPU offered a much larger number of cores than CPUs, and allowed 10x faster training time, which in turn allowed to use larger datasets and also bigger images.\n",
        "\n",
        "The success of AlexNet started a small revolution. Convolutional neural network were now the workhorse of Deep Learning, which became the new name for “large neural networks that can now solve useful tasks”."
      ]
    },
    {
      "metadata": {
        "id": "BDV2mdjR5FSa",
        "colab_type": "text",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": [
        "# VGG\n",
        "\n",
        "The VGG networks from Oxford were the first to use much smaller 3×3 filters in each convolutional layers and also combined them as a sequence of convolutions.\n",
        "\n",
        "This seems to be contrary to the principles of LeNet, where large convolutions were used to capture similar features in an image.\n",
        "\n",
        "Instead of the 9×9 or 11×11 filters of AlexNet, filters started to become smaller, too dangerously close to the infamous 1×1 convolutions that LeNet wanted to avoid, at least on the first layers of the network. \n",
        "\n",
        "But the great advantage of VGG was the insight that multiple 3×3 convolution in sequence can emulate the effect of larger receptive fields, for examples 5×5 and 7×7. These ideas will be also used in more recent network architectures as Inception and ResNet."
      ]
    },
    {
      "metadata": {
        "id": "qGyLurvo5XCG",
        "colab_type": "text",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://cdn-images-1.medium.com/max/800/0*HREIJ1hjF7z4y9Dd.jpg)"
      ]
    },
    {
      "metadata": {
        "id": "XQ-OKdbz5h5b",
        "colab_type": "text",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://qph.fs.quoracdn.net/main-qimg-ba81c87204be1a5d11d64a464bca39eb)"
      ]
    },
    {
      "metadata": {
        "id": "4OtJSxmv5yya",
        "colab_type": "text",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": [
        "The VGG networks uses multiple 3×3 convolutional layers to represent complex features. \n",
        "\n",
        "Notice blocks 3, 4, 5 of VGG-E: 256×256 and 512×512 3×3 filters are used multiple times in sequence to extract more complex features and the combination of such features. \n",
        "\n",
        "This is effectively like having large 512×512 classifiers with 3 layers, which are convolutional! This obviously amounts to a massive number of parameters, and also learning power. \n",
        "\n",
        "But training of these network was difficult, and had to be split into smaller networks with layers added one by one. All this because of the lack of strong ways to regularize the model, or to somehow restrict the massive search space promoted by the large amount of parameters.\n",
        "\n",
        "VGG used large feature sizes in many layers and thus inference was quite costly at run-time. Reducing the number of features, as done in Inception bottlenecks, will save some of the computational cost.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "S-h8urMM6E_m",
        "colab_type": "text",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": [
        "## Let's run inferences on VGG-16"
      ]
    },
    {
      "metadata": {
        "id": "gXjPvlSG60oz",
        "colab_type": "code",
        "slideshow": {
          "slide_type": "slide"
        },
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!wget https://upload.wikimedia.org/wikipedia/commons/thumb/3/37/African_Bush_Elephant.jpg/440px-African_Bush_Elephant.jpg -o  elephant.jpg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wgQ7AZbNZSAO",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "outputId": "82dfc378-c549-4a2a-b242-f7d2cfe82639"
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "440px-African_Bush_Elephant.jpg\r\n",
            "African_elephant_warning_raised_trunk.jpg\r\n",
            "CNN.ipynb\r\n",
            "\u001b[1m\u001b[36mTf_poets\u001b[m\u001b[m\r\n",
            "elephant.jpg\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8Vkm5rGe4aES",
        "colab_type": "code",
        "slideshow": {
          "slide_type": "slide"
        },
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "outputId": "403e198d-0e61-480c-8785-355ab03ddc7e",
        "executionInfo": {
          "elapsed": 4312,
          "status": "error",
          "timestamp": 1523537700319,
          "user": {
            "displayName": "Varun S",
            "photoUrl": "//lh6.googleusercontent.com/-ouTcePNtIug/AAAAAAAAAAI/AAAAAAAAAAA/htglc2tIvR8/s50-c-k-no/photo.jpg",
            "userId": "107242715894134261606"
          },
          "user_tz": -330
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.vgg16 import preprocess_input, decode_predictions\n",
        "import numpy as np\n",
        "\n",
        "model = VGG16(weights='imagenet')\n",
        "\n",
        "img_path='a.jpg'\n",
        "img = image.load_img(img_path, target_size=(224, 224))\n",
        "x = image.img_to_array(img)\n",
        "x = np.expand_dims(x, axis=0)\n",
        "x = preprocess_input(x)\n",
        "\n",
        "preds = model.predict(x)\n",
        "# decode the results into a list of tuples (class, description, probability)\n",
        "# (one such list for each sample in the batch)\n",
        "print('Predicted:', decode_predictions(preds, top=3)[0])\n",
        "# Predicted: [(u'n02504013', u'Indian_elephant', 0.82658225), (u'n01871265', u'tusker', 0.1122357), (u'n02504458', u'African_elephant', 0.061040461)]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\n",
            "40960/35363 [==================================] - 2s 55us/step\n",
            "Predicted: [('n02504458', 'African_elephant', 0.914321), ('n01871265', 'tusker', 0.08234793), ('n02504013', 'Indian_elephant', 0.0032958146)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DgZeRAvg7v2e",
        "colab_type": "text",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": [
        "# GoogleNet and Inception\n",
        "_Let the Games begin XD_\n",
        "\n",
        "[Christian Szegedy](https://arxiv.org/abs/1409.4842) from Google begun a quest aimed at reducing the computational burden of deep neural networks, and devised the GoogLeNet the first Inception architecture.\n",
        "\n",
        "By now, Fall 2014, deep learning models were becoming extermely useful in categorizing the content of images and video frames. \n",
        "\n",
        "Most skeptics had given in that Deep Learning and neural nets came back to stay this time. Given the usefulness of these techniques, the internet giants like Google were very interested in efficient and large deployments of architectures on their server farms."
      ]
    },
    {
      "metadata": {
        "id": "D6zWMZud-wvP",
        "colab_type": "text",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": [
        "Christian thought a lot about ways to reduce the computational burden of deep neural nets while obtaining state-of-art performance (on ImageNet, for example). Or be able to keep the computational cost the same, while offering improved performance.\n",
        "\n",
        "He and his team came up with the Inception module:\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/800/0*CJZdXZULMr_on1Ao.jpg)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "m7a-e6qs_q0t",
        "colab_type": "text",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": [
        "This at a first glance is basically the parallel combination of 1×1, 3×3, and 5×5 convolutional filters.\n",
        "\n",
        "But the great insight of the inception module was the use of 1×1 convolutional blocks (NiN) to reduce the number of features before the expensive parallel blocks. This is commonly referred as “bottleneck”. T\n",
        "\n",
        "GoogLeNet used a stem without inception modules as initial layers, and an average pooling plus softmax classifier similar to NiN. This classifier is also extremely low number of operations, compared to the ones of AlexNet and VGG. This also contributed to a very efficient network design."
      ]
    },
    {
      "metadata": {
        "id": "buo1PLqaE3A3",
        "colab_type": "text",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://i.stack.imgur.com/Xqv0n.png)"
      ]
    },
    {
      "metadata": {
        "id": "eGQENC8LFAiM",
        "colab_type": "text",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": [
        "## Why multiple softmaxes?\n",
        "\n",
        "Deep architectures, and specifically GoogLeNet (22 layers) are in danger of the vanishing gradients problem during training (back-propagation algorithm). \n",
        "\n",
        "The engineers of GoogLeNet addressed this issue by adding classifiers in the intermediate layers as well, such that the final loss is a combination of the intermediate loss and the final loss. This is why you see a total of three loss layers, unlike the usual single layer as the last layer of the network."
      ]
    },
    {
      "metadata": {
        "id": "hFvE6dtmFPbX",
        "colab_type": "text",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://i.stack.imgur.com/U6O7Wl.png)"
      ]
    },
    {
      "metadata": {
        "id": "-ud7tULm_2Np",
        "colab_type": "text",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": [
        "# Bottleneck layers\n",
        "\n",
        "Was reducing the number of features, and thus operations, at each layer, so the inference time could be kept low. \n",
        "\n",
        "Before passing data to the expensive convolution modules, the number of features was reduce by, say, 4 times. This led to large savings in computational cost, and the success of this architecture."
      ]
    },
    {
      "metadata": {
        "id": "8BL9twl5AbX4",
        "colab_type": "text",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": [
        "Say you have 256 features coming in, and 256 coming out, and let’s say the Inception layer only performs 3×3 convolutions. \n",
        "\n",
        "That is 256×256 x 3×3 convolutions that have to be performed (589,000s multiply-accumulate, or MAC operations). \n",
        "\n",
        "Instead of doing this, we decide to reduce the number of features that will have to be convolved, say to 64 or 256/4. In this case, we first perform 256 -> 64 1×1 convolutions, then 64 convolution on all Inception branches, and then we use again a 1×1 convolution from 64 -> 256 features back again. The operations are now:\n",
        "\n",
        "* 256×64 × 1×1 = 16,000s\n",
        "* 64×64 × 3×3 = 36,000s\n",
        "* 64×256 × 1×1 = 16,000s\n",
        "\n",
        "For a total of about 70,000 versus the almost 600,000 we had before. Almost 10x less operations!"
      ]
    },
    {
      "metadata": {
        "id": "09CyHtzWCNs0",
        "colab_type": "text",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": [
        "And although we are doing less operations, we are not losing generality in this layer. In fact the bottleneck layers have been proven to perform at state-of-art on the ImageNet dataset, for example, and will be also used in later architectures such as ResNet.\n",
        "\n",
        "The reason for the success is that the input features are correlated, and thus redundancy can be removed by combining them appropriately with the 1×1 convolutions. Then, after convolution with a smaller number of features, they can be expanded again into meaningful combination for the next layer."
      ]
    },
    {
      "metadata": {
        "id": "OfSiLOohCDUG",
        "colab_type": "text",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://raw.githubusercontent.com/ritchieng/machine-learning-nanodegree/master/deep_learning/convolutional_neural_nets/cn19.png)\n",
        "\n",
        "Credits: Udacity"
      ]
    },
    {
      "metadata": {
        "id": "lmSFA6gw9ciS",
        "colab_type": "text",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": [
        "Now let's redo an inference for Inception v3. \n",
        "\n",
        "Let's also redo a transfer learning for Resnet-50.\n",
        "\n",
        "Hint:\n",
        "\n",
        "Think of   \n",
        "`\n",
        "from keras.applications.inception_v3 import InceptionV3`\n",
        "\n",
        "And\n",
        "\n",
        "InceptionV3 is your new class."
      ]
    },
    {
      "metadata": {
        "id": "GWVjEDvGCeYa",
        "colab_type": "text",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": [
        "# INCEPTION V3 (AND V2)\n",
        "\n",
        "In February 2015 Batch-normalized Inception was introduced as Inception V2. \n",
        "\n",
        "Batch-normalization computes the mean and standard-deviation of all feature maps at the output of a layer, and normalizes their responses with these values. This corresponds to “whitening” the data, and thus making all the neural maps have responses in the same range, and with zero mean. This helps training as the next layer does not have to learn offsets in the input data, and can focus on how to best combine features."
      ]
    },
    {
      "metadata": {
        "id": "v4X7c4IlCmK9",
        "colab_type": "text",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": [
        "In December 2015 Google released a new version of the Inception modules and the corresponding architecture This article better explains the original GoogLeNet architecture, giving a lot more detail on the design choices. A list of the original ideas are:\n",
        "\n",
        "* maximize information flow into the network, by carefully constructing networks that balance depth and width. Before each pooling, increase the feature maps.\n",
        "\n",
        "* when depth is increased, the number of features, or width of the layer is also increased systematically\n",
        "\n",
        "* use width increase at each layer to increase the combination of features before next layer\n",
        "\n",
        "* use only 3×3 convolution, when possible, given that filter of 5×5 and 7×7 can be decomposed with multiple 3×3. "
      ]
    },
    {
      "metadata": {
        "id": "y68E-bd8CvqC",
        "colab_type": "text",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://cdn-images-1.medium.com/max/800/0*Y9mKbwp1R8vAmT2L.jpg)"
      ]
    },
    {
      "metadata": {
        "id": "s8vNa3MzC53y",
        "colab_type": "text",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": [
        "filters can also be decomposed by flattened convolutions into more complex modules:\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/800/0*rRv_N9rLYJnmq6jz.jpg)"
      ]
    },
    {
      "metadata": {
        "id": "BphE1qCNDGf6",
        "colab_type": "text",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": [
        "Inception modules can also decrease the size of the data by provide pooling while performing the inception computation. \n",
        "\n",
        "This is basically identical to performing a convolution with strides in parallel with a simple pooling layer:\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/800/0*rxf30_SJRsbFIFCW.jpg)\n",
        "\n",
        "Inception still uses a pooling layer plus softmax as final classifier."
      ]
    },
    {
      "metadata": {
        "id": "UNIImyEMF4FW",
        "colab_type": "text",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": [
        "Inception v3 in full glory:\n",
        "\n",
        "![alt text](https://www.packtpub.com/graphics/9781786466587/graphics/image_08_007.jpg)\n"
      ]
    },
    {
      "metadata": {
        "id": "CJu-no_ODkyr",
        "colab_type": "text",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": [
        "# RESNET\n",
        "\n",
        "The revolution then came in December 2015, at about the same time as Inception v3. ResNet have a simple ideas: feed the output of two successive convolutional layer AND also bypass the input to the next layers!\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/800/0*0r0vS8myiqyOb79L.jpg)\n"
      ]
    },
    {
      "metadata": {
        "id": "_kORNmOpD2-V",
        "colab_type": "text",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://i.stack.imgur.com/ElFiI.png)"
      ]
    },
    {
      "metadata": {
        "id": "EiYDo4OOFkEz",
        "colab_type": "text",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": [
        "Similar approaches as inception though (bottlenecks)\n",
        "\n",
        "Also, has variants- Resnet (34 layers), Resnet 50, Resnet 101 XD."
      ]
    },
    {
      "metadata": {
        "id": "zHbHusqPFubl",
        "colab_type": "text",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": [
        "# Inception v4 or Inception_Resnet_v2\n",
        "_There's no method to this madness_\n",
        "\n",
        "And Christian and team are at it again with a new version of Inception.\n",
        "\n",
        "The Inception module after the stem is rather similar to Inception V3:\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/800/0*SJ7DP_-0R1vdpVzv.jpg)\n"
      ]
    },
    {
      "metadata": {
        "id": "39E1hrRaH3Xl",
        "colab_type": "text",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": [
        "They also combined the Inception module with the ResNet module:\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/800/0*exGWbD4A0QKM2lU_.jpg)"
      ]
    },
    {
      "metadata": {
        "id": "eRa9mg6OIIPF",
        "colab_type": "text",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": [
        "# SqueezeNet\n",
        "\n",
        "The paper of SqueezeNet provides a smart architecture as well as a quantitative analysis. For the same accuracy of AlexNet, SqueezeNet can be 3 times faster and 500 times smaller.\n",
        "\n",
        "The main ideas of SqueezeNet are:\n",
        "\n",
        "1. Using 1x1(point-wise) filters to replace 3x3 filters, as the former only 1/9 of computation.\n",
        "2. Using 1x1 filters as a bottleneck layer to reduce depth to reduce computation of the following 3x3 filters.\n",
        "3. Downsample late to keep a big feature map.\n"
      ]
    },
    {
      "metadata": {
        "id": "BRiRbr8jIRRv",
        "colab_type": "text",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": [
        "The building brick of SqueezeNet is called fire module, which contains two layers: a squeeze layer and an expand layer. A SqueezeNet stackes a bunch of fire modules and a few pooling layers. \n",
        "\n",
        "The squeeze layer and expand layer keep the same feature map size, while the former reduce the depth to a smaller number, the later increase it. The squeezing (bottoleneck layer) and expansion behavior is common in neural architectures. Another common pattern is increasing depth while reducing feature map size to get high level abstract."
      ]
    },
    {
      "metadata": {
        "id": "3aKgMXlzIg2u",
        "colab_type": "text",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://cdn-images-1.medium.com/max/1600/1*dVaL1bcv5Ewpz-wen7IXCA.png)\n",
        "\n",
        "Credits: [Squeenet](https://github.com/DeepScale/SqueezeNet)"
      ]
    },
    {
      "metadata": {
        "id": "Xu3oyr0ONcA6",
        "colab_type": "text",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": [
        "# Inception V3 on CIFAR 10"
      ]
    },
    {
      "metadata": {
        "id": "QLrxlGs8Kp7H",
        "colab_type": "code",
        "slideshow": {
          "slide_type": "slide"
        },
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "from keras.datasets import cifar10\n",
        "from keras import backend as K\n",
        "from keras.utils import np_utils\n",
        "\n",
        "nb_train_samples = 3000 # 3000 training samples\n",
        "nb_valid_samples = 100 # 100 validation samples\n",
        "num_classes = 10\n",
        "\n",
        "def load_cifar10_data(img_rows, img_cols):\n",
        "\n",
        "    # Load cifar10 training and validation sets\n",
        "    (X_train, Y_train), (X_valid, Y_valid) = cifar10.load_data()\n",
        "\n",
        "    # Resize trainging images\n",
        "    if K.image_dim_ordering() == 'th':\n",
        "        X_train = np.array([cv2.resize(img.transpose(1,2,0), (img_rows,img_cols)).transpose(2,0,1) for img in X_train[:nb_train_samples,:,:,:]])\n",
        "        X_valid = np.array([cv2.resize(img.transpose(1,2,0), (img_rows,img_cols)).transpose(2,0,1) for img in X_valid[:nb_valid_samples,:,:,:]])\n",
        "    else:\n",
        "        X_train = np.array([cv2.resize(img, (img_rows,img_cols)) for img in X_train[:nb_train_samples,:,:,:]])\n",
        "        X_valid = np.array([cv2.resize(img, (img_rows,img_cols)) for img in X_valid[:nb_valid_samples,:,:,:]])\n",
        "\n",
        "    # Transform targets to keras compatible format\n",
        "    Y_train = np_utils.to_categorical(Y_train[:nb_train_samples], num_classes)\n",
        "    Y_valid = np_utils.to_categorical(Y_valid[:nb_valid_samples], num_classes)\n",
        "\n",
        "    return X_train, Y_train, X_valid, Y_valid"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TQNmSahqMrRw",
        "colab_type": "code",
        "slideshow": {
          "slide_type": "slide"
        },
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.preprocessing import image\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, GlobalAveragePooling2D\n",
        "from keras import backend as K\n",
        "\n",
        "# create the base pre-trained model\n",
        "base_model = InceptionV3(weights='imagenet', include_top=False)\n",
        "\n",
        "# add a global spatial average pooling layer\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "# let's add a fully-connected layer\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "# and a logistic layer -- let's say we have 200 classes\n",
        "predictions = Dense(200, activation='softmax')(x)\n",
        "\n",
        "# this is the model we will train\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# first: train only the top layers (which were randomly initialized)\n",
        "# i.e. freeze all convolutional InceptionV3 layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# compile the model (should be done *after* setting layers to non-trainable)\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "\n",
        "# train the model on the new data for a few epochs\n",
        "model.fit_generator(model, steps_per_epoch=None, epochs=1, verbose=1, callbacks=None, validation_data=X_valid, validation_steps=None, class_weight=None, max_queue_size=10, workers=1, use_multiprocessing=False, shuffle=True, initial_epoch=0)\n",
        "\n",
        "# at this point, the top layers are well trained and we can start fine-tuning\n",
        "# convolutional layers from inception V3. We will freeze the bottom N layers\n",
        "# and train the remaining top layers.\n",
        "\n",
        "# let's visualize layer names and layer indices to see how many layers\n",
        "# we should freeze:\n",
        "for i, layer in enumerate(base_model.layers):\n",
        "   print(i, layer.name)\n",
        "\n",
        "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
        "# the first 249 layers and unfreeze the rest:\n",
        "for layer in model.layers[:249]:\n",
        "   layer.trainable = False\n",
        "for layer in model.layers[249:]:\n",
        "   layer.trainable = True\n",
        "\n",
        "# we need to recompile the model for these modifications to take effect\n",
        "# we use SGD with a low learning rate\n",
        "from keras.optimizers import SGD\n",
        "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy')\n",
        "\n",
        "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
        "# alongside the top Dense layers\n",
        "model.fit_generator(generator, steps_per_epoch=None, epochs=1, verbose=1, callbacks=None, validation_data=X_valid, validation_steps=None, class_weight=None, max_queue_size=10, workers=1, use_multiprocessing=False, shuffle=True, initial_epoch=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mOeZNHmHJioZ",
        "colab_type": "text",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": [
        "# Mobilenets\n",
        "\n",
        " Core layers that MobileNet is built on which are depthwise separable filters (factorised filters).\n",
        " \n",
        " Depthwise separable convolution are made up of two layers: depthwise convolutions and pointwise convolutions. \n",
        " \n",
        " Depthwise convolutions are used to apply a single filter per each input channel (input depth). Pointwise convolution, a simple 1x1\n",
        " convolution, is then used to create a linear combination of the output of the depthwise layer. MobileNets use both batchnorm and ReLU nonlinearities for both layers.\n",
        " \n",
        " * Also uses width and resolution multipliers to save on computation\n",
        " \n",
        " * Even more effective than Squeezenet"
      ]
    },
    {
      "metadata": {
        "id": "Fjog4vCGLuta",
        "colab_type": "text",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": [
        "# Depth wise convolves\n",
        "\n",
        "Depthwise separable convolutions which is a form of factorized convolutions which factorize a standard convolution into a depthwise convolution and a 1x1 convolution called a pointwise convolution."
      ]
    },
    {
      "metadata": {
        "id": "QkfhfI-0L1AI",
        "colab_type": "text",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://raw.githubusercontent.com/joshua19881228/my_blogs/master/Computer_Vision/Reading_Note/figures/Reading_Note_20170719_MobileNet_0.png)"
      ]
    },
    {
      "metadata": {
        "id": "j_ERsI_wLnp2",
        "colab_type": "text",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://raw.githubusercontent.com/joshua19881228/my_blogs/master/Computer_Vision/Reading_Note/figures/Reading_Note_20170719_MobileNet_2.png)"
      ]
    },
    {
      "metadata": {
        "id": "OY0z3WawL_KA",
        "colab_type": "text",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": [
        "# Transfer Learning with Mobilenets\n",
        "\n",
        "Clone the repo here: [link](https://github.com/varun19299/Tensorflow_poets.git)"
      ]
    },
    {
      "metadata": {
        "id": "gptVN-CBL-Zj",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oA5dHOwqIjRC",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}